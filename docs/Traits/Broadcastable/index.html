<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>The `Broadcastable` Trait - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.119.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Traits/Broadcastable/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script>
<link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script>
<script src=https://mlir.llvm.org/js/bundle.js></script>
<script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li></ul></li><li><a href="https://github.com/llvm/llvm-project/issues?q=is%3Aissue%20state%3Aopen%20label%3Amlir">Bugs</a></li><li><a href=https://github.com/llvm/mlir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/MLIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>The `Broadcastable` Trait</h1><p><nav id=TableOfContents><ul><li><a href=#description>Description</a></li><li><a href=#dimension-inference>Dimension inference</a></li><li><a href=#shape-inference>Shape inference</a></li><li><a href=#verification>Verification</a></li><li><a href=#examples>Examples</a></li></ul></nav><h2 id=description>Description&nbsp;<a class=headline-hash href=#description>¶</a></h2><p>The <code>Broadcastable</code> trait enforces the following properties on an operation:</p><ul><li><p>The operation has at least one input operand.</p></li><li><p>The operation has exactly one result.</p></li><li><p>All input operands and result are of type <code>tensor</code> or <code>vector</code>.</p></li><li><p>A shape inference mechanism is able to compute the result shape solely based on input operand shapes.</p></li><li><p>Input operands have broadcast-compatible shapes, according to the verification rules presented below.</p></li><li><p>The operation&rsquo;s result shape is compatible with —though not necessarily identical to— the shape inferred from its input operands, according to the verification rules presented below.</p></li></ul><h2 id=dimension-inference>Dimension inference&nbsp;<a class=headline-hash href=#dimension-inference>¶</a></h2><p>Given an operation with two input operands, the size of dimension <code>i</code> of its result can be inferred from dimension <code>i</code> of the operands according to the table below. Here, <code>dim0</code> and <code>dim1</code> represent dimension <code>i</code> of the input operands in an interchangeable order, while <code>inferredDim</code> represents the inferred size for dimension <code>i</code> of the operation result. Dimensions are classified in three categories: dynamic ("?"), static equal to 1 (&ldquo;1&rdquo;), and static greater than 1 (">1").</p><table><thead><tr><th><code>dim0</code></th><th><code>dim1</code></th><th><code>inferredDim</code></th><th>Notes</th></tr></thead><tbody><tr><td>?</td><td>?</td><td>?</td><td>If <code>RuntimeSize(dim0)</code> is 1, dimension <code>dim0</code> is broadcast to <code>RuntimeSize(dim1)</code>. If <code>RuntimeSize(dim1)</code> is 1, dimension <code>dim1</code> is broadcast to <code>RuntimeSize(dim0)</code>. The operation produces undefined behavior if both runtime sizes are greater than 1 and not equal.</td></tr><tr><td>?</td><td>1</td><td>?</td><td>Dimension <code>dim1</code> is broadcast to <code>RuntimeSize(dim0)</code>.</td></tr><tr><td>?</td><td>>1</td><td><code>dim1</code></td><td>If <code>RuntimeSize(dim0)</code> is 1, <code>dim0</code> is broadcast to <code>dim1</code>. The operation produces undefined behavior if <code>RuntimeSize(dim0)</code> is greater than 1 and not equal to <code>dim1</code>.</td></tr><tr><td>1</td><td>1</td><td>1</td><td></td></tr><tr><td>1</td><td>>1</td><td><code>dim1</code></td><td>Dimension <code>dim0</code> is broadcast to <code>dim1</code>.</td></tr><tr><td>>1</td><td>>1</td><td><code>dim0</code></td><td>The operation verifier produces a compile-time error if <code>dim0</code> != <code>dim1</code>.</td></tr></tbody></table><p>The following pseudo-function is a formal representation of the dimension inference process:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>InferDim</span><span class=p>(</span><span class=n>dim0</span><span class=p>,</span> <span class=n>dim1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>switch</span> <span class=p>(</span><span class=n>dim0</span><span class=p>,</span> <span class=n>dim1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>case</span> <span class=p>(</span><span class=err>?</span><span class=p>,</span> <span class=err>?</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>case</span> <span class=p>(</span><span class=err>?</span><span class=p>,</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>case</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>case</span> <span class=p>(</span><span class=o>&gt;</span><span class=mi>1</span><span class=p>,</span> <span class=err>?</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>case</span> <span class=p>(</span><span class=o>&gt;</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>			<span class=k>return</span> <span class=n>dim0</span>
</span></span><span class=line><span class=cl>		<span class=k>case</span> <span class=p>(</span><span class=err>?</span><span class=p>,</span> <span class=o>&gt;</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>case</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=err>?</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>case</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>&gt;</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>			<span class=k>return</span> <span class=n>dim1</span>
</span></span><span class=line><span class=cl>		<span class=k>case</span> <span class=p>(</span><span class=o>&gt;</span><span class=mi>1</span><span class=p>,</span> <span class=o>&gt;</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>			<span class=n>ERROR_IF</span><span class=p>(</span><span class=n>dim0</span> <span class=o>!=</span> <span class=n>dim1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			<span class=k>return</span> <span class=n>dim0</span>
</span></span></code></pre></div><h2 id=shape-inference>Shape inference&nbsp;<a class=headline-hash href=#shape-inference>¶</a></h2><p>The shape inference process begins by correcting rank differences in input operands. A shape is expanded by adding additional dimensions of size 1 on its left until the desired rank is reached, as shown here:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>ExpandRank</span><span class=p>(</span><span class=n>shape</span><span class=p>,</span> <span class=n>rank</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=k>while</span> <span class=nb>len</span><span class=p>(</span><span class=n>shape</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>rank</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=n>shape</span><span class=o>.</span><span class=n>prepend</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span></code></pre></div><p>Given the shapes of two ranked input operands, the result&rsquo;s shape is inferred by equalizing input ranks and inferring individual dimensions, as shown here:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>InferShape</span><span class=p>(</span><span class=n>shape0</span><span class=p>,</span> <span class=n>shape1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># Equalize ranks</span>
</span></span><span class=line><span class=cl>  <span class=n>rank</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>GetRank</span><span class=p>(</span><span class=n>shape0</span><span class=p>),</span> <span class=n>GetRank</span><span class=p>(</span><span class=n>shape1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>  <span class=n>ExpandRank</span><span class=p>(</span><span class=n>shape0</span><span class=p>,</span> <span class=n>rank</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=n>ExpandRank</span><span class=p>(</span><span class=n>shape1</span><span class=p>,</span> <span class=n>rank</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>  <span class=c1># Infer shape</span>
</span></span><span class=line><span class=cl>  <span class=n>inferredShape</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>  <span class=k>for</span> <span class=p>(</span><span class=n>dim0</span><span class=p>,</span> <span class=n>dim1</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>shape0</span><span class=p>,</span> <span class=n>shape1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>inferredDim</span> <span class=o>=</span> <span class=n>InferDim</span><span class=p>(</span><span class=n>dim0</span><span class=p>,</span> <span class=n>dim1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>inferredShape</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>inferredDim</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>inferredShape</span>
</span></span></code></pre></div><p>The result shape for an operation with an arbitrary number of input operands is then inferred by discarding unranked operands, applying shape inference on the first ranked operand pair, and updating the inferred shape with each additional ranked operand. If the operation has no ranked operands, the result shape cannot be inferred. If the operation has exactly one ranked operand, its shape is directly provided as the inferred result shape. Formally:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>InferResultShape</span><span class=p>(</span><span class=n>op</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c1># Filter ranked operands</span>
</span></span><span class=line><span class=cl>	<span class=n>rankedOperands</span> <span class=o>=</span> <span class=nb>filter</span><span class=p>(</span><span class=n>op</span><span class=o>.</span><span class=n>operands</span><span class=p>,</span> <span class=n>IsRanked</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>rankedOperands</span><span class=p>)</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Infer result shape</span>
</span></span><span class=line><span class=cl>	<span class=n>inferredShape</span> <span class=o>=</span> <span class=n>GetShape</span><span class=p>(</span><span class=n>rankedOperands</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>operand</span> <span class=ow>in</span> <span class=n>rankedOperands</span><span class=p>[</span><span class=mi>1</span><span class=p>:]:</span>
</span></span><span class=line><span class=cl>		<span class=n>inferredShape</span> <span class=o>=</span> <span class=n>InferShape</span><span class=p>(</span><span class=n>inferredShape</span><span class=p>,</span> <span class=n>GetShape</span><span class=p>(</span><span class=n>operand</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>inferredShape</span>
</span></span></code></pre></div><h2 id=verification>Verification&nbsp;<a class=headline-hash href=#verification>¶</a></h2><p>The legality of an operation with the <code>Broadcastable</code> trait is verified by first running the shape inference process. If a failure occurs during shape inference, it is concluded that input operands are not broadcast-compatible, and verification fails. If shape inference succeeds, verification continues.</p><p>If either the result is unranked or all input operands are unranked, no further verification steps are needed, and the process ends here successfully. If, on the contrary, both the result and at least one input operand are ranked, verification continues by checking for a matching rank between the previously inferred shape and the result.</p><p>Once a rank match is guaranteed, each dimension of the inferred shape is compared with the corresponding dimension of the actual result shape according to the following table table:</p><table><thead><tr><th><code>inferredDim</code></th><th><code>actualDim</code></th><th>Verification outcome</th></tr></thead><tbody><tr><td>?</td><td>?</td><td><strong>OK</strong></td></tr><tr><td>?</td><td>static</td><td><strong>OK</strong><br>A failure to guarantee that the runtime dimension size of the result is equal to <code>actualDim</code> causes undefined behavior. While unusual, this implicit dynamic-to-static cast is convenient in certain scenarios, such as an intermediate state of a shape inference pass. Ultimately, a static dimension in the result implies that all input dimension sizes are also known at compile time and may therefore become static as well, preferably.</td></tr><tr><td>static</td><td>?</td><td><strong>OK</strong><br>The actual result dimension may be dynamic even when a static size can be inferred at compile time. The programmer may choose to relax the specificity of the result dimension for forward compatibility of the result type.</td></tr><tr><td>static</td><td>static</td><td><strong>OK if equal</strong><br>When both the inferred and actual dimensions are static, they must be set to the same size.</td></tr></tbody></table><p>The full verification process can be formally specified as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Verify</span><span class=p>(</span><span class=n>op</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c1># Run shape inference</span>
</span></span><span class=line><span class=cl>	<span class=n>inferredShape</span> <span class=o>=</span> <span class=n>InferResultShape</span><span class=p>(</span><span class=n>op</span><span class=o>.</span><span class=n>operands</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=c1># Done if result is unranked or all operands are unranked</span>
</span></span><span class=line><span class=cl>	<span class=k>if</span> <span class=ow>not</span> <span class=n>IsRanked</span><span class=p>(</span><span class=n>op</span><span class=o>.</span><span class=n>result</span><span class=p>)</span> <span class=ow>or</span> <span class=n>inferredShape</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Rank must match</span>
</span></span><span class=line><span class=cl>	<span class=n>actualShape</span> <span class=o>=</span> <span class=n>GetShape</span><span class=p>(</span><span class=n>op</span><span class=o>.</span><span class=n>result</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>ERROR_IF</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>inferredShape</span><span class=p>)</span> <span class=o>!=</span> <span class=nb>len</span><span class=p>(</span><span class=n>actualShape</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	
</span></span><span class=line><span class=cl>	<span class=c1># Verify</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=p>(</span><span class=n>inferredDim</span><span class=p>,</span> <span class=n>actualDim</span><span class=p>)</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>inferredShape</span><span class=p>,</span> <span class=n>actualShape</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>ERROR_IF</span><span class=p>(</span><span class=n>IsStatic</span><span class=p>(</span><span class=n>actualDim</span><span class=p>)</span> <span class=ow>and</span> <span class=n>inferredDim</span> <span class=o>!=</span> <span class=n>actualDim</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=examples>Examples&nbsp;<a class=headline-hash href=#examples>¶</a></h2><p>The following are correct uses of broadcastable ops:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// Exact match of static sizes.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x2x</span><span class=k>i32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x2x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x2x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// Dynamic sizes match. The programmer must guarantee that the runtime sizes of
</span></span></span><span class=line><span class=cl><span class=c>// %arg0 and %arg1 are equal at runtime.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// The shape of %arg0 is broadcast from tensor&lt;1xi32&gt; to tensor&lt;4xi32&gt;.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x</span><span class=k>i32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// The shape of %result is inferred as tensor&lt;4xi32&gt;, while the actual result
</span></span></span><span class=line><span class=cl><span class=c>// type is tensor&lt;?xi32&gt;. The inferred shape is compatible with the actual shape.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// The shape of %arg0 is first expanded to tensor&lt;1x1x4xi32&gt; and then broadcast
</span></span></span><span class=line><span class=cl><span class=c>// to tensor&lt;2x3x4xi32&gt;.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>i32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x4x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x4x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// Input and results tensors have different element types (i1, i32, i64). The
</span></span></span><span class=line><span class=cl><span class=c>// &#39;Broadcastable&#39; trait has no restrictions on element types.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i1</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// No result shape verification is needed when the result is unranked.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i32</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xi32<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// No result shape verification needed when all inputs are unranked.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;*</span>xi32<span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xi32<span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>The following are incorrect uses of broadcastable ops:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// Dimension 0 of input operands is static but not equal.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>3x</span><span class=k>i32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// The inferred result shape is tensor&lt;3xi32&gt;, but the actual result shape is
</span></span></span><span class=line><span class=cl><span class=c>// tensor&lt;1x3xi32&gt;. Inferred and actual shapes differ in rank.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>3x</span><span class=k>i32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>3x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x3x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// The inferred result shape is tensor&lt;?xi32&gt;, but the actual shape is
</span></span></span><span class=line><span class=cl><span class=c>// tensor&lt;4xi32&gt;. The inferred shape is not compatible with the actual shape.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// The inferred result shape is tensor&lt;2xi32&gt;, but the actual result shape is
</span></span></span><span class=line><span class=cl><span class=c>// tensor&lt;4xi32&gt;, which is not compatible.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// The inferred result shape is tensor&lt;1xi32&gt;, but the actual result shape is
</span></span></span><span class=line><span class=cl><span class=c>// tensor&lt;4xi32&gt;. Broadcast semantics are not applicable for results.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=s>&#34;test.broadcastable&#34;</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>,</span> <span class=nv>%arg1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x</span><span class=k>i32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x</span><span class=k>i32</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>4x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span></code></pre></div><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=https://mlir.llvm.org/docs/Traits/ title=Traits><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - Traits</a>
<a class="nav nav-next" href=https://mlir.llvm.org/docs/Tutorials/ title=Tutorials>Next - Tutorials <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=https://mlir.llvm.org/governance/>Governance</a></li><li><a href=https://mlir.llvm.org/users/>Users of MLIR</a></li><li><a href=https://mlir.llvm.org/pubs/>MLIR Related Publications</a></li><li><a href=https://mlir.llvm.org/talks/>Talks</a></li><li><a href=https://mlir.llvm.org/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://mlir.llvm.org/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://mlir.llvm.org/getting_started/Faq/>FAQ</a></li><li><a href=https://mlir.llvm.org/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://mlir.llvm.org/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=https://mlir.llvm.org/getting_started/openprojects/>Open Projects</a></li><li><a href=https://mlir.llvm.org/getting_started/Glossary/>Glossary</a></li><li><a href=https://mlir.llvm.org/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-reduce/>MLIR Reduce</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-rewrite/>mlir-rewrite</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/ActionTracing/>Action: Tracing and Debugging MLIR-based Compilers</a></li><li><a href=https://mlir.llvm.org/docs/Bufferization/>Bufferization</a></li><li><a href=https://mlir.llvm.org/docs/DataLayout/>Data Layout Modeling</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/DefiningDialects/>Defining Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Constraints/>Constraints</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Assembly/>Customizing Assembly Behavior</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/AttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Operations/>Operation Definition Specification (ODS)</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/DialectConversion/>Dialect Conversion</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/>Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMDGPU/>'amdgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArithOps/>'arith' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSME/>'ArmSME' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/BufferizationOps/>'bufferization' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ControlFlowDialect/>'cf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/DLTIDialect/>'dlti' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Func/>'func' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IndexOps/>'index' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IRDL/>'irdl' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MemRef/>'memref' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MLProgramOps/>'ml_program' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MPI/>'mpi' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVGPU/>'nvgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/>'omp' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/ODS/>ODS Documentation</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PtrOps/>'ptr' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Shard/>'shard' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SMT/>'smt' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/UBOps/>'ub' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/VCIXDialect/>'vcix' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeGPU/>'xegpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeVMDialect/>'xevm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MatchOpInterfaces/>OpInterface definitions</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SPIR-V/>SPIR-V Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Transform/>Transform Dialect</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Interfaces/>Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIR/>LLVM IR Target</a></li><li><a href=https://mlir.llvm.org/docs/BytecodeFormat/>MLIR Bytecode Format</a></li><li><a href=https://mlir.llvm.org/docs/CAPI/>MLIR C API</a></li><li><a href=https://mlir.llvm.org/docs/LangRef/>MLIR Language Reference</a></li><li><a href=https://mlir.llvm.org/docs/ReleaseNotes/>MLIR Release Notes</a></li><li><a href=https://mlir.llvm.org/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=https://mlir.llvm.org/docs/OwnershipBasedBufferDeallocation/>Ownership-based Buffer Deallocation</a></li><li><a href=https://mlir.llvm.org/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/Passes/>Passes</a></li><li><a href=https://mlir.llvm.org/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=https://mlir.llvm.org/docs/PDLL/>PDLL - PDL Language</a></li><li><a href=https://mlir.llvm.org/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/SideEffectsAndSpeculation/>Side Effects & Speculation</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/ShapeInference/>Shape Inference</a></li><li><a href=https://mlir.llvm.org/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=https://mlir.llvm.org/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=https://mlir.llvm.org/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/Traits/>Traits<span class="mark opened">-</span></a><ul class=sub-menu><li class=active><a href=https://mlir.llvm.org/docs/Traits/Broadcastable/>The `Broadcastable` Trait</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/Toy/>Toy Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/transform/>Transform Dialect Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch0/>Chapter 0: A Primer on “Structured” Linalg Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch1/>Chapter 1: Combining Existing Transformations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch2/>Chapter 2: Adding a Simple New Transformation Operation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch3/>Chapter 3: More than Simple Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch4/>Chapter 4: Matching Payload with Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/ChH/>Chapter H: Reproducing Halide Schedule</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/MlirOpt/>Using `mlir-opt`</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>