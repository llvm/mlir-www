<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>'shape' Dialect - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.119.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Dialects/ShapeDialect/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script>
<link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script>
<script src=https://mlir.llvm.org/js/bundle.js></script>
<script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li><li class=child><a href=/python-bindings/>Python Bindings API docs</a></li></ul></li><li><a href="https://github.com/llvm/llvm-project/issues?q=is%3Aissue%20state%3Aopen%20label%3Amlir">Bugs</a></li><li><a href=https://github.com/llvm/mlir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/MLIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>'shape' Dialect</h1><p>Description of operations & types within the Shape dialect as well as their
<a href=#different-stages-of-lowering-shape-dialect>usage</a>.</p><p><em>Types and operations for shape dialect</em></p><p>This dialect contains operations for shape inference.</p><p>Note: Unless explicitly stated, all functions that return a shape and take
shapes as input, return the invalid shape if one of its operands is an
invalid shape. This avoids flagging multiple errors for one verification
failure. The dialect itself does not specify how errors should be combined
(there are multiple different options, from always choosing first operand,
concatting etc. on how to combine them).</p><p><nav id=TableOfContents><ul><li><a href=#operations>Operations</a><ul><li><a href=#shapeadd-shapeaddop><code>shape.add</code> (shape::AddOp)</a></li><li><a href=#shapeany-shapeanyop><code>shape.any</code> (shape::AnyOp)</a></li><li><a href=#shapeassuming-shapeassumingop><code>shape.assuming</code> (shape::AssumingOp)</a></li><li><a href=#shapeassuming_all-shapeassumingallop><code>shape.assuming_all</code> (shape::AssumingAllOp)</a></li><li><a href=#shapeassuming_yield-shapeassumingyieldop><code>shape.assuming_yield</code> (shape::AssumingYieldOp)</a></li><li><a href=#shapebroadcast-shapebroadcastop><code>shape.broadcast</code> (shape::BroadcastOp)</a></li><li><a href=#shapeconcat-shapeconcatop><code>shape.concat</code> (shape::ConcatOp)</a></li><li><a href=#shapeconst_shape-shapeconstshapeop><code>shape.const_shape</code> (shape::ConstShapeOp)</a></li><li><a href=#shapeconst_size-shapeconstsizeop><code>shape.const_size</code> (shape::ConstSizeOp)</a></li><li><a href=#shapeconst_witness-shapeconstwitnessop><code>shape.const_witness</code> (shape::ConstWitnessOp)</a></li><li><a href=#shapecstr_broadcastable-shapecstrbroadcastableop><code>shape.cstr_broadcastable</code> (shape::CstrBroadcastableOp)</a></li><li><a href=#shapecstr_eq-shapecstreqop><code>shape.cstr_eq</code> (shape::CstrEqOp)</a></li><li><a href=#shapecstr_require-shapecstrrequireop><code>shape.cstr_require</code> (shape::CstrRequireOp)</a></li><li><a href=#shapedebug_print-shapedebugprintop><code>shape.debug_print</code> (shape::DebugPrintOp)</a></li><li><a href=#shapedim-shapedimop><code>shape.dim</code> (shape::DimOp)</a></li><li><a href=#shapediv-shapedivop><code>shape.div</code> (shape::DivOp)</a></li><li><a href=#shapefrom_extent_tensor-shapefromextenttensorop><code>shape.from_extent_tensor</code> (shape::FromExtentTensorOp)</a></li><li><a href=#shapefrom_extents-shapefromextentsop><code>shape.from_extents</code> (shape::FromExtentsOp)</a></li><li><a href=#shapefunc-shapefuncop><code>shape.func</code> (shape::FuncOp)</a></li><li><a href=#shapefunction_library-shapefunctionlibraryop><code>shape.function_library</code> (shape::FunctionLibraryOp)</a></li><li><a href=#shapeget_extent-shapegetextentop><code>shape.get_extent</code> (shape::GetExtentOp)</a></li><li><a href=#shapeindex_to_size-shapeindextosizeop><code>shape.index_to_size</code> (shape::IndexToSizeOp)</a></li><li><a href=#shapeis_broadcastable-shapeisbroadcastableop><code>shape.is_broadcastable</code> (shape::IsBroadcastableOp)</a></li><li><a href=#shapemax-shapemaxop><code>shape.max</code> (shape::MaxOp)</a></li><li><a href=#shapemeet-shapemeetop><code>shape.meet</code> (shape::MeetOp)</a></li><li><a href=#shapemin-shapeminop><code>shape.min</code> (shape::MinOp)</a></li><li><a href=#shapemul-shapemulop><code>shape.mul</code> (shape::MulOp)</a></li><li><a href=#shapenum_elements-shapenumelementsop><code>shape.num_elements</code> (shape::NumElementsOp)</a></li><li><a href=#shaperank-shaperankop><code>shape.rank</code> (shape::RankOp)</a></li><li><a href=#shapereduce-shapereduceop><code>shape.reduce</code> (shape::ReduceOp)</a></li><li><a href=#shapereturn-shapereturnop><code>shape.return</code> (shape::ReturnOp)</a></li><li><a href=#shapeshape_eq-shapeshapeeqop><code>shape.shape_eq</code> (shape::ShapeEqOp)</a></li><li><a href=#shapeshape_of-shapeshapeofop><code>shape.shape_of</code> (shape::ShapeOfOp)</a></li><li><a href=#shapesize_to_index-shapesizetoindexop><code>shape.size_to_index</code> (shape::SizeToIndexOp)</a></li><li><a href=#shapesplit_at-shapesplitatop><code>shape.split_at</code> (shape::SplitAtOp)</a></li><li><a href=#shapeto_extent_tensor-shapetoextenttensorop><code>shape.to_extent_tensor</code> (shape::ToExtentTensorOp)</a></li><li><a href=#shapevalue_as_shape-shapevalueasshapeop><code>shape.value_as_shape</code> (shape::ValueAsShapeOp)</a></li><li><a href=#shapevalue_of-shapevalueofop><code>shape.value_of</code> (shape::ValueOfOp)</a></li><li><a href=#shapewith_shape-shapewithop><code>shape.with_shape</code> (shape::WithOp)</a></li><li><a href=#shapeyield-shapeyieldop><code>shape.yield</code> (shape::YieldOp)</a></li></ul></li><li><a href=#types>Types</a><ul><li><a href=#shapetype>ShapeType</a></li><li><a href=#sizetype>SizeType</a></li><li><a href=#valueshapetype>ValueShapeType</a></li><li><a href=#witnesstype>WitnessType</a></li></ul></li><li><a href=#different-stages-of-lowering-shape-dialect>Different stages of lowering Shape dialect</a></li></ul></nav><h2 id=operations>Operations&nbsp;<a class=headline-hash href=#operations>¶</a></h2><p><a href=https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Shape/IR/ShapeOps.td>source</a></p><h3 id=shapeadd-shapeaddop><code>shape.add</code> (shape::AddOp)&nbsp;<a class=headline-hash href=#shapeadd-shapeaddop>¶</a></h3><p><em>Addition of sizes and indices</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.add` $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `-&gt;` type($result)
</code></pre><p>Adds two sizes or indices. If either operand is an error it will be
propagated to the result. The operands can be of type <code>size</code> or <code>index</code>. If
at least one of the operands can hold an error, i.e. if it is of type
<code>size</code>, the result must be of type <code>size</code>. If error propagation is not
possible because both operands are of type <code>index</code> then the result may be
of type <code>size</code> or <code>index</code>.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>Commutative</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands>Operands:&nbsp;<a class=headline-hash href=#operands>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>lhs</code></td><td>size or index</td></tr><tr><td style=text-align:center><code>rhs</code></td><td>size or index</td></tr></tbody></table><h4 id=results>Results:&nbsp;<a class=headline-hash href=#results>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>size or index</td></tr></tbody></table><h3 id=shapeany-shapeanyop><code>shape.any</code> (shape::AnyOp)&nbsp;<a class=headline-hash href=#shapeany-shapeanyop>¶</a></h3><p><em>Return any combination of the input shapes</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.any` $inputs attr-dict `:` type($inputs) `-&gt;` type($result)
</code></pre><p>This operation takes multiple input shapes or extent tensors and returns
some combination of their dimensions. This can be best seen with examples
below.</p><p>The result is undefined, but still side-effect free, in cases where the
inputs have differing ranks or differ in extents of shared dimensions.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%s0</span> <span class=p>=</span> shape<span class=p>.</span>any <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=err>?</span><span class=p>],</span> <span class=p>[</span><span class=err>?</span><span class=p>,</span><span class=m>3</span><span class=p>]</span> <span class=c>// [2,3]
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%s1</span> <span class=p>=</span> shape<span class=p>.</span>any <span class=p>[</span><span class=err>?</span><span class=p>,</span><span class=err>?</span><span class=p>],</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// [1,2]
</span></span></span></code></pre></div><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>Commutative</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-1>Operands:&nbsp;<a class=headline-hash href=#operands-1>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>inputs</code></td><td>variadic of shape or extent tensor</td></tr></tbody></table><h4 id=results-1>Results:&nbsp;<a class=headline-hash href=#results-1>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapeassuming-shapeassumingop><code>shape.assuming</code> (shape::AssumingOp)&nbsp;<a class=headline-hash href=#shapeassuming-shapeassumingop>¶</a></h3><p><em>Execute the region</em></p><p>Executes the region assuming all witnesses are true.</p><p>&ldquo;assuming&rdquo; operations represent an execution order restriction to the
compiler, information for dependent code to rely on (by assuming), and
nothing else. They should not exist after a program is fully lowered and
ready to execute.</p><p>Traits: <code>RecursiveMemoryEffects</code>, <code>SingleBlockImplicitTerminator&lt;AssumingYieldOp></code>, <code>SingleBlock</code></p><p>Interfaces: <code>RegionBranchOpInterface</code></p><h4 id=operands-2>Operands:&nbsp;<a class=headline-hash href=#operands-2>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>witness</code></td><td></td></tr></tbody></table><h4 id=results-2>Results:&nbsp;<a class=headline-hash href=#results-2>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>results</code></td><td>variadic of any type</td></tr></tbody></table><h3 id=shapeassuming_all-shapeassumingallop><code>shape.assuming_all</code> (shape::AssumingAllOp)&nbsp;<a class=headline-hash href=#shapeassuming_all-shapeassumingallop>¶</a></h3><p><em>Return a logical AND of all witnesses</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.assuming_all` $inputs attr-dict
</code></pre><p>Used to simplify constraints as any single failing precondition is enough
to prevent execution.</p><p>&ldquo;assuming&rdquo; operations represent an execution order restriction to the
compiler, information for dependent code to rely on (by assuming), and
nothing else. They should not exist after a program is fully lowered and
ready to execute.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%w0</span> <span class=p>=</span> shape<span class=p>.</span>cstr_broadcastable <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>3</span><span class=p>,</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Passing
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>cstr_broadcastable <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>3</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Failure
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%w2</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Passing
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%wf</span> <span class=p>=</span> shape<span class=p>.</span>assuming_all <span class=nv>%w0</span><span class=p>,</span> <span class=nv>%w1</span> <span class=c>// Failure
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%wt</span> <span class=p>=</span> shape<span class=p>.</span>assuming_all <span class=nv>%w0</span><span class=p>,</span> <span class=nv>%w2</span> <span class=c>// Passing
</span></span></span></code></pre></div><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>Commutative</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-3>Operands:&nbsp;<a class=headline-hash href=#operands-3>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>inputs</code></td><td>variadic of</td></tr></tbody></table><h4 id=results-3>Results:&nbsp;<a class=headline-hash href=#results-3>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td></td></tr></tbody></table><h3 id=shapeassuming_yield-shapeassumingyieldop><code>shape.assuming_yield</code> (shape::AssumingYieldOp)&nbsp;<a class=headline-hash href=#shapeassuming_yield-shapeassumingyieldop>¶</a></h3><p><em>Yield operation</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.assuming_yield` attr-dict ($operands^ `:` type($operands))?
</code></pre><p>This yield operation represents a return operation within the
<code>shape.assuming</code> operation region. The operation takes variable number of
operands and produces no results. The operand number and types must match
the number and types of parent <code>shape.assuming</code> results.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>HasParent&lt;AssumingOp></code>, <code>ReturnLike</code>, <code>Terminator</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code>, <code>RegionBranchTerminatorOpInterface</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-4>Operands:&nbsp;<a class=headline-hash href=#operands-4>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>operands</code></td><td>variadic of any type</td></tr></tbody></table><h3 id=shapebroadcast-shapebroadcastop><code>shape.broadcast</code> (shape::BroadcastOp)&nbsp;<a class=headline-hash href=#shapebroadcast-shapebroadcastop>¶</a></h3><p><em>Returns the broadcasted output shape of two or more inputs</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.broadcast` $shapes attr-dict `:` type($shapes) `-&gt;` type($result)
</code></pre><p>Returns the broadcasted shape for input shapes or extent tensors. The rest
of this description is simplified for the 2 input case but can be extended
to more inputs. Both operands can be of type <code>shape.shape</code> or
<code>tensor&lt;?xindex></code>. The result is of type <code>shape.shape</code> and, if both
operands are tensors, may be of type <code>tensor&lt;?xindex></code>.</p><p>If the two operand shapes are of different rank the smaller one is padded
with 1&rsquo;s from the left. The resulting broadcasted shape is then defined as</p><pre><code>result[i] = lhs[i] if lhs[i] == rhs[i]
          = lhs[i] if rhs[i] == 1
          = rhs[i] if lhs[i] == 1.
</code></pre><p>In case the resulting shape is undefined, i.e. if corresponding extents are
different from each other but none is 1, the result is an error shape.
Likewise error values are propagated if any of the operands holds an error
value. If the result type is an extent tensor (and can therefore not hold
the error value) the behavior may be undefined. The optional string
attribute can be used to describe the error case.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>Commutative</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=attributes>Attributes:&nbsp;<a class=headline-hash href=#attributes>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>error</code></td><td>::mlir::StringAttr</td><td>string attribute</td></tr></table><h4 id=operands-5>Operands:&nbsp;<a class=headline-hash href=#operands-5>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>shapes</code></td><td>variadic of shape or extent tensor</td></tr></tbody></table><h4 id=results-4>Results:&nbsp;<a class=headline-hash href=#results-4>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapeconcat-shapeconcatop><code>shape.concat</code> (shape::ConcatOp)&nbsp;<a class=headline-hash href=#shapeconcat-shapeconcatop>¶</a></h3><p><em>Concatenates two shapes</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.concat` $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `-&gt;` type($result)
</code></pre><p>Creates a shape whose dimensions consist of first the dimensions from <code>lhs</code>
followed by the dimensions of <code>rhs</code>.</p><p>Example:
concat([2,3], [4,5]) -> [2,3,4,5]
concat([], []) -> []
concat([], [4,5,6]) -> [4,5,6]</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-6>Operands:&nbsp;<a class=headline-hash href=#operands-6>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>lhs</code></td><td>shape or extent tensor</td></tr><tr><td style=text-align:center><code>rhs</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-5>Results:&nbsp;<a class=headline-hash href=#results-5>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapeconst_shape-shapeconstshapeop><code>shape.const_shape</code> (shape::ConstShapeOp)&nbsp;<a class=headline-hash href=#shapeconst_shape-shapeconstshapeop>¶</a></h3><p><em>Creates a constant shape or extent tensor</em></p><p>Creates a constant shape or extent tensor. The individual extents are given
as the <code>shape</code> attribute. The number of these values equals the shape&rsquo;s
rank.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%0</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[]</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl><span class=nv>%1</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>2</span><span class=p>,</span> <span class=m>3</span><span class=p>]</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl><span class=nv>%2</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[</span><span class=m>4</span><span class=p>,</span> <span class=m>5</span><span class=p>,</span> <span class=m>6</span><span class=p>]</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>3x</span><span class=k>index</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>ConstantLike</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=attributes-1>Attributes:&nbsp;<a class=headline-hash href=#attributes-1>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>shape</code></td><td>::mlir::DenseIntElementsAttr</td><td>index elements attribute</td></tr></table><h4 id=results-6>Results:&nbsp;<a class=headline-hash href=#results-6>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapeconst_size-shapeconstsizeop><code>shape.const_size</code> (shape::ConstSizeOp)&nbsp;<a class=headline-hash href=#shapeconst_size-shapeconstsizeop>¶</a></h3><p><em>Creates a constant of type <code>shape.size</code></em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.const_size` $value attr-dict
</code></pre><p>Creates a <code>shape.size</code> type representing the constant size given by <code>value</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%x</span> <span class=p>=</span> shape<span class=p>.</span>const_size <span class=m>10</span>
</span></span></code></pre></div><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>ConstantLike</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code>, <code>OpAsmOpInterface</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=attributes-2>Attributes:&nbsp;<a class=headline-hash href=#attributes-2>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>value</code></td><td>::mlir::IntegerAttr</td><td>index attribute</td></tr></table><h4 id=results-7>Results:&nbsp;<a class=headline-hash href=#results-7>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td></td></tr></tbody></table><h3 id=shapeconst_witness-shapeconstwitnessop><code>shape.const_witness</code> (shape::ConstWitnessOp)&nbsp;<a class=headline-hash href=#shapeconst_witness-shapeconstwitnessop>¶</a></h3><p><em>An operation that returns a statically known witness value</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.const_witness` $passing attr-dict
</code></pre><p>This operation represents a statically known witness result. This can be
often used to canonicalize/fold constraint and assuming code that will always
pass.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%0</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>,</span><span class=m>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nv>%1</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>,</span><span class=m>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nv>%w0</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq<span class=p>(</span><span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span><span class=p>)</span> <span class=c>// Can be folded to &#34;const_witness true&#34;
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>const_witness true
</span></span><span class=line><span class=cl><span class=nv>%w2</span> <span class=p>=</span> shape<span class=p>.</span>assuming_all<span class=p>(</span><span class=nv>%w0</span><span class=p>,</span> <span class=nv>%w2</span><span class=p>)</span> <span class=c>// Can be folded to &#34;const_witness true&#34;
</span></span></span></code></pre></div><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>ConstantLike</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=attributes-3>Attributes:&nbsp;<a class=headline-hash href=#attributes-3>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>passing</code></td><td>::mlir::BoolAttr</td><td>bool attribute</td></tr></table><h4 id=results-8>Results:&nbsp;<a class=headline-hash href=#results-8>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td></td></tr></tbody></table><h3 id=shapecstr_broadcastable-shapecstrbroadcastableop><code>shape.cstr_broadcastable</code> (shape::CstrBroadcastableOp)&nbsp;<a class=headline-hash href=#shapecstr_broadcastable-shapecstrbroadcastableop>¶</a></h3><p><em>Determines if 2+ shapes can be successfully broadcasted</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.cstr_broadcastable` $shapes attr-dict `:` type($shapes)
</code></pre><p>Given input shapes or extent tensors, return a witness specifying if they
are broadcastable. This broadcastable follows the same logic as what
shape.broadcast documents.</p><p>&ldquo;cstr&rdquo; operations represent runtime assertions.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%w0</span> <span class=p>=</span> shape<span class=p>.</span>cstr_broadcastable <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>3</span><span class=p>,</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Passing
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>cstr_broadcastable <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>3</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Failure
</span></span></span></code></pre></div><p>Traits: <code>Commutative</code></p><p>Interfaces: <code>InferTypeOpInterface</code></p><h4 id=operands-7>Operands:&nbsp;<a class=headline-hash href=#operands-7>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>shapes</code></td><td>variadic of shape or extent tensor</td></tr></tbody></table><h4 id=results-9>Results:&nbsp;<a class=headline-hash href=#results-9>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td></td></tr></tbody></table><h3 id=shapecstr_eq-shapecstreqop><code>shape.cstr_eq</code> (shape::CstrEqOp)&nbsp;<a class=headline-hash href=#shapecstr_eq-shapecstreqop>¶</a></h3><p><em>Determines if all input shapes are equal</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.cstr_eq` $shapes attr-dict `:` type($shapes)
</code></pre><p>Given 1 or more input shapes, determine if all shapes are the exact same.</p><p>&ldquo;cstr&rdquo; operations represent runtime assertions.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%w0</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Passing
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span> <span class=c>// Failure
</span></span></span></code></pre></div><p>Traits: <code>Commutative</code></p><p>Interfaces: <code>InferTypeOpInterface</code></p><h4 id=operands-8>Operands:&nbsp;<a class=headline-hash href=#operands-8>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>shapes</code></td><td>variadic of shape or extent tensor</td></tr></tbody></table><h4 id=results-10>Results:&nbsp;<a class=headline-hash href=#results-10>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td></td></tr></tbody></table><h3 id=shapecstr_require-shapecstrrequireop><code>shape.cstr_require</code> (shape::CstrRequireOp)&nbsp;<a class=headline-hash href=#shapecstr_require-shapecstrrequireop>¶</a></h3><p><em>Represents a runtime assertion that an i1 is <code>true</code></em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.cstr_require` $pred `,` $msg attr-dict
</code></pre><p>Represents a runtime assertion that an i1 is true. It returns a
!shape.witness to order this assertion.</p><p>For simplicity, prefer using other cstr_* ops if they are available for a
given constraint.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%bool</span> <span class=p>=</span> <span class=p>...</span>
</span></span><span class=line><span class=cl><span class=nv>%w0</span> <span class=p>=</span> shape<span class=p>.</span>cstr_require <span class=nv>%bool</span><span class=p>,</span> <span class=s>&#34;msg&#34;</span> <span class=c>// Passing if `%bool` is true.
</span></span></span></code></pre></div><p>Since this op can be used to express many different possible assertions
(depending on whatever computation calculated <code>pred</code>), the <code>msg</code>
should clarify the nature of the assertion for users.</p><p>Interfaces: <code>InferTypeOpInterface</code></p><h4 id=attributes-4>Attributes:&nbsp;<a class=headline-hash href=#attributes-4>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>msg</code></td><td>::mlir::StringAttr</td><td>string attribute</td></tr></table><h4 id=operands-9>Operands:&nbsp;<a class=headline-hash href=#operands-9>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>pred</code></td><td>1-bit signless integer</td></tr></tbody></table><h4 id=results-11>Results:&nbsp;<a class=headline-hash href=#results-11>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td></td></tr></tbody></table><h3 id=shapedebug_print-shapedebugprintop><code>shape.debug_print</code> (shape::DebugPrintOp)&nbsp;<a class=headline-hash href=#shapedebug_print-shapedebugprintop>¶</a></h3><p><em>Prints the input shape or size</em></p><p>Prints the input dim or shape and passes through input.</p><p>Note: This is intended for testing and debugging only.</p><h4 id=operands-10>Operands:&nbsp;<a class=headline-hash href=#operands-10>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>shape or size</td></tr></tbody></table><h4 id=results-12>Results:&nbsp;<a class=headline-hash href=#results-12>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>output</code></td><td>shape or size</td></tr></tbody></table><h3 id=shapedim-shapedimop><code>shape.dim</code> (shape::DimOp)&nbsp;<a class=headline-hash href=#shapedim-shapedimop>¶</a></h3><p><em>Gets the specified extent from the shape of a shaped input</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.dim` $value `,` $index attr-dict `:` type($value) `,`type($index) `-&gt;` type($extent)
</code></pre><p>Gets the extent indexed by <code>dim</code> from the shape of the <code>value</code> operand. If
the index is error or out-of-bound then it returns an invalid size if the
return type carries error information else the behavior is undefined.</p><p>This is a convenience op that performs the equivalent of getting the extent
of a shape (e.g., <code>dim(x, i) == get_extent(shape_of(x), i)</code>).</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-11>Operands:&nbsp;<a class=headline-hash href=#operands-11>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>value</code></td><td>shaped of any type values</td></tr><tr><td style=text-align:center><code>index</code></td><td>size or index</td></tr></tbody></table><h4 id=results-13>Results:&nbsp;<a class=headline-hash href=#results-13>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>extent</code></td><td>size or index</td></tr></tbody></table><h3 id=shapediv-shapedivop><code>shape.div</code> (shape::DivOp)&nbsp;<a class=headline-hash href=#shapediv-shapedivop>¶</a></h3><p><em>Division of sizes and indices</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.div` $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `-&gt;` type($result)
</code></pre><p>Divides two sizes or indices. If either operand is an error it will be
propagated to the result. The operands can be of type <code>size</code> or <code>index</code>.
If at least one of the operands can hold an error, i.e. if it is of type
<code>size</code>, the result must be of type <code>size</code>. If error propagation is not
possible because both operands are of type <code>index</code> then the result may be
of type <code>size</code> or <code>index</code>. If both operands and result are of type
<code>index</code>, their runtime values could be negative. The result is rounded
toward negative infinity, i.e. floor(lhs / rhs), such that</p><pre><code>div(lhs, rhs) * rhs + mod(lhs, rhs) = lhs
</code></pre><p>always holds. If any of the values is of type <code>size</code>, the behavior for
negative value is undefined.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-12>Operands:&nbsp;<a class=headline-hash href=#operands-12>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>lhs</code></td><td>size or index</td></tr><tr><td style=text-align:center><code>rhs</code></td><td>size or index</td></tr></tbody></table><h4 id=results-14>Results:&nbsp;<a class=headline-hash href=#results-14>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>size or index</td></tr></tbody></table><h3 id=shapefrom_extent_tensor-shapefromextenttensorop><code>shape.from_extent_tensor</code> (shape::FromExtentTensorOp)&nbsp;<a class=headline-hash href=#shapefrom_extent_tensor-shapefromextenttensorop>¶</a></h3><p><em>Creates a shape from a tensor of extents</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.from_extent_tensor` $input attr-dict `:` type($input)
</code></pre><p>Creates a shape from a 1D integral tensor of extents. The rank of the
resulting shape equals the number of elements in the tensor, and the
extents match the values of the elements.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-13>Operands:&nbsp;<a class=headline-hash href=#operands-13>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>1D tensor of index values</td></tr></tbody></table><h4 id=results-15>Results:&nbsp;<a class=headline-hash href=#results-15>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td></td></tr></tbody></table><h3 id=shapefrom_extents-shapefromextentsop><code>shape.from_extents</code> (shape::FromExtentsOp)&nbsp;<a class=headline-hash href=#shapefrom_extents-shapefromextentsop>¶</a></h3><p><em>Creates a shape from extents</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.from_extents` $extents attr-dict `:` type($extents)
</code></pre><p>Creates a shape from multiple SSA values representing the extents of
the shape.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// Rank 2 shape.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%s0</span> <span class=p>=</span> shape<span class=p>.</span>from_extents <span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span>
</span></span><span class=line><span class=cl><span class=c>// Rank 0 shape.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%s1</span> <span class=p>=</span> shape<span class=p>.</span>from_extents
</span></span></code></pre></div><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-14>Operands:&nbsp;<a class=headline-hash href=#operands-14>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>extents</code></td><td>variadic of size or index</td></tr></tbody></table><h4 id=results-16>Results:&nbsp;<a class=headline-hash href=#results-16>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>shape</code></td><td></td></tr></tbody></table><h3 id=shapefunc-shapefuncop><code>shape.func</code> (shape::FuncOp)&nbsp;<a class=headline-hash href=#shapefunc-shapefuncop>¶</a></h3><p><em>Shape function</em></p><p>An operation with a name containing a single <code>SSACFG</code> region which
represents a shape transfer function or helper function for shape transfer
function.</p><p>Traits: <code>AffineScope</code>, <code>AutomaticAllocationScope</code>, <code>IsolatedFromAbove</code></p><p>Interfaces: <code>ArgAndResultAttrsOpInterface</code>, <code>CallableOpInterface</code>, <code>FunctionOpInterface</code>, <code>OpAsmOpInterface</code>, <code>Symbol</code></p><h4 id=attributes-5>Attributes:&nbsp;<a class=headline-hash href=#attributes-5>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>sym_name</code></td><td>::mlir::StringAttr</td><td>string attribute</td></tr><tr><td><code>function_type</code></td><td>::mlir::TypeAttr</td><td>type attribute of function type</td></tr><tr><td><code>arg_attrs</code></td><td>::mlir::ArrayAttr</td><td>Array of dictionary attributes</td></tr><tr><td><code>res_attrs</code></td><td>::mlir::ArrayAttr</td><td>Array of dictionary attributes</td></tr><tr><td><code>sym_visibility</code></td><td>::mlir::StringAttr</td><td>string attribute</td></tr></table><h3 id=shapefunction_library-shapefunctionlibraryop><code>shape.function_library</code> (shape::FunctionLibraryOp)&nbsp;<a class=headline-hash href=#shapefunction_library-shapefunctionlibraryop>¶</a></h3><p><em>Represents shape functions and corresponding ops</em></p><p>Represents a list of shape functions and the ops whose shape transfer
functions they represent.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>shape<span class=p>.</span><span class=kt>func</span>tion_library <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kt>func</span> <span class=nf>@same_result_shape</span><span class=p>(</span><span class=nv>%arg</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nv>%0</span> <span class=p>=</span> shape_of <span class=nv>%arg</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>    <span class=kt>return</span> <span class=nv>%0</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span> mapping <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>std.atan =</span> <span class=nf>@same_result_shape</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Traits: <code>AffineScope</code>, <code>IsolatedFromAbove</code>, <code>NoRegionArguments</code>, <code>NoTerminator</code>, <code>SingleBlock</code>, <code>SymbolTable</code></p><p>Interfaces: <code>OpAsmOpInterface</code>, <code>Symbol</code></p><h4 id=attributes-6>Attributes:&nbsp;<a class=headline-hash href=#attributes-6>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>sym_name</code></td><td>::mlir::StringAttr</td><td>string attribute</td></tr><tr><td><code>sym_visibility</code></td><td>::mlir::StringAttr</td><td>string attribute</td></tr><tr><td><code>mapping</code></td><td>::mlir::DictionaryAttr</td><td>dictionary of named attribute values</td></tr></table><h3 id=shapeget_extent-shapegetextentop><code>shape.get_extent</code> (shape::GetExtentOp)&nbsp;<a class=headline-hash href=#shapeget_extent-shapegetextentop>¶</a></h3><p><em>Gets the specified extent from a shape or extent tensor</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.get_extent` $shape `,` $dim attr-dict `:` type($shape) `,` type($dim) `-&gt;` type($extent)
</code></pre><p>Gets the extent indexed by <code>dim</code> from the <code>shape</code> operand. If the shape is
an error then it returns an invalid size.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-15>Operands:&nbsp;<a class=headline-hash href=#operands-15>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>shape</code></td><td>shape or extent tensor</td></tr><tr><td style=text-align:center><code>dim</code></td><td>size or index</td></tr></tbody></table><h4 id=results-17>Results:&nbsp;<a class=headline-hash href=#results-17>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>extent</code></td><td>size or index</td></tr></tbody></table><h3 id=shapeindex_to_size-shapeindextosizeop><code>shape.index_to_size</code> (shape::IndexToSizeOp)&nbsp;<a class=headline-hash href=#shapeindex_to_size-shapeindextosizeop>¶</a></h3><p><em>Converts a standard index to a shape size</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.index_to_size` $arg attr-dict
</code></pre><p>Converts a standard index to a <code>shape.size</code>. This operation and its
inverse, <code>size_to_index</code>, facilitate index conversion between the standard
and the shape dialect.</p><p>The behavior is undefined for negative indices.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-16>Operands:&nbsp;<a class=headline-hash href=#operands-16>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>arg</code></td><td>index</td></tr></tbody></table><h4 id=results-18>Results:&nbsp;<a class=headline-hash href=#results-18>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td></td></tr></tbody></table><h3 id=shapeis_broadcastable-shapeisbroadcastableop><code>shape.is_broadcastable</code> (shape::IsBroadcastableOp)&nbsp;<a class=headline-hash href=#shapeis_broadcastable-shapeisbroadcastableop>¶</a></h3><p><em>Determines if 2+ shapes can be successfully broadcasted</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.is_broadcastable` $shapes attr-dict `:` type($shapes)
</code></pre><p>Given multiple input shapes or extent tensors, return a predicate
specifying if they are broadcastable. This broadcastable follows the same
logic as what shape.broadcast documents.</p><p>Concretely, shape.is_broadcastable returning true implies that
shape.broadcast will not give an error, and shape.cstr_broadcastable will
not result in an assertion failure. Similarly, false implies an error or
assertion failure.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%true</span> <span class=p>=</span> shape<span class=p>.</span>is_broadcastable <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>3</span><span class=p>,</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nv>%false</span> <span class=p>=</span> shape<span class=p>.</span>is_broadcastable <span class=p>[</span><span class=m>2</span><span class=p>,</span><span class=m>2</span><span class=p>],</span> <span class=p>[</span><span class=m>3</span><span class=p>,</span><span class=m>2</span><span class=p>]</span>
</span></span></code></pre></div><p>Traits: <code>Commutative</code></p><p>Interfaces: <code>InferTypeOpInterface</code></p><h4 id=operands-17>Operands:&nbsp;<a class=headline-hash href=#operands-17>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>shapes</code></td><td>variadic of shape or extent tensor</td></tr></tbody></table><h4 id=results-19>Results:&nbsp;<a class=headline-hash href=#results-19>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>1-bit signless integer</td></tr></tbody></table><h3 id=shapemax-shapemaxop><code>shape.max</code> (shape::MaxOp)&nbsp;<a class=headline-hash href=#shapemax-shapemaxop>¶</a></h3><p><em>Elementwise maximum</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.max` $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `-&gt;` type($result)
</code></pre><p>Computes the elementwise maximum of two sizes or shapes with equal ranks.
If either operand is an error, then an error will be propagated to the
result. If the input types mismatch or the ranks do not match, then the
result is an error.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>Commutative</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-18>Operands:&nbsp;<a class=headline-hash href=#operands-18>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>lhs</code></td><td>shape or size</td></tr><tr><td style=text-align:center><code>rhs</code></td><td>shape or size</td></tr></tbody></table><h4 id=results-20>Results:&nbsp;<a class=headline-hash href=#results-20>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>shape or size</td></tr></tbody></table><h3 id=shapemeet-shapemeetop><code>shape.meet</code> (shape::MeetOp)&nbsp;<a class=headline-hash href=#shapemeet-shapemeetop>¶</a></h3><p><em>Returns the least general shape or size of its operands</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.meet` $arg0 `,` $arg1 (`,` `error` `=` $error^)? attr-dict `:`
              type($arg0) `,` type($arg1) `-&gt;` type($result)
</code></pre><p>An operation that computes the least general shape or dim of input operands.
This effectively asserts that corresponding static dimensions are equal.
The behavior is to match each element of the shape/size and propagate the
most restrictive information, returning an invalid shape if there are
contradictory requirements. E.g., using pseudo code</p><pre tabindex=0><code>shape.meet([*], [*]) -&gt; [*]
shape.meet([*], [1, ?]) -&gt; [1, ?]
shape.meet([1, 2], [1, ?]) -&gt; [1, 2]
shape.meet([*], [1, 2]) -&gt; [1, 2]
shape.meet([], []) -&gt; []
shape.meet([], [*]) -&gt; []
shape.meet([], [?, ?]) -&gt; [invalid]
shape.meet([1, ?], [2, ?, ?]) -&gt; [invalid]
</code></pre><p><code>shape.meet</code> also allows specifying an optional error string, that may be
used to return an error to the user upon mismatch of dimensions.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%c</span> <span class=p>=</span> shape<span class=p>.</span>meet <span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span><span class=p>,</span> <span class=nl>error=</span><span class=s>&#34;&lt;reason&gt;&#34;</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>shape <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span></code></pre></div><p>Traits: <code>Commutative</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>InferTypeOpInterface</code></p><h4 id=attributes-7>Attributes:&nbsp;<a class=headline-hash href=#attributes-7>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>error</code></td><td>::mlir::StringAttr</td><td>string attribute</td></tr></table><h4 id=operands-19>Operands:&nbsp;<a class=headline-hash href=#operands-19>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>arg0</code></td><td>any shape or size</td></tr><tr><td style=text-align:center><code>arg1</code></td><td>any shape or size</td></tr></tbody></table><h4 id=results-21>Results:&nbsp;<a class=headline-hash href=#results-21>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>any shape or size</td></tr></tbody></table><h3 id=shapemin-shapeminop><code>shape.min</code> (shape::MinOp)&nbsp;<a class=headline-hash href=#shapemin-shapeminop>¶</a></h3><p><em>Elementwise minimum</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.min` $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `-&gt;` type($result)
</code></pre><p>Computes the elementwise minimum of two sizes or shapes with equal ranks.
If either operand is an error, then an error will be propagated to the
result. If the input types mismatch or the ranks do not match, then the
result is an error.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>Commutative</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-20>Operands:&nbsp;<a class=headline-hash href=#operands-20>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>lhs</code></td><td>shape or size</td></tr><tr><td style=text-align:center><code>rhs</code></td><td>shape or size</td></tr></tbody></table><h4 id=results-22>Results:&nbsp;<a class=headline-hash href=#results-22>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>shape or size</td></tr></tbody></table><h3 id=shapemul-shapemulop><code>shape.mul</code> (shape::MulOp)&nbsp;<a class=headline-hash href=#shapemul-shapemulop>¶</a></h3><p><em>Multiplication of sizes and indices</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.mul` $lhs `,` $rhs attr-dict `:` type($lhs) `,` type($rhs) `-&gt;` type($result)
</code></pre><p>Multiplies two sizes or indices. If either operand is an error it will be
propagated to the result. The operands can be of type <code>size</code> or <code>index</code>. If
at least one of the operands can hold an error, i.e. if it is of type
<code>size</code>, the result must be of type <code>size</code>. If error propagation is not
possible because both operands are of type <code>index</code> then the result may be
of type <code>size</code> or <code>index</code>.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>Commutative</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-21>Operands:&nbsp;<a class=headline-hash href=#operands-21>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>lhs</code></td><td>size or index</td></tr><tr><td style=text-align:center><code>rhs</code></td><td>size or index</td></tr></tbody></table><h4 id=results-23>Results:&nbsp;<a class=headline-hash href=#results-23>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>size or index</td></tr></tbody></table><h3 id=shapenum_elements-shapenumelementsop><code>shape.num_elements</code> (shape::NumElementsOp)&nbsp;<a class=headline-hash href=#shapenum_elements-shapenumelementsop>¶</a></h3><p><em>Returns the number of elements for a given shape</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.num_elements` $shape attr-dict `:` type($shape) `-&gt;` type($result)
</code></pre><p>Returns the number of elements for a given shape which is the product of
its extents. If the argument is of type <code>shape</code> then the result will be of
type <code>size</code> and potential errors will be propagated. Otherwise, if the
argument is and extent tensor <code>tensor&lt;?xindex></code> then the result will be of
type <code>index</code>.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-22>Operands:&nbsp;<a class=headline-hash href=#operands-22>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>shape</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-24>Results:&nbsp;<a class=headline-hash href=#results-24>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>size or index</td></tr></tbody></table><h3 id=shaperank-shaperankop><code>shape.rank</code> (shape::RankOp)&nbsp;<a class=headline-hash href=#shaperank-shaperankop>¶</a></h3><p><em>Gets the rank of a shape</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.rank` $shape attr-dict `:` type($shape) `-&gt;` type($rank)
</code></pre><p>Returns the rank of the shape or extent tensor, i.e. the number of extents.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-23>Operands:&nbsp;<a class=headline-hash href=#operands-23>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>shape</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-25>Results:&nbsp;<a class=headline-hash href=#results-25>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>rank</code></td><td>size or index</td></tr></tbody></table><h3 id=shapereduce-shapereduceop><code>shape.reduce</code> (shape::ReduceOp)&nbsp;<a class=headline-hash href=#shapereduce-shapereduceop>¶</a></h3><p><em>Returns an expression reduced over a shape or extent tensor</em></p><p>An operation that takes as input a shape or extent tensor, and a number of
initial values. This operation has a region that is applied repeatedly for
every extent of the input. Starting with the initial values, the individual
extents are then aggregated as defined by the associated region.</p><p>Conceptually this op performs the following reduction:</p><pre tabindex=0><code>res[] = init;
for (int i = 0, i &lt; shape.rank(); i++) {
  res = reduce(i, shape[i], res[0], ..., res[n]);
}
</code></pre><p>Where <code>reduce</code> represents the region attached and the result of the reduce
op is the last computed output of the reduce region. As an example, the
number of elements can be computed as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=kt>func</span><span class=p>.</span><span class=kt>func</span> <span class=nf>@reduce</span><span class=p>(</span><span class=nv>%shape</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape<span class=p>,</span> <span class=nv>%init</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span>
</span></span><span class=line><span class=cl>    <span class=p>!</span>shape<span class=p>.</span>size <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%num_elements</span> <span class=p>=</span> shape<span class=p>.</span>reduce<span class=p>(</span><span class=nv>%shape</span><span class=p>,</span> <span class=nv>%init</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size  <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%index</span><span class=p>:</span> <span class=k>index</span><span class=p>,</span> <span class=nv>%dim</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>,</span> <span class=nv>%acc</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>):</span>
</span></span><span class=line><span class=cl>      <span class=nv>%updated_acc</span> <span class=p>=</span> <span class=s>&#34;shape.mul&#34;</span><span class=p>(</span><span class=nv>%acc</span><span class=p>,</span> <span class=nv>%dim</span><span class=p>)</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=p>(!</span>shape<span class=p>.</span>size<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl>      shape<span class=p>.</span>yield <span class=nv>%updated_acc</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=kt>return</span> <span class=nv>%num_elements</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Traits: <code>SingleBlockImplicitTerminator&lt;YieldOp></code>, <code>SingleBlock</code></p><h4 id=operands-24>Operands:&nbsp;<a class=headline-hash href=#operands-24>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>shape</code></td><td>shape or extent tensor</td></tr><tr><td style=text-align:center><code>initVals</code></td><td>variadic of any type</td></tr></tbody></table><h4 id=results-26>Results:&nbsp;<a class=headline-hash href=#results-26>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>variadic of any type</td></tr></tbody></table><h3 id=shapereturn-shapereturnop><code>shape.return</code> (shape::ReturnOp)&nbsp;<a class=headline-hash href=#shapereturn-shapereturnop>¶</a></h3><p><em>Shape function return operation</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.return` attr-dict ($operands^ `:` type($operands))?
</code></pre><p>The <code>shape.return</code> operation represents a return operation within a
function. The operation takes variable number of operands and produces no
results.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>HasParent&lt;FuncOp></code>, <code>ReturnLike</code>, <code>Terminator</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code>, <code>RegionBranchTerminatorOpInterface</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-25>Operands:&nbsp;<a class=headline-hash href=#operands-25>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>operands</code></td><td>variadic of any type</td></tr></tbody></table><h3 id=shapeshape_eq-shapeshapeeqop><code>shape.shape_eq</code> (shape::ShapeEqOp)&nbsp;<a class=headline-hash href=#shapeshape_eq-shapeshapeeqop>¶</a></h3><p><em>Returns whether the input shapes or extent tensors are equal</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.shape_eq` $shapes attr-dict `:` type($shapes)
</code></pre><p>Takes one or more shape or extent tensor operands and determines whether
they are equal. When extent tensors are compared to shapes they are
regarded as their equivalent non-error shapes. Error shapes can be tested
for equality like any other shape value, meaning that the error value is
equal to itself.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>Commutative</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-26>Operands:&nbsp;<a class=headline-hash href=#operands-26>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>shapes</code></td><td>variadic of shape or extent tensor</td></tr></tbody></table><h4 id=results-27>Results:&nbsp;<a class=headline-hash href=#results-27>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>1-bit signless integer</td></tr></tbody></table><h3 id=shapeshape_of-shapeshapeofop><code>shape.shape_of</code> (shape::ShapeOfOp)&nbsp;<a class=headline-hash href=#shapeshape_of-shapeshapeofop>¶</a></h3><p><em>Returns shape of a value or shaped type operand</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.shape_of` $arg attr-dict `:` type($arg) `-&gt;` type($result)
</code></pre><p>The operation takes a value or a shaped operand as an argument and it
returns a shape or extent tensor.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>InferTypeOpAdaptor</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-27>Operands:&nbsp;<a class=headline-hash href=#operands-27>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>arg</code></td><td>shaped of any type values or</td></tr></tbody></table><h4 id=results-28>Results:&nbsp;<a class=headline-hash href=#results-28>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapesize_to_index-shapesizetoindexop><code>shape.size_to_index</code> (shape::SizeToIndexOp)&nbsp;<a class=headline-hash href=#shapesize_to_index-shapesizetoindexop>¶</a></h3><p><em>Casts between index types of the shape and standard dialect</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.size_to_index` $arg attr-dict `:` type($arg)
</code></pre><p>Converts a <code>shape.size</code> to a standard index. This operation and its
inverse, <code>index_to_size</code>, facilitate index conversion between the standard
and the shape dialect. The behavior is undefined for unknown and invalid
arguments.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>CastOpInterface</code>, <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-28>Operands:&nbsp;<a class=headline-hash href=#operands-28>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>arg</code></td><td>size or index</td></tr></tbody></table><h4 id=results-29>Results:&nbsp;<a class=headline-hash href=#results-29>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>index</td></tr></tbody></table><h3 id=shapesplit_at-shapesplitatop><code>shape.split_at</code> (shape::SplitAtOp)&nbsp;<a class=headline-hash href=#shapesplit_at-shapesplitatop>¶</a></h3><p><em>Splits a shape at a given index</em></p><p>Splits a shape at a given dimension <code>index</code>, returning two shapes. If
<code>index</code> is negative, it is treated as indexing from the back of the shape.
This negative-handling behavior is important when handling unranked shapes,
where the positive index is not necessarily knowable due to a dynamic
number of leading dimensions. If the result is in extent tensor form out of
bounds indices result in undefined behavior.</p><p>Examples:</p><ul><li>split_at([4,5,6], index=0) -> [], [4,5,6]</li><li>split_at([4,5,6], index=1) -> [4], [5,6]</li><li>split_at([4,5,6], index=2) -> [4,5], [6]</li><li>split_at([4,5,6], index=3) -> [4,5,6], []</li><li>split_at([4,5,6], index=4) -> error</li><li>split_at([4,5,6], index=-1) -> [4,5], [6]</li><li>split_at([4,5,6], index=-2) -> [4], [5,6]</li><li>split_at([4,5,6], index=-3) -> [], [4,5,6]</li><li>split_at([4,5,6], index=-4) -> error</li></ul><p>Requires:</p><ul><li><code>index</code> is in the range [-rank(operand),rank(operand)]</li></ul><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-29>Operands:&nbsp;<a class=headline-hash href=#operands-29>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>operand</code></td><td>shape or extent tensor</td></tr><tr><td style=text-align:center><code>index</code></td><td>size or index</td></tr></tbody></table><h4 id=results-30>Results:&nbsp;<a class=headline-hash href=#results-30>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>head</code></td><td>shape or extent tensor</td></tr><tr><td style=text-align:center><code>tail</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapeto_extent_tensor-shapetoextenttensorop><code>shape.to_extent_tensor</code> (shape::ToExtentTensorOp)&nbsp;<a class=headline-hash href=#shapeto_extent_tensor-shapetoextenttensorop>¶</a></h3><p><em>Creates a dimension tensor from a shape</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.to_extent_tensor` $input attr-dict `:` type($input) `-&gt;` type($result)
</code></pre><p>Converts a shape to a 1D integral tensor of extents. The number of elements
in the tensor equals the rank of the shape, and the elements equal the
extents of the shape.</p><p>If the shape represents an error, this op&rsquo;s behavior is undefined.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>CastOpInterface</code>, <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-30>Operands:&nbsp;<a class=headline-hash href=#operands-30>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>input</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-31>Results:&nbsp;<a class=headline-hash href=#results-31>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>tensor of index values</td></tr></tbody></table><h3 id=shapevalue_as_shape-shapevalueasshapeop><code>shape.value_as_shape</code> (shape::ValueAsShapeOp)&nbsp;<a class=headline-hash href=#shapevalue_as_shape-shapevalueasshapeop>¶</a></h3><p><em>Returns value as a shape</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.value_as_shape` $arg attr-dict `:` type($arg) `-&gt;` type($result)
</code></pre><p>The operations takes a ValueShape and returns a Shape corresponding to the
value. If the input value cannot be shape (e.g., not a 1D tensor of
integral value representing sizes) then this propagages the error shape.
E.g.,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// The following
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%0</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> dense<span class=p>&lt;[</span><span class=m>1</span><span class=p>,</span><span class=m>2</span><span class=p>]&gt;</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl><span class=nv>%shape</span> <span class=p>=</span> shape<span class=p>.</span>value_as_shape <span class=nv>%0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i32</span><span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl><span class=c>// is equivalent to
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%shape</span><span class=err>&#39;</span> <span class=p>=</span> shape<span class=p>.</span>const_shape <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>2</span><span class=p>]</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span></code></pre></div><p>This operation is the complement of <code>shape_of</code> wrt ValueShape values.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-31>Operands:&nbsp;<a class=headline-hash href=#operands-31>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>arg</code></td><td>1D tensor of integer or index values or</td></tr></tbody></table><h4 id=results-32>Results:&nbsp;<a class=headline-hash href=#results-32>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>shape or extent tensor</td></tr></tbody></table><h3 id=shapevalue_of-shapevalueofop><code>shape.value_of</code> (shape::ValueOfOp)&nbsp;<a class=headline-hash href=#shapevalue_of-shapevalueofop>¶</a></h3><p><em>Returns value of a !shape.value_shape operand</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.value_of` $arg attr-dict `:` type($result)
</code></pre><p>The operation takes !shape.value_shape, a.k.a. (value, shape) tuple as an
argument, and returns its value. The behavior is undefined for unknown and
invalid arguments.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-32>Operands:&nbsp;<a class=headline-hash href=#operands-32>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>arg</code></td><td></td></tr></tbody></table><h4 id=results-33>Results:&nbsp;<a class=headline-hash href=#results-33>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td>shaped of any type values</td></tr></tbody></table><h3 id=shapewith_shape-shapewithop><code>shape.with_shape</code> (shape::WithOp)&nbsp;<a class=headline-hash href=#shapewith_shape-shapewithop>¶</a></h3><p><em>Returns ValueShape with given shape</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.with_shape` operands attr-dict `:` type($operand) `,` type($shape)
</code></pre><p>Returns ValueShape with the shape updated to match the shape operand. That
is a new ValueShape tuple is created with value equal to <code>operand</code>&rsquo;s
value and shape equal to <code>shape</code>. If the ValueShape and given <code>shape</code> are
non-conformant, then the returned ValueShape will represent an error of
this mismatch. Similarly if either inputs are in an error state, then an
error is propagated.</p><p>Usage:
%0 = shape.with_shape %1, %2 : tensor&lt;&mldr;>, !shape.shape</p><p>This is used, for example, where one combines shape function calculations
and/or call one shape function from another. E.g.,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=kt>func</span><span class=p>.</span><span class=kt>func</span> <span class=nf>@shape_foobah</span><span class=p>(</span><span class=nv>%a</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>,</span>
</span></span><span class=line><span class=cl>                   <span class=nv>%b</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>,</span>
</span></span><span class=line><span class=cl>                   <span class=nv>%c</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%0</span> <span class=p>=</span> call <span class=nf>@shape_foo</span><span class=p>(</span><span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span><span class=p>)</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=p>(!</span>shape<span class=p>.</span>value_shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>  <span class=nv>%1</span> <span class=p>=</span> shape<span class=p>.</span>with_shape <span class=nv>%b</span><span class=p>,</span> <span class=nv>%0</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>  <span class=nv>%2</span> <span class=p>=</span> call <span class=nf>@shape_bah</span><span class=p>(</span><span class=nv>%c</span><span class=p>,</span> <span class=nv>%1</span><span class=p>)</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=p>(!</span>shape<span class=p>.</span>value_shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>  <span class=kt>return</span> <span class=nv>%2</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>This op need not be a refinement of the shape. In non-error cases the input
ValueShape&rsquo;s value and shape are conformant and so too for the output, but
the result may be less specified than <code>operand</code>&rsquo;s shape as <code>shape</code> is
merely used to construct the new ValueShape. If join behavior is desired
then a join op should be used.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-33>Operands:&nbsp;<a class=headline-hash href=#operands-33>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>operand</code></td><td>shaped of any type values or</td></tr><tr><td style=text-align:center><code>shape</code></td><td>shape or extent tensor</td></tr></tbody></table><h4 id=results-34>Results:&nbsp;<a class=headline-hash href=#results-34>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>result</code></td><td></td></tr></tbody></table><h3 id=shapeyield-shapeyieldop><code>shape.yield</code> (shape::YieldOp)&nbsp;<a class=headline-hash href=#shapeyield-shapeyieldop>¶</a></h3><p><em>Returns the value to parent op</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `shape.yield` attr-dict ($operands^ `:` type($operands))?
</code></pre><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>HasParent&lt;ReduceOp, FunctionLibraryOp></code>, <code>ReturnLike</code>, <code>Terminator</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code>, <code>RegionBranchTerminatorOpInterface</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=operands-34>Operands:&nbsp;<a class=headline-hash href=#operands-34>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>operands</code></td><td>variadic of any type</td></tr></tbody></table><h2 id=types>Types&nbsp;<a class=headline-hash href=#types>¶</a></h2><h3 id=shapetype>ShapeType&nbsp;<a class=headline-hash href=#shapetype>¶</a></h3><p>Syntax: <code>!shape.shape</code></p><p><code>shape.shape</code> represents either an unranked shape, a ranked shape with
possibly unknown dimensions or an invalid shape. The rank is of type
<code>shape.size</code> and, if rank is known, the extent is a 1D tensor of type
<code>shape.size</code>.</p><p>Shape is printed:</p><ul><li><code>[*]</code> if it is an unranked shape</li><li><code>[?, 2]</code> if a rank 2 tensor with one unknown dimension</li><li><code>[3, 4]</code> is a rank 2 static tensor</li><li><code>[]</code> is a scalar</li><li><code>[1]</code> is a rank 1 tensor with 1 element</li><li><code>[invalid]</code> for an invalid shape</li></ul><h3 id=sizetype>SizeType&nbsp;<a class=headline-hash href=#sizetype>¶</a></h3><p>Syntax: <code>!shape.size</code></p><p><code>shape.size</code> represents a non-negative integer with support for being
unknown and invalid.</p><p>Operations on <code>shape.size</code> types are specialized to handle unknown/dynamic
value. So, for example, <code>&lt;unknown> + x == &lt;unknown></code> for all non-error <code>x : !shape.size</code> (e.g., an unknown value does not become known due to addition).</p><h3 id=valueshapetype>ValueShapeType&nbsp;<a class=headline-hash href=#valueshapetype>¶</a></h3><p>Syntax: <code>!shape.value_shape</code></p><p><code>shape.value_shape</code> represents the value produced by an operation (this
corresponds to <code>Value</code> in the compiler) and a shape. Conceptually this is a
tuple of a value (potentially unknown) and <code>shape.shape</code>. The value and
shape can either or both be unknown. If both the <code>value</code> and <code>shape</code> are
known, then the shape of <code>value</code> is conformant with <code>shape</code>. That is, the
shape of the value conforms to the shape of the ValueShape, so that if we
have ValueShape <code>(value, shape)</code> then <code>join(shape_of(value), shape)</code> would
be error free and in particular it means that if both are statically known,
then they are equal.</p><h3 id=witnesstype>WitnessType&nbsp;<a class=headline-hash href=#witnesstype>¶</a></h3><p>Syntax: <code>!shape.witness</code></p><p>A witness is a structural device in the compiler to maintain ordering of
code relying on information obtained from passing assertions. Witnesses do
not represent any physical data.</p><p>&ldquo;cstr_&rdquo; operations will return witnesses and be lowered into assertion logic
when not resolvable at compile time.</p><p>&ldquo;assuming_&rdquo; operations will take witnesses as input and represent only
information to the compiler, so they do not exist in executing code. Code
that is dependent on &ldquo;assuming_&rdquo; operations can assume all cstr operations
transitively before are honored as true.</p><p>These abstractions are intended to allow the compiler more freedom with
assertions by merely showing the assertion through dataflow at this time
rather than a side effecting operation that acts as a barrier. This can be
viewed similarly to a compiler representation of promises from asynchronous,
possibly crashing assertions. Reliant code will not be reordered to before
the code and non-reliant code can be reordered freely, and there are no
guarantees on the final ordering of the assertions or their related code.</p><h2 id=different-stages-of-lowering-shape-dialect>Different stages of lowering Shape dialect&nbsp;<a class=headline-hash href=#different-stages-of-lowering-shape-dialect>¶</a></h2><p>In this section we shall give a brief overview of the different uses of the
shape dialect and the lowering between these uses. Currently we have 3 worlds /
stages of lowering of shape functions:</p><ol><li><p><em>Error monadic/error carrying/user specification</em>:
This &ldquo;input&rdquo; form carries both the shape and whether in error state as
value. Hence at this level all operations are pure operations producing and
consuming values where the values could represent an error.</p></li><li><p><em>Constrained</em>:
This form uses a variant of explicit evidence passing to allow leveraging
existing compiler infrastructure to preserve safety information during
optimization.</p></li><li><p><em>Side-effecting/asserting</em>:
This final lowered form is imperative form with side-effecting ops (e.g.,
assert) for final codegen.</p></li></ol><p>We are going to do a quick step through of the lowering using the example of
a matmul.</p><p>Starting from the shape function of matmul in the error monadic form
below<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>shape<span class=p>.</span><span class=kt>func</span>tion_library <span class=nf>@shplib</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kt>func</span><span class=p>.</span><span class=kt>func</span> <span class=nf>@matmul</span><span class=p>(</span><span class=nv>%lhs</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>,</span> <span class=nv>%rhs</span><span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%c1</span> <span class=p>=</span> shape<span class=p>.</span>const_size <span class=m>1</span>
</span></span><span class=line><span class=cl>  <span class=nv>%c2</span> <span class=p>=</span> shape<span class=p>.</span>const_size <span class=m>2</span>
</span></span><span class=line><span class=cl>  <span class=c>// We could also allow rank etc operations directly on value_shape too, that
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// would make it nicer as &#34;input&#34; language, but keeping it explicit inside the
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// IR instead and then we could have helper methods in front-end language.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%lhs_shape</span> <span class=p>=</span> shape<span class=p>.</span>shape_of <span class=nv>%lhs</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>  <span class=nv>%rhs_shape</span> <span class=p>=</span> shape<span class=p>.</span>shape_of <span class=nv>%rhs</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>value_shape <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>  <span class=nv>%lhs_rank</span> <span class=p>=</span> shape<span class=p>.</span>rank <span class=nv>%lhs_shape</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl>  <span class=nv>%rhs_rank</span> <span class=p>=</span> shape<span class=p>.</span>rank <span class=nv>%rhs_shape</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl>  <span class=c>// This is not minimal as one could ensure the ranks are the same below, also a
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// variadic meet would make it more concise too.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%r</span> <span class=p>=</span> <span class=s>&#34;shape.meet&#34;</span><span class=p>(</span><span class=nv>%lhs_rank</span><span class=p>,</span> <span class=nv>%rhs_rank</span><span class=p>)</span> <span class=p>:</span> <span class=p>(!</span>shape<span class=p>.</span>size<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl>  <span class=nv>%rank</span> <span class=p>=</span> shape<span class=p>.</span>meet <span class=nv>%c2</span><span class=p>,</span> <span class=nv>%r</span><span class=p>,</span> <span class=nl>error=</span><span class=s>&#34;requires rank 2 operands&#34;</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=p>!</span>shape<span class=p>.</span>size<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl>  <span class=nv>%l0</span><span class=p>,</span> <span class=nv>%l1</span> <span class=p>=</span> <span class=s>&#34;shape.split_at&#34;</span><span class=p>(</span><span class=nv>%lhs_shape</span><span class=p>,</span> <span class=nv>%c1</span><span class=p>)</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=p>(!</span>shape<span class=p>.</span>shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(!</span>shape<span class=p>.</span>shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>shape<span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nv>%r0</span><span class=p>,</span> <span class=nv>%r1</span> <span class=p>=</span> <span class=s>&#34;shape.split_at&#34;</span><span class=p>(</span><span class=nv>%rhs_shape</span><span class=p>,</span> <span class=nv>%c1</span><span class=p>)</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=p>(!</span>shape<span class=p>.</span>shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(!</span>shape<span class=p>.</span>shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>shape<span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nv>%c</span> <span class=p>=</span> shape<span class=p>.</span>meet <span class=nv>%l1</span><span class=p>,</span> <span class=nv>%r0</span><span class=p>,</span> <span class=nl>error=</span><span class=s>&#34;inner dimensions required to match&#34;</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=p>!</span>shape<span class=p>.</span>shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>shape <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>  <span class=nv>%res</span> <span class=p>=</span> shape<span class=p>.</span>concat <span class=nv>%l0</span><span class=p>,</span> <span class=nv>%r1</span>
</span></span><span class=line><span class=cl>  <span class=c>// Should have `shape.return %res requires %c, %rank` to enable
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=kt>return</span> <span class=nv>%res</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=p>}</span> mapping <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>foo.matmul =</span> <span class=nf>@matmul</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><ul><li><p>We are using the default builtin func and return here. Preferably we&rsquo;d use
‘shape_func’ as a special function op that allows passing multiple results
back that affect correct execution (e.g., serves as an error join)</p><ul><li>This would also means one can&rsquo;t reify it inside a regular function
without handling the shape.return - that is a feature here as these are
more of a template.</li><li>Currently we also have not marked <code>meet</code> as having no side-effects to
avoid DCE until we have <code>shape.return</code>, at which point computing the
meet could be treated as purely computational returning error.</li></ul></li><li><p>Meet represents a constraint that should hold, so should not be used to see
<em>if</em> something is equal. E.g., this means <code>meet</code> can&rsquo;t be used to represent</p><pre tabindex=0><code>   either(meet(x, y), meet(y,z))
</code></pre></li><li><p>This could have been written more concisely as something like</p><pre tabindex=0><code>  concat(lhs[0], rhs[1]) if rank(lhs) == 2 &amp;&amp;
    rank(rhs) == 2 &amp;&amp; lhs[1] == rhs[0]
</code></pre><p>but not focusing on front-end proper here.</p></li></ul><p>We are going to lower to &ldquo;most&rdquo; nested form directly (see
<a href=https://github.com/tensorflow/tensorflow/blob/64062b5c51e04e370df26551d247496787d3f5c2/tensorflow/compiler/mlir/xla/tests/legalize-tf.mlir#L3088>test</a>
for an example reification along with legalization). In the above this was in a
separate shape function library, while here we would normally reify it as part
of lowering, but for simplicity will show as a standalone shape function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=kt>func</span><span class=p>.</span><span class=kt>func</span> <span class=nf>@matmul_shape1</span><span class=p>(</span><span class=nv>%lhs</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;,</span> <span class=nv>%rhs</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xindex<span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%c1</span> <span class=p>=</span> shape<span class=p>.</span>const_size <span class=m>1</span>
</span></span><span class=line><span class=cl>  <span class=nv>%c2</span> <span class=p>=</span> shape<span class=p>.</span>const_size <span class=m>2</span>
</span></span><span class=line><span class=cl>  <span class=c>// We allow `shape.shape_of` to return either a `!shape.shape` or
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// `tensor&lt;?xindex&gt;` type, in the case where the input is a tensor the most
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// refined type is a tensor of `index` but not required.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%lhs_shape</span> <span class=p>=</span> shape<span class=p>.</span>shape_of <span class=nv>%lhs</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>  <span class=nv>%rhs_shape</span> <span class=p>=</span> shape<span class=p>.</span>shape_of <span class=nv>%rhs</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>  <span class=nv>%lhs_rank</span> <span class=p>=</span> shape<span class=p>.</span>rank <span class=nv>%lhs_shape</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl>  <span class=nv>%rhs_rank</span> <span class=p>=</span> shape<span class=p>.</span>rank <span class=nv>%rhs_shape</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>shape <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl>  <span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=nv>%lhs_rank</span><span class=p>,</span> <span class=nv>%rhs_rank</span> <span class=p>:</span> <span class=p>!</span>shape<span class=p>.</span>witness
</span></span><span class=line><span class=cl>  <span class=nv>%res</span> <span class=p>=</span> shape<span class=p>.</span>assuming <span class=nv>%w1</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nv>%r1</span> <span class=p>=</span> shape<span class=p>.</span>any <span class=nv>%lhs_rank</span><span class=p>,</span> <span class=nv>%rhs_rank</span> <span class=p>:</span> <span class=p>(!</span>shape<span class=p>.</span>size<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl>    <span class=c>// Error message needs an addition, currently only on cstr_require.
</span></span></span><span class=line><span class=cl><span class=c></span>    <span class=nv>%w2</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=nv>%c2</span><span class=p>,</span> <span class=nv>%r1</span><span class=p>,</span> <span class=nl>error=</span><span class=s>&#34;requires rank 2 operands&#34;</span>
</span></span><span class=line><span class=cl>    <span class=nv>%res_1</span> <span class=p>=</span> shape<span class=p>.</span>assuming <span class=nv>%w2</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=c>// Here the lowered
</span></span></span><span class=line><span class=cl><span class=c></span>      <span class=c>//   %rank = shape.any %c2, %r1 (!shape.size, !shape.size) -&gt; !shape.size
</span></span></span><span class=line><span class=cl><span class=c></span>      <span class=c>// is dead and so elided further. But if `%rank` was actually consumed,
</span></span></span><span class=line><span class=cl><span class=c></span>      <span class=c>// then it could have been folded in `shape.any`.
</span></span></span><span class=line><span class=cl><span class=c></span>      <span class=nv>%l0</span><span class=p>,</span> <span class=nv>%r0</span> <span class=p>=</span> <span class=s>&#34;shape.split_at&#34;</span><span class=p>(</span><span class=nv>%lhs_shape</span><span class=p>,</span> <span class=nv>%c1</span><span class=p>)</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=p>(!</span>shape<span class=p>.</span>shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>      <span class=nv>%l1</span><span class=p>,</span> <span class=nv>%r1</span> <span class=p>=</span> <span class=s>&#34;shape.split_at&#34;</span><span class=p>(</span><span class=nv>%lhs_shape</span><span class=p>,</span> <span class=nv>%c1</span><span class=p>)</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=p>(!</span>shape<span class=p>.</span>shape<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>shape
</span></span><span class=line><span class=cl>      <span class=nv>%c</span> <span class=p>=</span> shape<span class=p>.</span>meet <span class=nv>%l1</span><span class=p>,</span> <span class=nv>%r0</span><span class=p>,</span> <span class=nl>error=</span><span class=s>&#34;inner dimensions required to match&#34;</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=p>!</span>shape<span class=p>.</span>size<span class=p>,</span> <span class=p>!</span>shape<span class=p>.</span>size <span class=p>-&gt;</span> <span class=p>!</span>shape<span class=p>.</span>size
</span></span><span class=line><span class=cl>      <span class=nv>%res</span> <span class=p>=</span> concat<span class=p>(</span><span class=nv>%l0</span><span class=p>,</span> <span class=nv>%r1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      shape<span class=p>.</span>assuming_yield <span class=nv>%res</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    shape<span class=p>.</span>assuming_yield <span class=nv>%res_1</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=kt>return</span> <span class=nv>%res</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We can now hoist computations of constraint were possible (which in the case
below is not too many as we need to verify the rank before we can split)</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=kt>func</span><span class=p>.</span><span class=kt>func</span> <span class=nf>@matmul_shape2</span><span class=p>(</span><span class=nv>%lhs</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;,</span> <span class=nv>%lhs</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%c1</span> <span class=p>=</span> shape<span class=p>.</span>const_size <span class=m>1</span>
</span></span><span class=line><span class=cl>  <span class=nv>%c2</span> <span class=p>=</span> shape<span class=p>.</span>const_size <span class=m>2</span>
</span></span><span class=line><span class=cl>  <span class=nv>%lhs_shape</span> <span class=p>=</span> shape<span class=p>.</span>shape_of <span class=nv>%lhs</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%rhs_shape</span> <span class=p>=</span> shape<span class=p>.</span>shape_of <span class=nv>%rhs</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%lhs_rank</span> <span class=p>=</span> shape<span class=p>.</span>rank <span class=nv>%lhs_shape</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%rhs_rank</span> <span class=p>=</span> shape<span class=p>.</span>rank <span class=nv>%rhs_shape</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=nv>%c2</span><span class=p>,</span> <span class=nv>%lhs_rank</span><span class=p>,</span> <span class=nl>error=</span><span class=s>&#34;requires rank 2 operands&#34;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%w2</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=nv>%c2</span><span class=p>,</span> <span class=nv>%rhs_rank</span><span class=p>,</span> <span class=nl>error=</span><span class=s>&#34;requires rank 2 operands&#34;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%w</span> <span class=p>=</span> shape<span class=p>.</span>assuming_all <span class=nv>%w1</span><span class=p>,</span> <span class=nv>%w2</span>
</span></span><span class=line><span class=cl>  <span class=nv>%res</span> <span class=p>=</span> shape<span class=p>.</span>assuming <span class=nv>%w</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nv>%l0</span><span class=p>,</span> <span class=nv>%r0</span> <span class=p>=</span> <span class=s>&#34;shape.split_at&#34;</span><span class=p>(</span><span class=nv>%lhs_shape</span><span class=p>,</span> <span class=nv>%c1</span><span class=p>)</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nv>%l1</span><span class=p>,</span> <span class=nv>%r1</span> <span class=p>=</span> <span class=s>&#34;shape.split_at&#34;</span><span class=p>(</span><span class=nv>%lhs_shape</span><span class=p>,</span> <span class=nv>%c1</span><span class=p>)</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>      <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;,</span> <span class=p>!</span>shape<span class=p>.</span>size<span class=p>)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>    <span class=nv>%w3</span> <span class=p>=</span> shape<span class=p>.</span>cstr_eq <span class=nv>%l1</span><span class=p>,</span> <span class=nv>%r0</span><span class=p>,</span> <span class=nl>error=</span><span class=s>&#34;inner dimensions required to match&#34;</span>
</span></span><span class=line><span class=cl>    <span class=nv>%res_2</span> <span class=p>=</span> shape<span class=p>.</span>assuming <span class=nv>%w3</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nv>%res</span> <span class=p>=</span> concat<span class=p>(</span><span class=nv>%l0</span><span class=p>,</span> <span class=nv>%r1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>      shape<span class=p>.</span>assuming_yield <span class=nv>%res</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    shape<span class=p>.</span>assuming_yield <span class=nv>%res_1</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=kt>return</span> <span class=nv>%res</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The above form can now be lowered to the fully imperative form (see
<a href=https://github.com/tensorflow/mlir-hlo/blob/af14e1ded33c3164d4418c5d234b5b346b6d017c/tests/rank-specialization.mlir#L22>test</a>
for example).</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=kt>func</span><span class=p>.</span><span class=kt>func</span> <span class=nf>@matmul_shape3</span><span class=p>(</span><span class=nv>%lhs</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;,</span> <span class=nv>%lhs</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%c1</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>1</span> <span class=p>:</span> <span class=k>index</span>
</span></span><span class=line><span class=cl>  <span class=nv>%c2</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>2</span> <span class=p>:</span> <span class=k>index</span>
</span></span><span class=line><span class=cl>  <span class=nv>%lhs_shape</span> <span class=p>=</span> shape<span class=p>.</span>shape_of <span class=nv>%lhs</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%rhs_shape</span> <span class=p>=</span> shape<span class=p>.</span>shape_of <span class=nv>%rhs</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf32<span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%lhs_rank</span> <span class=p>=</span> shape<span class=p>.</span>rank <span class=nv>%lhs_shape</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%rhs_rank</span> <span class=p>=</span> shape<span class=p>.</span>rank <span class=nv>%rhs_shape</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%w1</span> <span class=p>=</span> shape<span class=p>.</span>shape_eq <span class=nv>%lhs_rank</span><span class=p>,</span> <span class=nv>%rhs_rank</span>
</span></span><span class=line><span class=cl>  <span class=nv>%w2</span> <span class=p>=</span> shape<span class=p>.</span>shape_eq <span class=nv>%c2</span><span class=p>,</span> <span class=nv>%lhs_rank</span>
</span></span><span class=line><span class=cl>  <span class=nv>%w3</span> <span class=p>=</span> and <span class=nv>%w1</span><span class=p>,</span> <span class=nv>%w2</span>
</span></span><span class=line><span class=cl>  assert <span class=nv>%w3</span><span class=p>,</span> <span class=s>&#34;requires rank 2 operands&#34;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%l0</span><span class=p>,</span> <span class=nv>%l1</span> <span class=p>=</span> shape<span class=p>.</span>split_at<span class=p>(</span><span class=nv>%lhs_shape</span><span class=p>,</span> <span class=nv>%c1</span><span class=p>)</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%r0</span><span class=p>,</span> <span class=nv>%r1</span> <span class=p>=</span> shape<span class=p>.</span>split_at<span class=p>(</span><span class=nv>%rhs_shape</span><span class=p>,</span> <span class=nv>%c1</span><span class=p>)</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>index</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%w4</span> <span class=p>=</span> shape<span class=p>.</span>eq <span class=nv>%l1</span><span class=p>,</span> <span class=nv>%r0</span>
</span></span><span class=line><span class=cl>  assert <span class=nv>%w4</span><span class=p>,</span> <span class=s>&#34;inner dimensions required to match&#34;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%res</span> <span class=p>=</span> concat<span class=p>(</span><span class=nv>%l0</span><span class=p>,</span> <span class=nv>%r1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=kt>return</span> <span class=nv>%res</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><ul><li>In this case form 3 is as easy and closer to form 1 (but only as no
reordering was required). So it is a good question if the frontend authoring
language could be more similar to the imperative form (under discussion).</li><li>The above form presented here is an intermittent form during a lowering
pass. If used as input we would need to restrict the optimizations on it as
the <code>shape</code> dialect operations are no longer connected by producer-consumer
to enforce guard checking.</li></ul><p>The above could be further lowered by using <code>tensor.dim</code>, <code>tensor.from_elements</code>
etc (or one could even lower these by way of, say, MHLO or TOSA dialect).</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>This form is least use inside the current workflows and needs more work. In particular in the example we use <code>shape_func</code> where in the code we instead use standard func as first form 1 isn&rsquo;t used explicitly.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=https://mlir.llvm.org/docs/Dialects/SCFDialect/ title="'scf' Dialect"><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - 'scf' Dialect</a>
<a class="nav nav-next" href=https://mlir.llvm.org/docs/Dialects/Shard/ title="'shard' Dialect">Next - 'shard' Dialect <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=https://mlir.llvm.org/governance/>Governance</a></li><li><a href=https://mlir.llvm.org/users/>Users of MLIR</a></li><li><a href=https://mlir.llvm.org/pubs/>MLIR Related Publications</a></li><li><a href=https://mlir.llvm.org/talks/>Talks</a></li><li><a href=https://mlir.llvm.org/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://mlir.llvm.org/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://mlir.llvm.org/getting_started/Faq/>FAQ</a></li><li><a href=https://mlir.llvm.org/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://mlir.llvm.org/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=https://mlir.llvm.org/getting_started/openprojects/>Open Projects</a></li><li><a href=https://mlir.llvm.org/getting_started/Glossary/>Glossary</a></li><li><a href=https://mlir.llvm.org/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-reduce/>MLIR Reduce</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-rewrite/>mlir-rewrite</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/OpenMPPasses/></a></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIRTransforms/></a></li><li><a href=https://mlir.llvm.org/docs/ActionTracing/>Action: Tracing and Debugging MLIR-based Compilers</a></li><li><a href=https://mlir.llvm.org/docs/Bufferization/>Bufferization</a></li><li><a href=https://mlir.llvm.org/docs/DataLayout/>Data Layout Modeling</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/DefiningDialects/>Defining Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Constraints/>Constraints</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Assembly/>Customizing Assembly Behavior</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/AttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Operations/>Operation Definition Specification (ODS)</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/DialectConversion/>Dialect Conversion</a></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/Dialects/>Dialects<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/XeGPUTransformOps/></a></li><li><a href=https://mlir.llvm.org/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMDGPU/>'amdgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArithOps/>'arith' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSME/>'ArmSME' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/BufferizationOps/>'bufferization' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ControlFlowDialect/>'cf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/DLTIDialect/>'dlti' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Func/>'func' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IndexOps/>'index' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IRDL/>'irdl' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MemRef/>'memref' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MLProgramOps/>'ml_program' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MPI/>'mpi' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVGPU/>'nvgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/>'omp' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/ODS/>ODS Documentation</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PtrOps/>'ptr' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li class=active><a href=https://mlir.llvm.org/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Shard/>'shard' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SMT/>'smt' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/UBOps/>'ub' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/VCIXDialect/>'vcix' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/WasmSSAOps/>'wasmssa' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeGPU/>'xegpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeVMDialect/>'xevm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MatchOpInterfaces/>OpInterface definitions</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SPIR-V/>SPIR-V Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Transform/>Transform Dialect</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Interfaces/>Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIR/>LLVM IR Target</a></li><li><a href=https://mlir.llvm.org/docs/BytecodeFormat/>MLIR Bytecode Format</a></li><li><a href=https://mlir.llvm.org/docs/CAPI/>MLIR C API</a></li><li><a href=https://mlir.llvm.org/docs/LangRef/>MLIR Language Reference</a></li><li><a href=https://mlir.llvm.org/docs/ReleaseNotes/>MLIR Release Notes</a></li><li><a href=https://mlir.llvm.org/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=https://mlir.llvm.org/docs/OwnershipBasedBufferDeallocation/>Ownership-based Buffer Deallocation</a></li><li><a href=https://mlir.llvm.org/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/Passes/>Passes</a></li><li><a href=https://mlir.llvm.org/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=https://mlir.llvm.org/docs/PatternSearch/>Pattern Search</a></li><li><a href=https://mlir.llvm.org/docs/PDLL/>PDLL - PDL Language</a></li><li><a href=https://mlir.llvm.org/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/SideEffectsAndSpeculation/>Side Effects & Speculation</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Remarks/>Remark Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/ShapeInference/>Shape Inference</a></li><li><a href=https://mlir.llvm.org/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=https://mlir.llvm.org/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=https://mlir.llvm.org/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Traits/>Traits<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Traits/Broadcastable/>The `Broadcastable` Trait</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/Toy/>Toy Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/transform/>Transform Dialect Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch0/>Chapter 0: A Primer on “Structured” Linalg Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch1/>Chapter 1: Combining Existing Transformations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch2/>Chapter 2: Adding a Simple New Transformation Operation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch3/>Chapter 3: More than Simple Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch4/>Chapter 4: Matching Payload with Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/ChH/>Chapter H: Reproducing Halide Schedule</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/MlirOpt/>Using `mlir-opt`</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>