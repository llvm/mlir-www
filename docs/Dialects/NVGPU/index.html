<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>'nvgpu' Dialect - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.119.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Dialects/NVGPU/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script>
<link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script>
<script src=https://mlir.llvm.org/js/bundle.js></script>
<script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li></ul></li><li><a href="https://github.com/llvm/llvm-project/issues?q=is%3Aissue%20state%3Aopen%20label%3Amlir">Bugs</a></li><li><a href=https://github.com/llvm/mlir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/MLIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>'nvgpu' Dialect</h1><p>The <code>NVGPU</code> dialect provides a bridge between higher-level target-agnostic
dialects (GPU and Vector) and the lower-level target-specific dialect
(LLVM IR based NVVM dialect) for NVIDIA GPUs. This allow representing PTX
specific operations while using MLIR high level dialects such as Memref
and Vector for memory and target-specific register operands, respectively.</p><p><nav id=TableOfContents><ul><li><a href=#operations>Operations</a><ul><li><a href=#nvgpudevice_async_copy-nvgpudeviceasynccopyop><code>nvgpu.device_async_copy</code> (nvgpu::DeviceAsyncCopyOp)</a></li><li><a href=#nvgpudevice_async_create_group-nvgpudeviceasynccreategroupop><code>nvgpu.device_async_create_group</code> (nvgpu::DeviceAsyncCreateGroupOp)</a></li><li><a href=#nvgpudevice_async_wait-nvgpudeviceasyncwaitop><code>nvgpu.device_async_wait</code> (nvgpu::DeviceAsyncWaitOp)</a></li><li><a href=#nvgpuldmatrix-nvgpuldmatrixop><code>nvgpu.ldmatrix</code> (nvgpu::LdMatrixOp)</a></li><li><a href=#nvgpumbarrierarrive-nvgpumbarrierarriveop><code>nvgpu.mbarrier.arrive</code> (nvgpu::MBarrierArriveOp)</a></li><li><a href=#nvgpumbarrierarriveexpect_tx-nvgpumbarrierarriveexpecttxop><code>nvgpu.mbarrier.arrive.expect_tx</code> (nvgpu::MBarrierArriveExpectTxOp)</a></li><li><a href=#nvgpumbarrierarrivenocomplete-nvgpumbarrierarrivenocompleteop><code>nvgpu.mbarrier.arrive.nocomplete</code> (nvgpu::MBarrierArriveNoCompleteOp)</a></li><li><a href=#nvgpumbarriercreate-nvgpumbarriercreateop><code>nvgpu.mbarrier.create</code> (nvgpu::MBarrierCreateOp)</a></li><li><a href=#nvgpumbarrierget-nvgpumbarriergetop><code>nvgpu.mbarrier.get</code> (nvgpu::MBarrierGetOp)</a></li><li><a href=#nvgpumbarrierinit-nvgpumbarrierinitop><code>nvgpu.mbarrier.init</code> (nvgpu::MBarrierInitOp)</a></li><li><a href=#nvgpumbarriertestwait-nvgpumbarriertestwaitop><code>nvgpu.mbarrier.test.wait</code> (nvgpu::MBarrierTestWaitOp)</a></li><li><a href=#nvgpumbarriertry_waitparity-nvgpumbarriertrywaitparityop><code>nvgpu.mbarrier.try_wait.parity</code> (nvgpu::MBarrierTryWaitParityOp)</a></li><li><a href=#nvgpummaspsync-nvgpummasparsesyncop><code>nvgpu.mma.sp.sync</code> (nvgpu::MmaSparseSyncOp)</a></li><li><a href=#nvgpummasync-nvgpummasyncop><code>nvgpu.mma.sync</code> (nvgpu::MmaSyncOp)</a></li><li><a href=#nvgpurcp-nvgpurcpop><code>nvgpu.rcp</code> (nvgpu::RcpOp)</a></li><li><a href=#nvgputmaasyncload-nvgputmaasyncloadop><code>nvgpu.tma.async.load</code> (nvgpu::TmaAsyncLoadOp)</a></li><li><a href=#nvgputmaasyncstore-nvgputmaasyncstoreop><code>nvgpu.tma.async.store</code> (nvgpu::TmaAsyncStoreOp)</a></li><li><a href=#nvgputmacreatedescriptor-nvgputmacreatedescriptorop><code>nvgpu.tma.create.descriptor</code> (nvgpu::TmaCreateDescriptorOp)</a></li><li><a href=#nvgputmafencedescriptor-nvgputmafenceop><code>nvgpu.tma.fence.descriptor</code> (nvgpu::TmaFenceOp)</a></li><li><a href=#nvgputmaprefetchdescriptor-nvgputmaprefetchop><code>nvgpu.tma.prefetch.descriptor</code> (nvgpu::TmaPrefetchOp)</a></li><li><a href=#nvgpuwarpgroupgeneratedescriptor-nvgpuwarpgroupgeneratedescriptorop><code>nvgpu.warpgroup.generate.descriptor</code> (nvgpu::WarpgroupGenerateDescriptorOp)</a></li><li><a href=#nvgpuwarpgroupmma-nvgpuwarpgroupmmaop><code>nvgpu.warpgroup.mma</code> (nvgpu::WarpgroupMmaOp)</a></li><li><a href=#nvgpuwarpgroupmmainitaccumulator-nvgpuwarpgroupmmainitaccumulatorop><code>nvgpu.warpgroup.mma.init.accumulator</code> (nvgpu::WarpgroupMmaInitAccumulatorOp)</a></li><li><a href=#nvgpuwarpgroupmmastore-nvgpuwarpgroupmmastoreop><code>nvgpu.warpgroup.mma.store</code> (nvgpu::WarpgroupMmaStoreOp)</a></li></ul></li><li><a href=#attributes-7>Attributes</a><ul><li><a href=#rcproundingmodeattr>RcpRoundingModeAttr</a></li><li><a href=#tensormapinterleavekindattr>TensorMapInterleaveKindAttr</a></li><li><a href=#tensormapl2promokindattr>TensorMapL2PromoKindAttr</a></li><li><a href=#tensormapoobkindattr>TensorMapOOBKindAttr</a></li><li><a href=#tensormapswizzlekindattr>TensorMapSwizzleKindAttr</a></li></ul></li><li><a href=#types>Types</a><ul><li><a href=#deviceasynctokentype>DeviceAsyncTokenType</a></li><li><a href=#mbarriergrouptype>MBarrierGroupType</a></li><li><a href=#mbarriertokentype>MBarrierTokenType</a></li><li><a href=#tensormapdescriptortype>TensorMapDescriptorType</a></li><li><a href=#warpgroupaccumulatortype>WarpgroupAccumulatorType</a></li><li><a href=#warpgroupmatrixdescriptortype>WarpgroupMatrixDescriptorType</a></li></ul></li><li><a href=#enums>Enums</a><ul><li><a href=#rcproundingmode>RcpRoundingMode</a></li><li><a href=#tensormapinterleavekind>TensorMapInterleaveKind</a></li><li><a href=#tensormapl2promokind>TensorMapL2PromoKind</a></li><li><a href=#tensormapoobkind>TensorMapOOBKind</a></li><li><a href=#tensormapswizzlekind>TensorMapSwizzleKind</a></li></ul></li></ul></nav><h2 id=operations>Operations&nbsp;<a class=headline-hash href=#operations>¶</a></h2><p><a href=https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/NVGPU/IR/NVGPUOps.td>source</a></p><h3 id=nvgpudevice_async_copy-nvgpudeviceasynccopyop><code>nvgpu.device_async_copy</code> (nvgpu::DeviceAsyncCopyOp)&nbsp;<a class=headline-hash href=#nvgpudevice_async_copy-nvgpudeviceasynccopyop>¶</a></h3><p><em>Device-side asynchronous copy</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.device_async_copy` $src `[` $srcIndices `]` `,` $dst `[` $dstIndices `]` `,` $dstElements (`,` $srcElements^)?
              attr-dict `:` type($src) `to` type($dst)
</code></pre><p>The <code>nvgpu.device_async_copy</code> op initiates an asynchronous copy operation of
elements from source (global memory) to the destination (shared memory)
without blocking the thread. The async copy is added to a group.</p><p>This op is meant to be used with <code>nvgpu.device_async_create_group</code> and
<code>nvgpu.device_async_wait</code> to synchronize copies as explained in those ops
descriptions.</p><p><code>bypassL1</code> attribute is hint to the hardware to bypass the L1 cache during
async copy, this hint may be ignored by the hardware.</p><p><code>dstElements</code> attribute is the total number of elements written to
destination (shared memory).</p><p><code>srcElements</code> argument is the total number of elements read from
source (global memory).</p><p><code>srcElements</code> is an optional argument and when present the op only reads
<code>srcElements</code> number of elements from the source (global memory) and zero fills
the rest of the elements in the destination (shared memory).</p><p>In order to do a copy and wait for the result we need the following
combination:</p><pre tabindex=0><code>// copy 1.
%cp1 = nvgpu.device_async_copy %A[%c0], %B[%c0], 4 :memref&lt;16xf32&gt; to memref&lt;16xf32, 3&gt;
// copy 2.
%cp2 = nvgpu.device_async_copy %C[%c0], %D[%c0], 4 : memref&lt;16xf32&gt; to memref&lt;16xf32, 3&gt;
// group 1 contains copy 1 and copy 2.
%token1 = nvgpu.device_async_create_group %cp1, %cp2
// copy 3.
%cp3 = nvgpu.device_async_copy %E[%c0], %F[%c0], 4 : memref&lt;16xf32&gt; to memref&lt;16xf32, 3&gt;
// group 2 contains copy 3.
%token2 = nvgpu.device_async_create_group %cp3
// after the wait copy 1 and copy 2 are complete.
nvgpu.device_async_wait %token1
// after the wait copy 3 is complete.
nvgpu.device_async_wait %token2
</code></pre><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%0</span> <span class=p>=</span> nvgpu<span class=p>.</span>device_async_copy <span class=nv>%src</span><span class=p>[</span><span class=nv>%c0</span><span class=p>,</span> <span class=nv>%c0</span><span class=p>],</span> <span class=nv>%dst</span><span class=p>[</span><span class=nv>%c0</span><span class=p>,</span> <span class=nv>%c0</span><span class=p>,</span> <span class=nv>%c0</span><span class=p>],</span> <span class=m>4</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=kt>memref</span><span class=p>&lt;</span><span class=m>4x5x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>memref</span><span class=p>&lt;</span><span class=m>2x7x5x</span><span class=k>f32</span><span class=p>,</span> <span class=m>3</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Traits: <code>AttrSizedOperandSegments</code></p><p>Interfaces: <code>InferTypeOpInterface</code></p><h4 id=attributes>Attributes:&nbsp;<a class=headline-hash href=#attributes>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>dstElements</code></td><td>::mlir::IntegerAttr</td><td>index attribute</td></tr><tr><td><code>bypassL1</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr></table><h4 id=operands>Operands:&nbsp;<a class=headline-hash href=#operands>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>dst</code></td><td>memref of any type values</td></tr><tr><td style=text-align:center><code>dstIndices</code></td><td>variadic of index</td></tr><tr><td style=text-align:center><code>src</code></td><td>memref of any type values</td></tr><tr><td style=text-align:center><code>srcIndices</code></td><td>variadic of index</td></tr><tr><td style=text-align:center><code>srcElements</code></td><td>index</td></tr></tbody></table><h4 id=results>Results:&nbsp;<a class=headline-hash href=#results>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>asyncToken</code></td><td>device async token type</td></tr></tbody></table><h3 id=nvgpudevice_async_create_group-nvgpudeviceasynccreategroupop><code>nvgpu.device_async_create_group</code> (nvgpu::DeviceAsyncCreateGroupOp)&nbsp;<a class=headline-hash href=#nvgpudevice_async_create_group-nvgpudeviceasynccreategroupop>¶</a></h3><p><em>Device side asynchronous create group operation</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.device_async_create_group` $inputTokens attr-dict
</code></pre><p>The <code>nvgpu.device_async_create_group</code> op creates a group of memory accesses
containing all the pending <code>device_async_copy</code> operations associated with
argument tokens. Each token can only be part of one group.</p><p>It returns a token that can be use to wait until the group fully completes.</p><p>This is meant to be used with <code>nvgpu.device_async_wait</code> to synchronize copies
as explained in those ops descriptions.</p><p>Groups are executed in the order they are created.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%0</span> <span class=p>=</span> nvgpu<span class=p>.</span>device_async_create_group
</span></span></code></pre></div><p>Interfaces: <code>InferTypeOpInterface</code></p><h4 id=operands-1>Operands:&nbsp;<a class=headline-hash href=#operands-1>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>inputTokens</code></td><td>variadic of device async token type</td></tr></tbody></table><h4 id=results-1>Results:&nbsp;<a class=headline-hash href=#results-1>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>asyncToken</code></td><td>device async token type</td></tr></tbody></table><h3 id=nvgpudevice_async_wait-nvgpudeviceasyncwaitop><code>nvgpu.device_async_wait</code> (nvgpu::DeviceAsyncWaitOp)&nbsp;<a class=headline-hash href=#nvgpudevice_async_wait-nvgpudeviceasyncwaitop>¶</a></h3><p><em>Wait for async gpu ops to complete.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.device_async_wait` $asyncDependencies attr-dict
</code></pre><p>The <code>nvgpu.device_async_wait</code> op will block the execution thread until the group
associated with the source token is fully completed.</p><p>The optional <code>$numGroups</code> attribute gives an upper bound of the number of
groups uncompleted when the wait can unblock the thread. For example, if
16 async groups are pushe and <code>$numGroups</code> is set to 12, then the thread
will unblock when 12 groups or fewer are in flight (4 groups have
completed).</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>nvgpu<span class=p>.</span>device_async_wait <span class=nv>%0</span>
</span></span></code></pre></div><h4 id=attributes-1>Attributes:&nbsp;<a class=headline-hash href=#attributes-1>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>numGroups</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute</td></tr></table><h4 id=operands-2>Operands:&nbsp;<a class=headline-hash href=#operands-2>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>asyncDependencies</code></td><td>device async token type</td></tr></tbody></table><h3 id=nvgpuldmatrix-nvgpuldmatrixop><code>nvgpu.ldmatrix</code> (nvgpu::LdMatrixOp)&nbsp;<a class=headline-hash href=#nvgpuldmatrix-nvgpuldmatrixop>¶</a></h3><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.ldmatrix` $srcMemref`[` $indices `]` attr-dict `:` type($srcMemref) `-&gt;` type($res)
</code></pre><p>The <code>nvgpu.ldmatrix</code> op represents loading a matrix fragment from
memory to registers. The source and result type must be compatible
with lowering to the <code>nvvm.ldmatrix</code> instruction. This op represents
the distributed version of a <code>vector.transfer_read</code> as an intermediate
step between lowering from <code>vector.transfer_read</code> to <code>nvvm.ldmatrix</code>.</p><p>This operation is meant to follow the semantic of described here:
<a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix>https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-ldmatrix</a></p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%0</span> <span class=p>=</span> nvgpu<span class=p>.</span>ldmatrix <span class=nv>%sm</span><span class=p>[</span><span class=nv>%c0</span><span class=p>,</span> <span class=nv>%c0</span><span class=p>]</span> <span class=p>{</span><span class=nl>numTiles =</span> <span class=m>4</span> <span class=p>:</span> <span class=k>i32</span><span class=p>,</span> <span class=nl>transpose =</span> false<span class=p>}</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f16</span><span class=p>,</span> <span class=m>3</span><span class=p>&gt;</span> <span class=p>-&gt;</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x2x</span><span class=k>f16</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Interfaces: <code>MemoryEffectOpInterface (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{MemoryEffects::Read on ::mlir::SideEffects::DefaultResource}</code></p><h4 id=attributes-2>Attributes:&nbsp;<a class=headline-hash href=#attributes-2>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>transpose</code></td><td>::mlir::BoolAttr</td><td>bool attribute</td></tr><tr><td><code>numTiles</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute</td></tr></table><h4 id=operands-3>Operands:&nbsp;<a class=headline-hash href=#operands-3>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>srcMemref</code></td><td>memref of any type values</td></tr><tr><td style=text-align:center><code>indices</code></td><td>variadic of index</td></tr></tbody></table><h4 id=results-2>Results:&nbsp;<a class=headline-hash href=#results-2>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=nvgpumbarrierarrive-nvgpumbarrierarriveop><code>nvgpu.mbarrier.arrive</code> (nvgpu::MBarrierArriveOp)&nbsp;<a class=headline-hash href=#nvgpumbarrierarrive-nvgpumbarrierarriveop>¶</a></h3><p><em>Performs arrive operation on the <code>nvgpu.mbarrier.arrive</code>.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.mbarrier.arrive` $barriers `[` $mbarId `]` attr-dict `:` type($barriers) `-&gt;` type($token)
</code></pre><p>The Op performs arrive-on operation on the <code>mbarrier</code> object and returns a
<code>nvgpu.mbarrier.token</code>.</p><p>For more information, see
<a href=https://docs.nvidia.com/cuda/parallel-thread-execution/#arrive-on-operation-on-mbarrier-object>https://docs.nvidia.com/cuda/parallel-thread-execution/#arrive-on-operation-on-mbarrier-object</a></p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>  <span class=nv>%token</span> <span class=p>=</span> nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>arrive <span class=nv>%barrier</span> <span class=p>:</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>barrier<span class=p>&lt;</span><span class=nl>memorySpace =</span> <span class=nv>#gpu.address_space</span><span class=p>&lt;</span>workgroup<span class=p>&gt;&gt;</span> <span class=p>-&gt;</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>token
</span></span></code></pre></div><p>Interfaces: <code>InferTypeOpInterface</code></p><h4 id=operands-4>Operands:&nbsp;<a class=headline-hash href=#operands-4>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>barriers</code></td><td>mbarrier barrier type</td></tr><tr><td style=text-align:center><code>mbarId</code></td><td>index</td></tr></tbody></table><h4 id=results-3>Results:&nbsp;<a class=headline-hash href=#results-3>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>token</code></td><td></td></tr></tbody></table><h3 id=nvgpumbarrierarriveexpect_tx-nvgpumbarrierarriveexpecttxop><code>nvgpu.mbarrier.arrive.expect_tx</code> (nvgpu::MBarrierArriveExpectTxOp)&nbsp;<a class=headline-hash href=#nvgpumbarrierarriveexpect_tx-nvgpumbarrierarriveexpecttxop>¶</a></h3><p><em>Performs expect_tx operation on the <code>nvgpu.mbarrier.arrive</code></em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.mbarrier.arrive.expect_tx` $barriers `[` $mbarId `]` `,` $txcount  (`,` `predicate` `=` $predicate^)? attr-dict `:` type($barriers)
</code></pre><p>A thread executing the Op performs an expect-tx operation on the mbarrier
object at the location specified by the address operand $barrier. The
expect-tx operation, with an $txcount argument, increases the tx-count of
an mbarrier object by the value specified by $txcount. This makes the
current phase of the mbarrier object to expect and track the completion of
additional asynchronous transactions.</p><p>The <code>$txCount</code> specifies the number of element to the expect-tx operation.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>  nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>arrive<span class=p>.</span>expect_tx <span class=nv>%barrier</span><span class=p>,</span> <span class=nv>%ic0</span> <span class=p>:</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>barrier<span class=p>&lt;</span><span class=nl>memorySpace =</span> <span class=nv>#gpu.address_space</span><span class=p>&lt;</span>workgroup<span class=p>&gt;&gt;</span>
</span></span></code></pre></div><h4 id=operands-5>Operands:&nbsp;<a class=headline-hash href=#operands-5>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>barriers</code></td><td>mbarrier barrier type</td></tr><tr><td style=text-align:center><code>txcount</code></td><td>index</td></tr><tr><td style=text-align:center><code>mbarId</code></td><td>index</td></tr><tr><td style=text-align:center><code>predicate</code></td><td>1-bit signless integer</td></tr></tbody></table><h3 id=nvgpumbarrierarrivenocomplete-nvgpumbarrierarrivenocompleteop><code>nvgpu.mbarrier.arrive.nocomplete</code> (nvgpu::MBarrierArriveNoCompleteOp)&nbsp;<a class=headline-hash href=#nvgpumbarrierarrivenocomplete-nvgpumbarrierarrivenocompleteop>¶</a></h3><p><em>Performs arrive operation on the <code>nvgpu.mbarrier.arrive.nocomplete</code> as non-blocking.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.mbarrier.arrive.nocomplete` $barriers `[` $mbarId `]` `,` $count attr-dict `:` type($barriers) `-&gt;` type($token)
</code></pre><p>The Op performs arrive-on operation on the <code>mbarrier</code> object and returns a
<code>nvgpu.mbarrier.token</code>.</p><p>The Op does not cause the <code>nvgpu.mbarrier</code> to complete its current phase.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>  <span class=nv>%token</span> <span class=p>=</span> nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>arrive<span class=p>.</span>noComplete <span class=nv>%barrier</span><span class=p>,</span> <span class=nv>%count</span> <span class=p>:</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>barrier<span class=p>&lt;</span><span class=nl>memorySpace =</span> <span class=nv>#gpu.address_space</span><span class=p>&lt;</span>workgroup<span class=p>&gt;&gt;</span> <span class=p>-&gt;</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>token
</span></span></code></pre></div><p>Interfaces: <code>InferTypeOpInterface</code></p><h4 id=operands-6>Operands:&nbsp;<a class=headline-hash href=#operands-6>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>barriers</code></td><td>mbarrier barrier type</td></tr><tr><td style=text-align:center><code>mbarId</code></td><td>index</td></tr><tr><td style=text-align:center><code>count</code></td><td>index</td></tr></tbody></table><h4 id=results-4>Results:&nbsp;<a class=headline-hash href=#results-4>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>token</code></td><td></td></tr></tbody></table><h3 id=nvgpumbarriercreate-nvgpumbarriercreateop><code>nvgpu.mbarrier.create</code> (nvgpu::MBarrierCreateOp)&nbsp;<a class=headline-hash href=#nvgpumbarriercreate-nvgpumbarriercreateop>¶</a></h3><p><em>Creates a <code>nvgpu.mbarrier</code> object.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.mbarrier.create` attr-dict `-&gt;` type($barriers)
</code></pre><p>The Op generates one or more <code>mbarrier</code> object, which is a barrier created in
shared memory and supports various synchronization behaviors for threads.</p><p>The <code>mbarrier</code> object has the following type and alignment requirements:
Type: .b64, Alignment: 8, Memory space: .shared</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>  <span class=nv>%barrier</span> <span class=p>=</span> nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>create <span class=p>-&gt;</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>barrier<span class=p>&lt;</span><span class=nl>memorySpace =</span> <span class=nv>#gpu.address_space</span><span class=p>&lt;</span>workgroup<span class=p>&gt;&gt;</span>
</span></span></code></pre></div><h4 id=results-5>Results:&nbsp;<a class=headline-hash href=#results-5>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>barriers</code></td><td>mbarrier barrier type</td></tr></tbody></table><h3 id=nvgpumbarrierget-nvgpumbarriergetop><code>nvgpu.mbarrier.get</code> (nvgpu::MBarrierGetOp)&nbsp;<a class=headline-hash href=#nvgpumbarrierget-nvgpumbarriergetop>¶</a></h3><p><em>Return a pointer to an <code>nvgpu.mbarrier</code>.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.mbarrier.get` $barriers `[` $mbarId `]` attr-dict `:` type($barriers) `-&gt;` type($mbarrierPointer)
</code></pre><p>The <code>nvgpu.mbarrier.get</code> operation retrieves a pointer to a specific
<code>mbarrier</code> object from a group of barriers created by the <code>nvgpu.mbarrier.create</code> operation.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>  <span class=nv>%mbars</span> <span class=p>=</span> nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>create <span class=p>-&gt;</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>group<span class=p>&lt;</span><span class=nl>memorySpace =</span> <span class=nv>#gpu.address_space</span><span class=p>&lt;</span>workgroup<span class=p>&gt;,</span> <span class=nl>num_barriers =</span> <span class=m>10</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%mbar_pointer</span> <span class=p>=</span> nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>get <span class=nv>%mbars</span><span class=p>[</span><span class=nv>%c2</span><span class=p>]</span> <span class=p>:</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>group<span class=p>&lt;</span><span class=nl>memorySpace =</span> <span class=nv>#gpu.address_space</span><span class=p>&lt;</span>workgroup<span class=p>&gt;&gt;</span>
</span></span></code></pre></div><h4 id=operands-7>Operands:&nbsp;<a class=headline-hash href=#operands-7>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>barriers</code></td><td>mbarrier barrier type</td></tr><tr><td style=text-align:center><code>mbarId</code></td><td>index</td></tr></tbody></table><h4 id=results-6>Results:&nbsp;<a class=headline-hash href=#results-6>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>mbarrierPointer</code></td><td>32-bit signless integer or 64-bit signless integer</td></tr></tbody></table><h3 id=nvgpumbarrierinit-nvgpumbarrierinitop><code>nvgpu.mbarrier.init</code> (nvgpu::MBarrierInitOp)&nbsp;<a class=headline-hash href=#nvgpumbarrierinit-nvgpumbarrierinitop>¶</a></h3><p><em>Initialize the <code>nvgpu.mbarrier</code>.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.mbarrier.init` $barriers `[` $mbarId `]` `,` $count (`,` `predicate` `=` $predicate^)? attr-dict `:` type($barriers)
</code></pre><p>The Op initializes the <code>mbarrier</code> object with the given number of threads.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>  <span class=nv>%num_threads</span> <span class=p>=</span> gpu<span class=p>.</span>block_dim <span class=p>x</span>
</span></span><span class=line><span class=cl>  <span class=nv>%barrier</span> <span class=p>=</span> nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>create <span class=p>-&gt;</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>barrier<span class=p>&lt;</span><span class=nl>memorySpace =</span> <span class=nv>#gpu.address_space</span><span class=p>&lt;</span>workgroup<span class=p>&gt;&gt;</span>
</span></span><span class=line><span class=cl>  nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>init <span class=nv>%barrier</span><span class=p>,</span> <span class=nv>%num_threads</span> <span class=p>:</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>barrier<span class=p>&lt;</span><span class=nl>memorySpace =</span> <span class=nv>#gpu.address_space</span><span class=p>&lt;</span>workgroup<span class=p>&gt;&gt;</span>
</span></span></code></pre></div><h4 id=operands-8>Operands:&nbsp;<a class=headline-hash href=#operands-8>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>barriers</code></td><td>mbarrier barrier type</td></tr><tr><td style=text-align:center><code>count</code></td><td>index</td></tr><tr><td style=text-align:center><code>mbarId</code></td><td>index</td></tr><tr><td style=text-align:center><code>predicate</code></td><td>1-bit signless integer</td></tr></tbody></table><h3 id=nvgpumbarriertestwait-nvgpumbarriertestwaitop><code>nvgpu.mbarrier.test.wait</code> (nvgpu::MBarrierTestWaitOp)&nbsp;<a class=headline-hash href=#nvgpumbarriertestwait-nvgpumbarriertestwaitop>¶</a></h3><p><em>Checks if the <code>nvgpu.mbarrier</code> has completed its current phase.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.mbarrier.test.wait` $barriers `[` $mbarId `]` `,` $token attr-dict `:` type($barriers) `,` type($token)
</code></pre><p>Checks whether the mbarrier object has completed the phase. It is is a
non-blocking instruction which tests for the completion of the phase.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>  <span class=nv>%isComplete</span> <span class=p>=</span> nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>test<span class=p>.</span>wait <span class=nv>%barrier</span><span class=p>,</span> <span class=nv>%token</span> <span class=p>:</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>barrier<span class=p>&lt;</span><span class=nl>memorySpace =</span> <span class=nv>#gpu.address_space</span><span class=p>&lt;</span>workgroup<span class=p>&gt;&gt;,</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>token
</span></span></code></pre></div><p>Interfaces: <code>InferTypeOpInterface</code></p><h4 id=operands-9>Operands:&nbsp;<a class=headline-hash href=#operands-9>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>barriers</code></td><td>mbarrier barrier type</td></tr><tr><td style=text-align:center><code>token</code></td><td></td></tr><tr><td style=text-align:center><code>mbarId</code></td><td>index</td></tr></tbody></table><h4 id=results-7>Results:&nbsp;<a class=headline-hash href=#results-7>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>waitComplete</code></td><td>1-bit signless integer</td></tr></tbody></table><h3 id=nvgpumbarriertry_waitparity-nvgpumbarriertrywaitparityop><code>nvgpu.mbarrier.try_wait.parity</code> (nvgpu::MBarrierTryWaitParityOp)&nbsp;<a class=headline-hash href=#nvgpumbarriertry_waitparity-nvgpumbarriertrywaitparityop>¶</a></h3><p><em>Waits for the <code>nvgpu.mbarrier</code> to complete its current phase.</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.mbarrier.try_wait.parity` $barriers `[` $mbarId `]` `,` $phaseParity `,` $ticks attr-dict `:` type($barriers)
</code></pre><p>Checks whether the mbarrier object has completed the phase. It is is a
potentially blocking instruction which tests for the completion of the
phase. Suspended thread resumes execution when the specified phase completes
OR before the phase completes following a system-dependent time limit.</p><p>The <code>$phaseParity</code> specifies either even phase (0) or odd phase (1) to
wait.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>  nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>try_wait<span class=p>.</span>parity <span class=nv>%barrier</span><span class=p>,</span> <span class=nv>%phaseParity</span><span class=p>,</span> <span class=nv>%ticks</span> <span class=p>:</span> <span class=p>!</span>nvgpu<span class=p>.</span>mbarrier<span class=p>.</span>barrier<span class=p>&lt;</span><span class=nl>memorySpace =</span> <span class=nv>#gpu.address_space</span><span class=p>&lt;</span>workgroup<span class=p>&gt;&gt;</span>
</span></span></code></pre></div><h4 id=operands-10>Operands:&nbsp;<a class=headline-hash href=#operands-10>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>barriers</code></td><td>mbarrier barrier type</td></tr><tr><td style=text-align:center><code>phaseParity</code></td><td>1-bit signless integer</td></tr><tr><td style=text-align:center><code>ticks</code></td><td>index</td></tr><tr><td style=text-align:center><code>mbarId</code></td><td>index</td></tr></tbody></table><h3 id=nvgpummaspsync-nvgpummasparsesyncop><code>nvgpu.mma.sp.sync</code> (nvgpu::MmaSparseSyncOp)&nbsp;<a class=headline-hash href=#nvgpummaspsync-nvgpummasparsesyncop>¶</a></h3><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.mma.sp.sync` `(` $matrixA`,` $matrixB`,` $matrixC `)` `metadata` `(` $sparseMetadata `)` attr-dict
              `:` `(` type($matrixA) `,` type($matrixB) `,` type($matrixC) `)` `-&gt;` type($res)
</code></pre><p>The <code>nvgu.mma.sp.sync</code> operation performs a warp-distributed MMA operation
where operand A is &ldquo;structured sparse&rdquo;. In this case, the <code>matrixA</code> operand
represents the (warp-distributed) non-zero values of operand A, and the
<code>sparse_metadata</code> operand provides the indices.</p><p>The full description of the sparsity storage format and distribution scheme is
described in the PTX docs. This operation is meant to follow the semantic
described in the PTX documentation here:
<a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-for-sparse-mma>https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-for-sparse-mma</a></p><p>The way the indices are distributed among the threads in a warp is controlled
by the optional <code>sparsity_selector</code> operand, which is <code>0</code> by default. For
more information, please consult the PTX documentation linked above.</p><p>Example (targetingthe f16 16x8x32 <code>mma.sp</code> PTX instruction):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>nvgpu<span class=p>.</span>mma<span class=p>.</span>sp<span class=p>.</span>sync <span class=p>(</span><span class=nv>%a</span><span class=p>,</span> <span class=nv>%b</span><span class=p>,</span> <span class=nv>%c</span><span class=p>)</span> metadata <span class=p>(</span><span class=nv>%meta</span><span class=p>)</span> <span class=p>{</span><span class=nl>mmaShape =</span> <span class=p>[</span><span class=m>16</span><span class=p>,</span> <span class=m>8</span><span class=p>,</span> <span class=m>32</span><span class=p>]}</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>  <span class=p>(</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f16</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f16</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=attributes-3>Attributes:&nbsp;<a class=headline-hash href=#attributes-3>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mmaShape</code></td><td>::mlir::ArrayAttr</td><td>64-bit integer array attribute</td></tr><tr><td><code>sparsitySelector</code></td><td>::mlir::IntegerAttr</td><td>32-bit signless integer attribute</td></tr><tr><td><code>tf32Enabled</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr></table><h4 id=operands-11>Operands:&nbsp;<a class=headline-hash href=#operands-11>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>matrixA</code></td><td>vector of any type values</td></tr><tr><td style=text-align:center><code>matrixB</code></td><td>vector of any type values</td></tr><tr><td style=text-align:center><code>matrixC</code></td><td>vector of any type values</td></tr><tr><td style=text-align:center><code>sparseMetadata</code></td><td>fixed-length vector of 16-bit signless integer values of length 2</td></tr></tbody></table><h4 id=results-8>Results:&nbsp;<a class=headline-hash href=#results-8>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=nvgpummasync-nvgpummasyncop><code>nvgpu.mma.sync</code> (nvgpu::MmaSyncOp)&nbsp;<a class=headline-hash href=#nvgpummasync-nvgpummasyncop>¶</a></h3><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.mma.sync` `(` $matrixA`,` $matrixB`,` $matrixC `)` attr-dict
              `:` `(` type($matrixA) `,` type($matrixB) `,` type($matrixC) `)` `-&gt;` type($res)
</code></pre><p>The <code>nvgpu.mma.sync</code> op represents the warp-level matrix-multiply-and-
accumulate (mma) operation that is compatible with <code>nvvm.mma.sync</code>.
The operands and results vector sizes are thread-level onwership to
the warp-level mma operation shape. <code>mmaShape</code> attribute holds the
warp-level matrix-multiply shape.</p><p>The <code>nvgpu.mma.sync</code> op serves as an intermediate point between lowering from
<code>vector.contract</code> to <code>nvvm.mma.sync</code>.</p><p>This operation is meant to follow the semantic of described here:
<a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma>https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma</a></p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%res</span> <span class=p>=</span> nvgpu<span class=p>.</span>mma<span class=p>.</span>sync <span class=p>(</span><span class=nv>%matrixA</span><span class=p>,</span> <span class=nv>%matrixB</span><span class=p>,</span> <span class=nv>%matrixC</span><span class=p>)</span> <span class=p>{</span><span class=nl>mmaShape =</span> <span class=p>[</span><span class=m>16</span><span class=p>,</span> <span class=m>8</span><span class=p>,</span> <span class=m>16</span><span class=p>]}</span> <span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=kt>vector</span><span class=p>&lt;</span><span class=m>4x2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Traits: <code>AlwaysSpeculatableImplTrait</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=attributes-4>Attributes:&nbsp;<a class=headline-hash href=#attributes-4>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>mmaShape</code></td><td>::mlir::ArrayAttr</td><td>64-bit integer array attribute</td></tr><tr><td><code>tf32Enabled</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr></table><h4 id=operands-12>Operands:&nbsp;<a class=headline-hash href=#operands-12>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>matrixA</code></td><td>vector of any type values</td></tr><tr><td style=text-align:center><code>matrixB</code></td><td>vector of any type values</td></tr><tr><td style=text-align:center><code>matrixC</code></td><td>vector of any type values</td></tr></tbody></table><h4 id=results-9>Results:&nbsp;<a class=headline-hash href=#results-9>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>res</code></td><td>vector of any type values</td></tr></tbody></table><h3 id=nvgpurcp-nvgpurcpop><code>nvgpu.rcp</code> (nvgpu::RcpOp)&nbsp;<a class=headline-hash href=#nvgpurcp-nvgpurcpop>¶</a></h3><p><em>The reciprocal calculation for vector types</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.rcp` $in `{` `rounding` `=` $rounding (`,` `ftz` $ftz^)? `}`
              attr-dict `:` type($out)
</code></pre><p>Reciprocal calculation for <code>vector</code> types using <code>nvvm.rcp</code> OPs.</p><p>Currently, only the <code>approx</code> rounding mode and <code>ftz</code> are supported, and only for the <code>f32</code> type.</p><p>The input and output must be of the same vector type and shape.</p><p>Traits: <code>AlwaysSpeculatableImplTrait</code>, <code>SameOperandsAndResultType</code></p><p>Interfaces: <code>ConditionallySpeculatable</code>, <code>InferTypeOpInterface</code>, <code>NoMemoryEffect (MemoryEffectOpInterface)</code></p><p>Effects: <code>MemoryEffects::Effect{}</code></p><h4 id=attributes-5>Attributes:&nbsp;<a class=headline-hash href=#attributes-5>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>rounding</code></td><td>::mlir::nvgpu::RcpRoundingModeAttr</td><td>Rounding mode of rcp</td></tr><tr><td><code>ftz</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr></table><h4 id=operands-13>Operands:&nbsp;<a class=headline-hash href=#operands-13>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>in</code></td><td>vector of 32-bit float values</td></tr></tbody></table><h4 id=results-10>Results:&nbsp;<a class=headline-hash href=#results-10>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>out</code></td><td>vector of 32-bit float values</td></tr></tbody></table><h3 id=nvgputmaasyncload-nvgputmaasyncloadop><code>nvgpu.tma.async.load</code> (nvgpu::TmaAsyncLoadOp)&nbsp;<a class=headline-hash href=#nvgputmaasyncload-nvgputmaasyncloadop>¶</a></h3><p><em>TMA asynchronous load</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.tma.async.load` $tensorMapDescriptor `[` $coordinates `]` `,` $barriers `[` $mbarId `]`
              `to` $dst
              (`multicast_mask` `=` $multicastMask^ )?
              (`,` `predicate` `=` $predicate^)?
              attr-dict `:` type($tensorMapDescriptor) `,` type($barriers)
              `-&gt;` type($dst)
</code></pre><p>The Op loads a tile memory region from global memory to shared memory by
Tensor Memory Access (TMA).</p><p><code>$tensorMapDescriptor</code> is tensor map descriptor which has information about
tile shape. The descriptor is created by <code>nvgpu.tma.create.descriptor</code></p><p>The Op uses <code>$barrier</code> mbarrier based completion mechanism.</p><p>Traits: <code>AttrSizedOperandSegments</code></p><h4 id=operands-14>Operands:&nbsp;<a class=headline-hash href=#operands-14>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>dst</code></td><td>memref of any type values</td></tr><tr><td style=text-align:center><code>barriers</code></td><td>mbarrier barrier type</td></tr><tr><td style=text-align:center><code>tensorMapDescriptor</code></td><td>TensorMap descriptor</td></tr><tr><td style=text-align:center><code>coordinates</code></td><td>variadic of index</td></tr><tr><td style=text-align:center><code>mbarId</code></td><td>index</td></tr><tr><td style=text-align:center><code>multicastMask</code></td><td>16-bit signless integer</td></tr><tr><td style=text-align:center><code>predicate</code></td><td>1-bit signless integer</td></tr></tbody></table><h3 id=nvgputmaasyncstore-nvgputmaasyncstoreop><code>nvgpu.tma.async.store</code> (nvgpu::TmaAsyncStoreOp)&nbsp;<a class=headline-hash href=#nvgputmaasyncstore-nvgputmaasyncstoreop>¶</a></h3><p><em>TMA asynchronous store</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.tma.async.store` $src `to` $tensorMapDescriptor `[` $coordinates `]`
              (`,` `predicate` `=` $predicate^)?
              attr-dict `:` type($src)
              `-&gt;` type($tensorMapDescriptor)
</code></pre><p>The Op store a tile memory region from global memory to shared memory by
Tensor Memory Access (TMA).</p><p><code>$tensorMapDescriptor</code> is tensor map descriptor which has information about
tile shape. The descriptor is created by <code>nvgpu.tma.create.descriptor</code></p><p>Traits: <code>AttrSizedOperandSegments</code></p><h4 id=operands-15>Operands:&nbsp;<a class=headline-hash href=#operands-15>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>src</code></td><td>memref of any type values</td></tr><tr><td style=text-align:center><code>tensorMapDescriptor</code></td><td>TensorMap descriptor</td></tr><tr><td style=text-align:center><code>coordinates</code></td><td>variadic of index</td></tr><tr><td style=text-align:center><code>predicate</code></td><td>1-bit signless integer</td></tr></tbody></table><h3 id=nvgputmacreatedescriptor-nvgputmacreatedescriptorop><code>nvgpu.tma.create.descriptor</code> (nvgpu::TmaCreateDescriptorOp)&nbsp;<a class=headline-hash href=#nvgputmacreatedescriptor-nvgputmacreatedescriptorop>¶</a></h3><p><em>TMA create descriptor</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.tma.create.descriptor` $tensor `box` `[` $boxDimensions `]` attr-dict `:` type($tensor) `-&gt;` type($tensorMap)
</code></pre><p>The Op creates a tensor map descriptor object representing tiled memory
region. To do that it calls CUDA Driver&rsquo;s <code>cuTensorMapEncodeTiled</code>. The
descriptor is used by Tensor Memory Access (TMA).</p><p>The <code>tensor</code> is the source tensor to be tiled.</p><p>The <code>boxDimensions</code> is the size of the tiled memory region in each dimension.</p><p>For more information see below:
<a href=https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html>https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__TENSOR__MEMORY.html</a></p><h4 id=operands-16>Operands:&nbsp;<a class=headline-hash href=#operands-16>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>unranked.memref of any type values</td></tr><tr><td style=text-align:center><code>boxDimensions</code></td><td>variadic of index</td></tr></tbody></table><h4 id=results-11>Results:&nbsp;<a class=headline-hash href=#results-11>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensorMap</code></td><td>TensorMap descriptor</td></tr></tbody></table><h3 id=nvgputmafencedescriptor-nvgputmafenceop><code>nvgpu.tma.fence.descriptor</code> (nvgpu::TmaFenceOp)&nbsp;<a class=headline-hash href=#nvgputmafencedescriptor-nvgputmafenceop>¶</a></h3><p><em>Insert fence given <code>nvgpu.tensormap.descriptor</code></em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.tma.fence.descriptor` $tensorMapDescriptor attr-dict `:` type($tensorMapDescriptor)
</code></pre><p>The Op fences the given <code>$tmaDescriptor</code>. This is necessary if the tensor map
descriptor was modified from the host using cudaMemcpy. In this case, the
kernel needs a fence after which it is safe to use <code>tensor.map</code>.</p><h4 id=operands-17>Operands:&nbsp;<a class=headline-hash href=#operands-17>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensorMapDescriptor</code></td><td>TensorMap descriptor</td></tr></tbody></table><h3 id=nvgputmaprefetchdescriptor-nvgputmaprefetchop><code>nvgpu.tma.prefetch.descriptor</code> (nvgpu::TmaPrefetchOp)&nbsp;<a class=headline-hash href=#nvgputmaprefetchdescriptor-nvgputmaprefetchop>¶</a></h3><p><em>Prefetch given <code>nvgpu.tensormap.descriptor</code></em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.tma.prefetch.descriptor` $tensorMapDescriptor (`,` `predicate` `=` $predicate^)? attr-dict `:` type($tensorMapDescriptor)
</code></pre><p>The Op brings the cache line containing the given <code>$tmaDescriptor</code> for
subsequent use by the <code>tma.async.load</code> instruction.</p><h4 id=operands-18>Operands:&nbsp;<a class=headline-hash href=#operands-18>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensorMapDescriptor</code></td><td>TensorMap descriptor</td></tr><tr><td style=text-align:center><code>predicate</code></td><td>1-bit signless integer</td></tr></tbody></table><h3 id=nvgpuwarpgroupgeneratedescriptor-nvgpuwarpgroupgeneratedescriptorop><code>nvgpu.warpgroup.generate.descriptor</code> (nvgpu::WarpgroupGenerateDescriptorOp)&nbsp;<a class=headline-hash href=#nvgpuwarpgroupgeneratedescriptor-nvgpuwarpgroupgeneratedescriptorop>¶</a></h3><p><em>Generate a warpgroup matrix descriptor</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.warpgroup.generate.descriptor` $tensor `,` $tensorMap attr-dict `:` type($tensor) `,` type($tensorMap) `-&gt;` type($descriptor)
</code></pre><p>This Op builds a <code>nvgpu.warpgroup.descriptor</code> that is used by
<code>nvgpu.warpgroup.mma</code> to perform warpgroup-level matrix multiply and
accumulate.</p><p>The descriptor specifies the properties of the matrix in shared memory that
is a multiplicand in the matrix multiply and accumulate operation.</p><h4 id=operands-19>Operands:&nbsp;<a class=headline-hash href=#operands-19>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>tensor</code></td><td>memref of any type values</td></tr><tr><td style=text-align:center><code>tensorMap</code></td><td>TensorMap descriptor</td></tr></tbody></table><h4 id=results-12>Results:&nbsp;<a class=headline-hash href=#results-12>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>descriptor</code></td><td>Warpgroup matrix descriptor type</td></tr></tbody></table><h3 id=nvgpuwarpgroupmma-nvgpuwarpgroupmmaop><code>nvgpu.warpgroup.mma</code> (nvgpu::WarpgroupMmaOp)&nbsp;<a class=headline-hash href=#nvgpuwarpgroupmma-nvgpuwarpgroupmmaop>¶</a></h3><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.warpgroup.mma` $descriptorA`,` $descriptorB`,` $matrixC attr-dict
              `:` type($descriptorA) `,` type($descriptorB) `,` type($matrixC) `-&gt;` type($matrixD)
</code></pre><p>The <code>nvgpu.warpgroup.mma</code> op performs the warpgroup-level (4 warps)
matrix-multiply-and-accumulate (mma) operation that results in
<code>nvvm.wgmma.mma_async</code>.</p><p>The operands are <code>descriptorA</code> and <code>descriptorB</code> that are wgmma matrix
descriptors that shows the properties of the matrix in shared memory. The
results are thread-level ownership to the warpgroup-level mma operation
shape. The shape is deduced from the descriptor types and output vector.</p><p>The Op encapsulates multiple <code>nvvm.wgmma.mma_async</code> operations to complete
the given shape. As <code>nvvm.wgmma.async</code> Op, or its corresponding PTX
instruction, is asynchronous, this Op groups the <code>nvvm.wgmma.async</code> and
surrounds them between <code>wgmma.fence.aligned</code> and
<code>wgmma.commit.group.sync.aligned</code>, <code>wgmma.wait.group.sync.aligned</code> Ops.</p><p>Example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>  <span class=nv>%r1</span><span class=p>,</span><span class=nv>%r2</span> <span class=p>=</span> nvgpu<span class=p>.</span>warpgroup<span class=p>.</span>mma <span class=nv>%descA</span><span class=p>,</span> <span class=nv>%descB</span><span class=p>,</span> <span class=nv>%acc1</span><span class=p>,</span> <span class=nv>%acc2</span><span class=p>:</span> 
</span></span><span class=line><span class=cl>             <span class=p>!</span>nvgpu<span class=p>.</span>warpgroup<span class=p>.</span>descriptor<span class=p>&lt;</span><span class=nl>tensor =</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>128x64x</span><span class=k>f16</span><span class=p>,</span> <span class=m>3</span><span class=p>&gt;&gt;,</span> 
</span></span><span class=line><span class=cl>             <span class=p>!</span>nvgpu<span class=p>.</span>warpgroup<span class=p>.</span>descriptor<span class=p>&lt;</span><span class=nl>tensor =</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>64x128x</span><span class=k>f16</span><span class=p>,</span> <span class=m>3</span><span class=p>&gt;&gt;,</span> 
</span></span><span class=line><span class=cl>             <span class=p>!</span>nvgpu<span class=p>.</span>warpgroup<span class=p>.</span>accumulator<span class=p>&lt;</span><span class=nl>fragmented =</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>64x128x</span><span class=k>f32</span><span class=p>&gt;&gt;,</span>
</span></span><span class=line><span class=cl>             <span class=p>!</span>nvgpu<span class=p>.</span>warpgroup<span class=p>.</span>accumulator<span class=p>&lt;</span><span class=nl>fragmented =</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>64x128x</span><span class=k>f32</span><span class=p>&gt;&gt;</span>
</span></span><span class=line><span class=cl>             <span class=p>-&gt;</span> 
</span></span><span class=line><span class=cl>             <span class=p>!</span>nvgpu<span class=p>.</span>warpgroup<span class=p>.</span>accumulator<span class=p>&lt;</span><span class=nl>fragmented =</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>64x128x</span><span class=k>f32</span><span class=p>&gt;&gt;,</span>
</span></span><span class=line><span class=cl>             <span class=p>!</span>nvgpu<span class=p>.</span>warpgroup<span class=p>.</span>accumulator<span class=p>&lt;</span><span class=nl>fragmented =</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>64x128x</span><span class=k>f32</span><span class=p>&gt;&gt;</span>
</span></span></code></pre></div><h4 id=attributes-6>Attributes:&nbsp;<a class=headline-hash href=#attributes-6>¶</a></h4><table><tr><th>Attribute</th><th>MLIR Type</th><th>Description</th></tr><tr><td><code>waitGroup</code></td><td>::mlir::IntegerAttr</td><td>64-bit signless integer attribute</td></tr><tr><td><code>transposeA</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr><tr><td><code>transposeB</code></td><td>::mlir::UnitAttr</td><td>unit attribute</td></tr></table><h4 id=operands-20>Operands:&nbsp;<a class=headline-hash href=#operands-20>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>descriptorA</code></td><td>Warpgroup matrix descriptor type</td></tr><tr><td style=text-align:center><code>descriptorB</code></td><td>Warpgroup matrix descriptor type</td></tr><tr><td style=text-align:center><code>matrixC</code></td><td></td></tr></tbody></table><h4 id=results-13>Results:&nbsp;<a class=headline-hash href=#results-13>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>matrixD</code></td><td></td></tr></tbody></table><h3 id=nvgpuwarpgroupmmainitaccumulator-nvgpuwarpgroupmmainitaccumulatorop><code>nvgpu.warpgroup.mma.init.accumulator</code> (nvgpu::WarpgroupMmaInitAccumulatorOp)&nbsp;<a class=headline-hash href=#nvgpuwarpgroupmmainitaccumulator-nvgpuwarpgroupmmainitaccumulatorop>¶</a></h3><p><em>Initializes the accumulator matrix</em></p><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.warpgroup.mma.init.accumulator` attr-dict `-&gt;` type($matrixC)
</code></pre><p>This Op generates and initializes the accumulator matrix for
<code>nvgpu.warpgroup.mma</code> op to perform matrix-multiply-and-accumulate.</p><h4 id=results-14>Results:&nbsp;<a class=headline-hash href=#results-14>¶</a></h4><table><thead><tr><th style=text-align:center>Result</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>matrixC</code></td><td></td></tr></tbody></table><h3 id=nvgpuwarpgroupmmastore-nvgpuwarpgroupmmastoreop><code>nvgpu.warpgroup.mma.store</code> (nvgpu::WarpgroupMmaStoreOp)&nbsp;<a class=headline-hash href=#nvgpuwarpgroupmmastore-nvgpuwarpgroupmmastoreop>¶</a></h3><p>Syntax:</p><pre tabindex=0><code>operation ::= `nvgpu.warpgroup.mma.store` $matrixD `,` $dstMemref attr-dict `:` type($matrixD) `to` type($dstMemref)
</code></pre><p>The <code>nvgpu.warpgroup.mma.store</code> op performs the store of fragmented result
in $matrixD to given memref.</p><p>[See the details of register fragment layout for accumulator matrix D]
(
<a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#wgmma-64n16-d>https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#wgmma-64n16-d</a>)</p><p>Note that, the op must be run with warp group.</p><h4 id=operands-21>Operands:&nbsp;<a class=headline-hash href=#operands-21>¶</a></h4><table><thead><tr><th style=text-align:center>Operand</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center><code>matrixD</code></td><td></td></tr><tr><td style=text-align:center><code>dstMemref</code></td><td>memref of any type values</td></tr></tbody></table><h2 id=attributes-7>Attributes&nbsp;<a class=headline-hash href=#attributes-7>¶</a></h2><h3 id=rcproundingmodeattr>RcpRoundingModeAttr&nbsp;<a class=headline-hash href=#rcproundingmodeattr>¶</a></h3><p><em>Rounding mode of rcp</em></p><p>Syntax:</p><pre tabindex=0><code>#nvgpu.rcp_rounding_mode&lt;
  ::mlir::nvgpu::RcpRoundingMode   # value
&gt;
</code></pre><h4 id=parameters>Parameters:&nbsp;<a class=headline-hash href=#parameters>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>value</td><td style=text-align:center><code>::mlir::nvgpu::RcpRoundingMode</code></td><td>an enum of type RcpRoundingMode</td></tr></tbody></table><h3 id=tensormapinterleavekindattr>TensorMapInterleaveKindAttr&nbsp;<a class=headline-hash href=#tensormapinterleavekindattr>¶</a></h3><p><em>Tensor map interleave layout type</em></p><p>Syntax:</p><pre tabindex=0><code>#nvgpu.interleave&lt;
  ::mlir::nvgpu::TensorMapInterleaveKind   # value
&gt;
</code></pre><h4 id=parameters-1>Parameters:&nbsp;<a class=headline-hash href=#parameters-1>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>value</td><td style=text-align:center><code>::mlir::nvgpu::TensorMapInterleaveKind</code></td><td>an enum of type TensorMapInterleaveKind</td></tr></tbody></table><h3 id=tensormapl2promokindattr>TensorMapL2PromoKindAttr&nbsp;<a class=headline-hash href=#tensormapl2promokindattr>¶</a></h3><p><em>Tensor map L2 promotion type</em></p><p>Syntax:</p><pre tabindex=0><code>#nvgpu.l2promo&lt;
  ::mlir::nvgpu::TensorMapL2PromoKind   # value
&gt;
</code></pre><h4 id=parameters-2>Parameters:&nbsp;<a class=headline-hash href=#parameters-2>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>value</td><td style=text-align:center><code>::mlir::nvgpu::TensorMapL2PromoKind</code></td><td>an enum of type TensorMapL2PromoKind</td></tr></tbody></table><h3 id=tensormapoobkindattr>TensorMapOOBKindAttr&nbsp;<a class=headline-hash href=#tensormapoobkindattr>¶</a></h3><p><em>Tensor map out-of-bounds fill type</em></p><p>Syntax:</p><pre tabindex=0><code>#nvgpu.oob&lt;
  ::mlir::nvgpu::TensorMapOOBKind   # value
&gt;
</code></pre><h4 id=parameters-3>Parameters:&nbsp;<a class=headline-hash href=#parameters-3>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>value</td><td style=text-align:center><code>::mlir::nvgpu::TensorMapOOBKind</code></td><td>an enum of type TensorMapOOBKind</td></tr></tbody></table><h3 id=tensormapswizzlekindattr>TensorMapSwizzleKindAttr&nbsp;<a class=headline-hash href=#tensormapswizzlekindattr>¶</a></h3><p><em>Tensor map swizzling mode of shared memory banks</em></p><p>Syntax:</p><pre tabindex=0><code>#nvgpu.swizzle&lt;
  ::mlir::nvgpu::TensorMapSwizzleKind   # value
&gt;
</code></pre><h4 id=parameters-4>Parameters:&nbsp;<a class=headline-hash href=#parameters-4>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>value</td><td style=text-align:center><code>::mlir::nvgpu::TensorMapSwizzleKind</code></td><td>an enum of type TensorMapSwizzleKind</td></tr></tbody></table><h2 id=types>Types&nbsp;<a class=headline-hash href=#types>¶</a></h2><h3 id=deviceasynctokentype>DeviceAsyncTokenType&nbsp;<a class=headline-hash href=#deviceasynctokentype>¶</a></h3><p><em>Device async token type</em></p><p>Syntax: <code>!nvgpu.device.async.token</code></p><p><code>nvgpu.device.async.token</code> is a type returned by an asynchronous operation
that runs on the GPU (device). It is used to establish an SSA-based link
between the async operation (e.g. DeviceAsyncCopy) and operations that
group or synchronize the async operations (e.g. DeviceAsyncCreateGroupOp,
DeviceAsyncWaitOp).</p><h3 id=mbarriergrouptype>MBarrierGroupType&nbsp;<a class=headline-hash href=#mbarriergrouptype>¶</a></h3><p><em>Mbarrier barrier type</em></p><p>Syntax:</p><pre tabindex=0><code>!nvgpu.mbarrier.group&lt;
  Attribute,   # memorySpace
  unsigned   # num_barriers
&gt;
</code></pre><p>This is the type for one or more mbarrier object in shared memory that is
used to synchronize a variable number of threads.</p><p>If <code>num_barriers</code> is not set, the number of mbarrier objects is 1.</p><p>A mbarrier object is 64 bit with 8 byte alignment. The mbarrier object
can be initiated and invalidated.</p><p><a href=https://docs.nvidia.com/cuda/parallel-thread-execution/#size-and-alignment-of-mbarrier-object>See for more details in PTX ISA</a></p><h4 id=parameters-5>Parameters:&nbsp;<a class=headline-hash href=#parameters-5>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>memorySpace</td><td style=text-align:center><code>Attribute</code></td><td></td></tr><tr><td style=text-align:center>num_barriers</td><td style=text-align:center><code>unsigned</code></td><td></td></tr></tbody></table><h3 id=mbarriertokentype>MBarrierTokenType&nbsp;<a class=headline-hash href=#mbarriertokentype>¶</a></h3><p>Syntax: <code>!nvgpu.mbarrier.token</code></p><h3 id=tensormapdescriptortype>TensorMapDescriptorType&nbsp;<a class=headline-hash href=#tensormapdescriptortype>¶</a></h3><p><em>TensorMap descriptor</em></p><p>Syntax:</p><pre tabindex=0><code>!nvgpu.tensormap.descriptor&lt;
  MemRefType,   # tensor
  ::mlir::nvgpu::TensorMapSwizzleKind,   # swizzle
  ::mlir::nvgpu::TensorMapL2PromoKind,   # l2promo
  ::mlir::nvgpu::TensorMapOOBKind,   # oob
  ::mlir::nvgpu::TensorMapInterleaveKind   # interleave
&gt;
</code></pre><p><code>nvgpu.tma.descriptor</code> is a type that represents a TMA descriptor. It is
128-byte object either in constant space or kernel paramater.</p><h4 id=parameters-6>Parameters:&nbsp;<a class=headline-hash href=#parameters-6>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>tensor</td><td style=text-align:center><code>MemRefType</code></td><td></td></tr><tr><td style=text-align:center>swizzle</td><td style=text-align:center><code>::mlir::nvgpu::TensorMapSwizzleKind</code></td><td>an enum of type TensorMapSwizzleKind</td></tr><tr><td style=text-align:center>l2promo</td><td style=text-align:center><code>::mlir::nvgpu::TensorMapL2PromoKind</code></td><td>an enum of type TensorMapL2PromoKind</td></tr><tr><td style=text-align:center>oob</td><td style=text-align:center><code>::mlir::nvgpu::TensorMapOOBKind</code></td><td>an enum of type TensorMapOOBKind</td></tr><tr><td style=text-align:center>interleave</td><td style=text-align:center><code>::mlir::nvgpu::TensorMapInterleaveKind</code></td><td>an enum of type TensorMapInterleaveKind</td></tr></tbody></table><h3 id=warpgroupaccumulatortype>WarpgroupAccumulatorType&nbsp;<a class=headline-hash href=#warpgroupaccumulatortype>¶</a></h3><p>Syntax:</p><pre tabindex=0><code>!nvgpu.warpgroup.accumulator&lt;
  VectorType   # fragmented
&gt;
</code></pre><p>This type represents the result matrix obtained from <code>nvgpu.warpgroup.mma</code>.
The <code>$fragmented</code> type signifies the distributed or fragmented result
vector that is collectively owned by all the threads in the warp-group
that executed <code>nvgpu.warpgroup.mma</code>.
[See the details of register fragment layout for accumulator matrix D]
(
<a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#wgmma-64n16-d>https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#wgmma-64n16-d</a>)</p><h4 id=parameters-7>Parameters:&nbsp;<a class=headline-hash href=#parameters-7>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>fragmented</td><td style=text-align:center><code>VectorType</code></td><td></td></tr></tbody></table><h3 id=warpgroupmatrixdescriptortype>WarpgroupMatrixDescriptorType&nbsp;<a class=headline-hash href=#warpgroupmatrixdescriptortype>¶</a></h3><p><em>Warpgroup matrix descriptor type</em></p><p>Syntax:</p><pre tabindex=0><code>!nvgpu.warpgroup.descriptor&lt;
  MemRefType   # tensor
&gt;
</code></pre><p>The descriptor specifies the properties of the matrix in shared memory that
is a multiplicand in the matrix multiply and accumulate operation.</p><p>The descriptor is a 64-bit value contained in a register with the following:</p><pre tabindex=0><code>+---------+-----+-----------+-----+-----------+-----+-----+-----------+-----+
|   0-13  |14-15|   16-29   |30-31|   32-45   |46-48|49-51|   52-61   |62-63|
+---------+-----+-----------+-----+-----------+-----+-----+-----------+-----+
|  14bits |2bits|   14bits  |2bits|   14bits  |2bits|3bits|   10bits  |2bits|
+---------+-----+-----------+-----+-----------+-----+-----+-----------+-----+
| BaseAddr|  0  | LeadingDim|  0  |   Stride  |  0  |Offst|     0     |Swzle|
+---------+-----+-----------+-----+-----------+-----+-----+-----------+-----+
</code></pre><p><a href=https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-shared-memory-layout-matrix-descriptor>See for more details in PTX ISA</a></p><h4 id=parameters-8>Parameters:&nbsp;<a class=headline-hash href=#parameters-8>¶</a></h4><table><thead><tr><th style=text-align:center>Parameter</th><th style=text-align:center>C++ type</th><th>Description</th></tr></thead><tbody><tr><td style=text-align:center>tensor</td><td style=text-align:center><code>MemRefType</code></td><td></td></tr></tbody></table><h2 id=enums>Enums&nbsp;<a class=headline-hash href=#enums>¶</a></h2><h3 id=rcproundingmode>RcpRoundingMode&nbsp;<a class=headline-hash href=#rcproundingmode>¶</a></h3><p><em>Rounding mode of rcp</em></p><h4 id=cases>Cases:&nbsp;<a class=headline-hash href=#cases>¶</a></h4><table><thead><tr><th style=text-align:center>Symbol</th><th style=text-align:center>Value</th><th>String</th></tr></thead><tbody><tr><td style=text-align:center>APPROX</td><td style=text-align:center><code>0</code></td><td>approx</td></tr><tr><td style=text-align:center>RN</td><td style=text-align:center><code>1</code></td><td>rn</td></tr><tr><td style=text-align:center>RZ</td><td style=text-align:center><code>2</code></td><td>rz</td></tr><tr><td style=text-align:center>RM</td><td style=text-align:center><code>3</code></td><td>rm</td></tr><tr><td style=text-align:center>RP</td><td style=text-align:center><code>4</code></td><td>rp</td></tr></tbody></table><h3 id=tensormapinterleavekind>TensorMapInterleaveKind&nbsp;<a class=headline-hash href=#tensormapinterleavekind>¶</a></h3><p><em>Tensor map interleave layout type</em></p><h4 id=cases-1>Cases:&nbsp;<a class=headline-hash href=#cases-1>¶</a></h4><table><thead><tr><th style=text-align:center>Symbol</th><th style=text-align:center>Value</th><th>String</th></tr></thead><tbody><tr><td style=text-align:center>INTERLEAVE_NONE</td><td style=text-align:center><code>0</code></td><td>none</td></tr><tr><td style=text-align:center>INTERLEAVE_16B</td><td style=text-align:center><code>1</code></td><td>interleave_16b</td></tr><tr><td style=text-align:center>INTERLEAVE_32B</td><td style=text-align:center><code>2</code></td><td>interleave_32b</td></tr></tbody></table><h3 id=tensormapl2promokind>TensorMapL2PromoKind&nbsp;<a class=headline-hash href=#tensormapl2promokind>¶</a></h3><p><em>Tensor map L2 promotion type</em></p><h4 id=cases-2>Cases:&nbsp;<a class=headline-hash href=#cases-2>¶</a></h4><table><thead><tr><th style=text-align:center>Symbol</th><th style=text-align:center>Value</th><th>String</th></tr></thead><tbody><tr><td style=text-align:center>L2PROMO_NONE</td><td style=text-align:center><code>0</code></td><td>none</td></tr><tr><td style=text-align:center>L2PROMO_64B</td><td style=text-align:center><code>1</code></td><td>l2promo_64b</td></tr><tr><td style=text-align:center>L2PROMO_128B</td><td style=text-align:center><code>2</code></td><td>l2promo_128b</td></tr><tr><td style=text-align:center>L2PROMO_256B</td><td style=text-align:center><code>3</code></td><td>l2promo_256b</td></tr></tbody></table><h3 id=tensormapoobkind>TensorMapOOBKind&nbsp;<a class=headline-hash href=#tensormapoobkind>¶</a></h3><p><em>Tensor map out-of-bounds fill type</em></p><h4 id=cases-3>Cases:&nbsp;<a class=headline-hash href=#cases-3>¶</a></h4><table><thead><tr><th style=text-align:center>Symbol</th><th style=text-align:center>Value</th><th>String</th></tr></thead><tbody><tr><td style=text-align:center>OOB_ZERO</td><td style=text-align:center><code>0</code></td><td>zero</td></tr><tr><td style=text-align:center>OOB_NAN</td><td style=text-align:center><code>1</code></td><td>nan</td></tr></tbody></table><h3 id=tensormapswizzlekind>TensorMapSwizzleKind&nbsp;<a class=headline-hash href=#tensormapswizzlekind>¶</a></h3><p><em>Tensor map swizzling mode of shared memory banks</em></p><h4 id=cases-4>Cases:&nbsp;<a class=headline-hash href=#cases-4>¶</a></h4><table><thead><tr><th style=text-align:center>Symbol</th><th style=text-align:center>Value</th><th>String</th></tr></thead><tbody><tr><td style=text-align:center>SWIZZLE_NONE</td><td style=text-align:center><code>0</code></td><td>none</td></tr><tr><td style=text-align:center>SWIZZLE_32B</td><td style=text-align:center><code>1</code></td><td>swizzle_32b</td></tr><tr><td style=text-align:center>SWIZZLE_64B</td><td style=text-align:center><code>2</code></td><td>swizzle_64b</td></tr><tr><td style=text-align:center>SWIZZLE_128B</td><td style=text-align:center><code>3</code></td><td>swizzle_128b</td></tr></tbody></table><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=https://mlir.llvm.org/docs/Dialects/MPI/ title="'mpi' Dialect"><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - 'mpi' Dialect</a>
<a class="nav nav-next" href=https://mlir.llvm.org/docs/Dialects/NVVMDialect/ title="'nvvm' Dialect">Next - 'nvvm' Dialect <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=https://mlir.llvm.org/governance/>Governance</a></li><li><a href=https://mlir.llvm.org/users/>Users of MLIR</a></li><li><a href=https://mlir.llvm.org/pubs/>MLIR Related Publications</a></li><li><a href=https://mlir.llvm.org/talks/>Talks</a></li><li><a href=https://mlir.llvm.org/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://mlir.llvm.org/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://mlir.llvm.org/getting_started/Faq/>FAQ</a></li><li><a href=https://mlir.llvm.org/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://mlir.llvm.org/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=https://mlir.llvm.org/getting_started/openprojects/>Open Projects</a></li><li><a href=https://mlir.llvm.org/getting_started/Glossary/>Glossary</a></li><li><a href=https://mlir.llvm.org/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-reduce/>MLIR Reduce</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-rewrite/>mlir-rewrite</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIRTransforms/></a></li><li><a href=https://mlir.llvm.org/docs/ActionTracing/>Action: Tracing and Debugging MLIR-based Compilers</a></li><li><a href=https://mlir.llvm.org/docs/Bufferization/>Bufferization</a></li><li><a href=https://mlir.llvm.org/docs/DataLayout/>Data Layout Modeling</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/DefiningDialects/>Defining Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Constraints/>Constraints</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Assembly/>Customizing Assembly Behavior</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/AttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Operations/>Operation Definition Specification (ODS)</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/DialectConversion/>Dialect Conversion</a></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/Dialects/>Dialects<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/SMTExtensionOps/></a></li><li><a href=https://mlir.llvm.org/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMDGPU/>'amdgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArithOps/>'arith' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSME/>'ArmSME' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/BufferizationOps/>'bufferization' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ControlFlowDialect/>'cf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/DLTIDialect/>'dlti' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Func/>'func' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IndexOps/>'index' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IRDL/>'irdl' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MemRef/>'memref' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MLProgramOps/>'ml_program' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MPI/>'mpi' Dialect</a></li><li class=active><a href=https://mlir.llvm.org/docs/Dialects/NVGPU/>'nvgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/>'omp' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/ODS/>ODS Documentation</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PtrOps/>'ptr' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Shard/>'shard' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SMT/>'smt' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/UBOps/>'ub' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/VCIXDialect/>'vcix' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/WasmSSAOps/>'wasmssa' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeGPU/>'xegpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeVMDialect/>'xevm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MatchOpInterfaces/>OpInterface definitions</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SPIR-V/>SPIR-V Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Transform/>Transform Dialect</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Interfaces/>Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIR/>LLVM IR Target</a></li><li><a href=https://mlir.llvm.org/docs/BytecodeFormat/>MLIR Bytecode Format</a></li><li><a href=https://mlir.llvm.org/docs/CAPI/>MLIR C API</a></li><li><a href=https://mlir.llvm.org/docs/LangRef/>MLIR Language Reference</a></li><li><a href=https://mlir.llvm.org/docs/ReleaseNotes/>MLIR Release Notes</a></li><li><a href=https://mlir.llvm.org/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=https://mlir.llvm.org/docs/OwnershipBasedBufferDeallocation/>Ownership-based Buffer Deallocation</a></li><li><a href=https://mlir.llvm.org/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/Passes/>Passes</a></li><li><a href=https://mlir.llvm.org/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=https://mlir.llvm.org/docs/PatternSearch/>Pattern Search</a></li><li><a href=https://mlir.llvm.org/docs/PDLL/>PDLL - PDL Language</a></li><li><a href=https://mlir.llvm.org/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/SideEffectsAndSpeculation/>Side Effects & Speculation</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Remarks/>Remark Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/ShapeInference/>Shape Inference</a></li><li><a href=https://mlir.llvm.org/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=https://mlir.llvm.org/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=https://mlir.llvm.org/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Traits/>Traits<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Traits/Broadcastable/>The `Broadcastable` Trait</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/Toy/>Toy Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/transform/>Transform Dialect Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch0/>Chapter 0: A Primer on “Structured” Linalg Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch1/>Chapter 1: Combining Existing Transformations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch2/>Chapter 2: Adding a Simple New Transformation Operation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch3/>Chapter 3: More than Simple Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch4/>Chapter 4: Matching Payload with Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/ChH/>Chapter H: Reproducing Halide Schedule</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/MlirOpt/>Using `mlir-opt`</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>