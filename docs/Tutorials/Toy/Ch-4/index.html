<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Chapter 4: Enabling Generic Transformation with Interfaces - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.119.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-4/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script>
<link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script>
<script src=https://mlir.llvm.org/js/bundle.js></script>
<script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li></ul></li><li><a href="https://github.com/llvm/llvm-project/issues?q=is%3Aissue%20state%3Aopen%20label%3Amlir">Bugs</a></li><li><a href=https://github.com/llvm/mlir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/MLIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>Chapter 4: Enabling Generic Transformation with Interfaces</h1><p><nav id=TableOfContents><ul><li><a href=#background-grappling-with-an-extensible-ir>Background: Grappling with an Extensible IR</a></li><li><a href=#shape-inference-preparing-for-code-generation>Shape Inference: Preparing for Code Generation</a><ul><li><a href=#inlining>Inlining</a></li><li><a href=#intraprocedural-shape-inference>Intraprocedural Shape Inference</a></li></ul></li></ul></nav><h2 id=background-grappling-with-an-extensible-ir>Background: Grappling with an Extensible IR&nbsp;<a class=headline-hash href=#background-grappling-with-an-extensible-ir>¶</a></h2><p>Through dialects, MLIR allows for the representation of many different levels of
abstraction; the Toy dialect that we have previously defined is one such
example. Though these different dialects may represent different abstractions,
there is often a set of common transformations and analyses that we would like
to perform. The problem that arises is that naively implementing each
transformation for each dialect leads to large amounts of code duplication, as
the internal algorithms are generally very similar, if not the same. We would
like to provide the ability for transformations to opaquely hook into dialects
like Toy to get the information they need.</p><p>MLIR provides a set of always available-hooks for certain core transformations,
as seen in the
<a href=/docs/Tutorials/Toy/Ch-3/>previous chapter</a>, where we registered some
canonicalizations via a hook on our operations (<code>getCanonicalizationPatterns</code>).
However, these types of hooks don&rsquo;t really scale well. Therefore, a more generic
solution was designed, in the form of
<a href=/docs/Interfaces/>interfaces</a>, to make
the MLIR infrastructure as extensible as the representation. Interfaces provide
a generic mechanism for dialects and operations to provide information to a
transformation or analysis.</p><h2 id=shape-inference-preparing-for-code-generation>Shape Inference: Preparing for Code Generation&nbsp;<a class=headline-hash href=#shape-inference-preparing-for-code-generation>¶</a></h2><p>Our Toy IR currently operates on generic tensors, meaning that we don&rsquo;t know the
shape of tensors other than during the initialization of constants. This
complicates optimizations, as well as code generation. Fortunately, we can
simply propagate the shapes through the computation until they are all known.
The issue is how to handle calls to user-defined generic functions: every call
site could deduce different shapes. One possibility would be to perform symbolic
inference based on the argument types, but this would be hard to generalize if
we were to introduce more control flow in the language. Another approach would
be function specialization, where every call site with new argument shapes
duplicates the called function and specializes it. The approach we take for Toy
is to inline all of the function calls, then perform intraprocedural shape
propagation.</p><h3 id=inlining>Inlining&nbsp;<a class=headline-hash href=#inlining>¶</a></h3><p>Here we could write an inlining algorithm specifically designed for the Toy
dialect, but that can become quite complicated depending on the level of
complexity that we want. Disregarding cost modeling, the pure structural
transformation is already complex to implement from scratch. Thankfully, MLIR
provides a generic inliner algorithm that dialects can plug into. All we need to
do in Toy is to provide the
<a href=/docs/Interfaces/>interfaces</a> for the inliner to
hook into.</p><p>The first thing we need to do is to define the constraints on inlining
operations in the Toy dialect. This information is provided through a
<a href=/docs/Interfaces/#dialect-interfaces>dialect interface</a>. This is essentially
a class containing a set of virtual hooks which the dialect can override.
In this case, the interface is <code>DialectInlinerInterface</code>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=c1>/// This class defines the interface for handling inlining with Toy operations.
</span></span></span><span class=line><span class=cl><span class=c1>/// We simplify inherit from the base interface class and override
</span></span></span><span class=line><span class=cl><span class=c1>/// the necessary methods.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>struct</span> <span class=nc>ToyInlinerInterface</span> <span class=o>:</span> <span class=k>public</span> <span class=n>DialectInlinerInterface</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>using</span> <span class=n>DialectInlinerInterface</span><span class=o>::</span><span class=n>DialectInlinerInterface</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>/// This hook checks to see if the given callable operation is legal to inline
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// into the given call. For Toy this hook can simply return true, as the Toy
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// Call operation is always inlinable.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>bool</span> <span class=nf>isLegalToInline</span><span class=p>(</span><span class=n>Operation</span> <span class=o>*</span><span class=n>call</span><span class=p>,</span> <span class=n>Operation</span> <span class=o>*</span><span class=n>callable</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=kt>bool</span> <span class=n>wouldBeCloned</span><span class=p>)</span> <span class=k>const</span> <span class=k>final</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>true</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>/// This hook checks to see if the given operation is legal to inline into the
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// given region. For Toy this hook can simply return true, as all Toy
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// operations are inlinable.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>bool</span> <span class=nf>isLegalToInline</span><span class=p>(</span><span class=n>Operation</span> <span class=o>*</span><span class=p>,</span> <span class=n>Region</span> <span class=o>*</span><span class=p>,</span> <span class=kt>bool</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>IRMapping</span> <span class=o>&amp;</span><span class=p>)</span> <span class=k>const</span> <span class=k>final</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>true</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>/// This hook cheks if the given &#39;src&#39; region can be inlined into the &#39;dest&#39;
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// region. The regions here are the bodies of the callable functions. For
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// Toy, any function can be inlined, so we simply return true.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>bool</span> <span class=nf>isLegalToInline</span><span class=p>(</span><span class=n>Region</span> <span class=o>*</span><span class=n>dest</span><span class=p>,</span> <span class=n>Region</span> <span class=o>*</span><span class=n>src</span><span class=p>,</span> <span class=kt>bool</span> <span class=n>wouldBeCloned</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                       <span class=n>IRMapping</span> <span class=o>&amp;</span><span class=n>valueMapping</span><span class=p>)</span> <span class=k>const</span> <span class=k>final</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>true</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>/// This hook is called when a terminator operation has been inlined. The only
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// terminator that we have in the Toy dialect is the return
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// operation(toy.return). We handle the return by replacing the values
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// previously returned by the call operation with the operands of the
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// return.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>void</span> <span class=nf>handleTerminator</span><span class=p>(</span><span class=n>Operation</span> <span class=o>*</span><span class=n>op</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                        <span class=n>ValueRange</span> <span class=n>valuesToRepl</span><span class=p>)</span> <span class=k>const</span> <span class=k>final</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Only &#34;toy.return&#34; needs to be handled here.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>auto</span> <span class=n>returnOp</span> <span class=o>=</span> <span class=n>cast</span><span class=o>&lt;</span><span class=n>ReturnOp</span><span class=o>&gt;</span><span class=p>(</span><span class=n>op</span><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>// Replace the values directly with the return operands.
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>assert</span><span class=p>(</span><span class=n>returnOp</span><span class=p>.</span><span class=n>getNumOperands</span><span class=p>()</span> <span class=o>==</span> <span class=n>valuesToRepl</span><span class=p>.</span><span class=n>size</span><span class=p>());</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=p>(</span><span class=k>const</span> <span class=k>auto</span> <span class=o>&amp;</span><span class=nl>it</span> <span class=p>:</span> <span class=n>llvm</span><span class=o>::</span><span class=n>enumerate</span><span class=p>(</span><span class=n>returnOp</span><span class=p>.</span><span class=n>getOperands</span><span class=p>()))</span>
</span></span><span class=line><span class=cl>      <span class=n>valuesToRepl</span><span class=p>[</span><span class=n>it</span><span class=p>.</span><span class=n>index</span><span class=p>()].</span><span class=n>replaceAllUsesWith</span><span class=p>(</span><span class=n>it</span><span class=p>.</span><span class=n>value</span><span class=p>());</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><p>Besides, the inliner will only discard private-visible unused function
definitions. We also have to set the visibility of functions (except the
main function) in the MLIR generator.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=c1>/// Emit a new function and add it to the MLIR module.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>mlir</span><span class=o>::</span><span class=n>toy</span><span class=o>::</span><span class=n>FuncOp</span> <span class=n>mlirGen</span><span class=p>(</span><span class=n>FunctionAST</span> <span class=o>&amp;</span><span class=n>funcAST</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=c1>// If this function isn&#39;t main, then set the visibility to private.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>if</span> <span class=p>(</span><span class=n>funcAST</span><span class=p>.</span><span class=n>getProto</span><span class=p>()</span><span class=o>-&gt;</span><span class=n>getName</span><span class=p>()</span> <span class=o>!=</span> <span class=s>&#34;main&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>function</span><span class=p>.</span><span class=n>setPrivate</span><span class=p>();</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>function</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We then register our dialect interface directly on the Toy dialect, similarly to
how we did for operations.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=kt>void</span> <span class=n>ToyDialect</span><span class=o>::</span><span class=n>initialize</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>addInterfaces</span><span class=o>&lt;</span><span class=n>ToyInlinerInterface</span><span class=o>&gt;</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Next, we need to provide a way for the inliner to know that <code>toy.generic_call</code>
represents a call, and <code>toy.func</code> represents a function. MLIR provides
<a href=/docs/Interfaces/#attributeoperationtype-interfaces>operation interfaces</a> that can be used
to mark an operation as being &ldquo;call-like&rdquo; or &ldquo;callable-like&rdquo;. Unlike dialect interfaces,
operation interfaces provide a more refined granularity of information that is specific
and core to a single operation. The interfaces that we will be adding here is the
<code>CallOpInterface</code> and <code>CallableOpInterface</code>.</p><p>To add this interface we just need to include the definition into our operation
specification file (<code>Ops.td</code>):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-tablegen data-lang=tablegen><span class=line><span class=cl><span class=nv>include</span> <span class=s>&#34;mlir/Interfaces/CallInterfaces.td&#34;</span>
</span></span></code></pre></div><p>and add it to the traits list of <code>GenericCallOp</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-tablegen data-lang=tablegen><span class=line><span class=cl><span class=k>def</span> <span class=nv>FuncOp</span> <span class=p>:</span> <span class=nv>Toy_Op</span><span class=p>&lt;</span><span class=s>&#34;func&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=nv>FunctionOpInterface</span><span class=p>,</span> <span class=nv>IsolatedFromAbove</span><span class=p>]&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nv>GenericCallOp</span> <span class=p>:</span> <span class=nv>Toy_Op</span><span class=p>&lt;</span><span class=s>&#34;generic_call&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>[</span><span class=nv>DeclareOpInterfaceMethods</span><span class=p>&lt;</span><span class=nv>CallOpInterface</span><span class=p>&gt;]&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>In the above we also use the <code>DeclareOpInterfaceMethods</code> directive to
auto-declare all of the interface methods in the class declaration of
<code>GenericCallOp</code>. However, using this directive with <code>CallOpInterface</code>
includes methods for handling argument and result attributes. Therefore,
we need to add these specifically named attributes to our <code>GenericCallOp</code>
definition:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-tablegen data-lang=tablegen><span class=line><span class=cl><span class=k>let</span> <span class=nv>arguments</span> <span class=p>=</span> <span class=p>(</span><span class=nv>ins</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=nv>OptionalAttr</span><span class=p>&lt;</span><span class=nv>DictArrayAttr</span><span class=p>&gt;:</span><span class=nv>$arg_attrs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nv>OptionalAttr</span><span class=p>&lt;</span><span class=nv>DictArrayAttr</span><span class=p>&gt;:</span><span class=nv>$res_attrs</span>
</span></span><span class=line><span class=cl><span class=p>);</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>We</span> <span class=nv>have</span> <span class=nv>already</span> <span class=nv>provided</span> <span class=nv>the</span> <span class=nv>definition</span> <span class=k>in</span> <span class=nv>the</span> <span class=err>`</span><span class=nv>extraClassDeclaration</span><span class=err>`</span>
</span></span><span class=line><span class=cl><span class=k>field</span> <span class=nv>of</span> <span class=nv>the</span> <span class=err>`</span><span class=nv>FuncOp</span><span class=err>`</span> <span class=k>class</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=err>```</span><span class=nv>c</span><span class=err>++</span>
</span></span><span class=line><span class=cl><span class=c>/// Returns the region on the function operation that is callable.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>Region</span> <span class=p>*</span><span class=nv>FuncOp</span><span class=p>::</span><span class=nv>getCallableRegion</span><span class=p>()</span> <span class=p>{</span> <span class=nv>return</span> <span class=err>&amp;</span><span class=nv>getBody</span><span class=p>();</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// ....
</span></span></span><span class=line><span class=cl><span class=c></span>
</span></span><span class=line><span class=cl><span class=c>/// Return the callee of the generic call operation, this is required by the
</span></span></span><span class=line><span class=cl><span class=c>/// call interface.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>CallInterfaceCallable</span> <span class=nv>GenericCallOp</span><span class=p>::</span><span class=nv>getCallableForCallee</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>return</span> <span class=p>(*</span><span class=nv>this</span><span class=p>)</span><span class=err>-</span><span class=p>&gt;</span><span class=nv>getAttrOfType</span><span class=p>&lt;</span><span class=nv>SymbolRefAttr</span><span class=p>&gt;(</span><span class=s>&#34;callee&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>/// Set the callee for the generic call operation, this is required by the call
</span></span></span><span class=line><span class=cl><span class=c>/// interface.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>void</span> <span class=nv>GenericCallOp</span><span class=p>::</span><span class=nv>setCalleeFromCallable</span><span class=p>(</span><span class=nv>CallInterfaceCallable</span> <span class=nv>callee</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=p>(*</span><span class=nv>this</span><span class=p>)</span><span class=err>-</span><span class=p>&gt;</span><span class=nv>setAttr</span><span class=p>(</span><span class=s>&#34;callee&#34;</span><span class=p>,</span> <span class=nv>callee</span><span class=p>.</span><span class=nv>get</span><span class=p>&lt;</span><span class=nv>SymbolRefAttr</span><span class=p>&gt;());</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>/// Get the argument operands to the called function, this is required by the
</span></span></span><span class=line><span class=cl><span class=c>/// call interface.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>Operation</span><span class=p>::</span><span class=nv>operand_range</span> <span class=nv>GenericCallOp</span><span class=p>::</span><span class=nv>getArgOperands</span><span class=p>()</span> <span class=p>{</span> <span class=nv>return</span> <span class=nv>getInputs</span><span class=p>();</span> <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>/// Get the argument operands to the called function as a mutable range, this is
</span></span></span><span class=line><span class=cl><span class=c>/// required by the call interface.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>MutableOperandRange</span> <span class=nv>GenericCallOp</span><span class=p>::</span><span class=nv>getArgOperandsMutable</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>return</span> <span class=nv>getInputsMutable</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Now that the inliner has been informed about the Toy dialect, we can add the
inliner pass to the pass manager for Toy:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>  <span class=n>pm</span><span class=p>.</span><span class=n>addPass</span><span class=p>(</span><span class=n>mlir</span><span class=o>::</span><span class=n>createInlinerPass</span><span class=p>());</span>
</span></span></code></pre></div><p>Now let&rsquo;s look at a working example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>toy<span class=p>.</span><span class=kt>func</span> <span class=nf>@multiply_transpose</span><span class=p>(</span><span class=nv>%arg0</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;,</span> <span class=nv>%arg1</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%0</span> <span class=p>=</span> toy<span class=p>.</span>transpose<span class=p>(</span><span class=nv>%arg0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;)</span> to <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%1</span> <span class=p>=</span> toy<span class=p>.</span>transpose<span class=p>(</span><span class=nv>%arg1</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;)</span> to <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%2</span> <span class=p>=</span> toy<span class=p>.</span>mul <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  toy<span class=p>.</span><span class=kt>return</span> <span class=nv>%2</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>toy<span class=p>.</span><span class=kt>func</span> <span class=nf>@main</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%0</span> <span class=p>=</span> toy<span class=p>.</span><span class=kt>constant</span> dense<span class=p>&lt;[[</span><span class=m>1.000000e+00</span><span class=p>,</span> <span class=m>2.000000e+00</span><span class=p>,</span> <span class=m>3.000000e+00</span><span class=p>],</span> <span class=p>[</span><span class=m>4.000000e+00</span><span class=p>,</span> <span class=m>5.000000e+00</span><span class=p>,</span> <span class=m>6.000000e+00</span><span class=p>]]&gt;</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%1</span> <span class=p>=</span> toy<span class=p>.</span>reshape<span class=p>(</span><span class=nv>%0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;)</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%2</span> <span class=p>=</span> toy<span class=p>.</span><span class=kt>constant</span> dense<span class=p>&lt;[</span><span class=m>1.000000e+00</span><span class=p>,</span> <span class=m>2.000000e+00</span><span class=p>,</span> <span class=m>3.000000e+00</span><span class=p>,</span> <span class=m>4.000000e+00</span><span class=p>,</span> <span class=m>5.000000e+00</span><span class=p>,</span> <span class=m>6.000000e+00</span><span class=p>]&gt;</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>6x</span><span class=k>f64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%3</span> <span class=p>=</span> toy<span class=p>.</span>reshape<span class=p>(</span><span class=nv>%2</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>6x</span><span class=k>f64</span><span class=p>&gt;)</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%4</span> <span class=p>=</span> toy<span class=p>.</span>generic_call <span class=nf>@multiply_transpose</span><span class=p>(</span><span class=nv>%1</span><span class=p>,</span> <span class=nv>%3</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%5</span> <span class=p>=</span> toy<span class=p>.</span>generic_call <span class=nf>@multiply_transpose</span><span class=p>(</span><span class=nv>%3</span><span class=p>,</span> <span class=nv>%1</span><span class=p>)</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  toy<span class=p>.</span>print <span class=nv>%5</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  toy<span class=p>.</span><span class=kt>return</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>We have two calls to multiply_transpose that we would like to inline into main,
but if we look at the output nothing has changed. We are missing one last subtle
piece: there is a hidden type conversion on the edge of the call. If we look at
the above, the operands to the generic_call are of type <code>tensor&lt;2x3xf64></code>, while
the inputs to the function expect <code>tensor&lt;*xf64></code>. To resolve this difference,
the inliner expects an explicit cast operation to be inserted. For this, we need
to add a new operation to the Toy dialect, <code>ToyCastOp</code>(toy.cast), to represent
casts between two different shapes.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-tablegen data-lang=tablegen><span class=line><span class=cl><span class=k>def</span> <span class=nv>CastOp</span> <span class=p>:</span> <span class=nv>Toy_Op</span><span class=p>&lt;</span><span class=s>&#34;cast&#34;</span><span class=p>,</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=nv>DeclareOpInterfaceMethods</span><span class=p>&lt;</span><span class=nv>CastOpInterface</span><span class=p>&gt;,</span>
</span></span><span class=line><span class=cl>    <span class=nv>Pure</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nv>SameOperandsAndResultShape</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>let</span> <span class=nv>summary</span> <span class=p>=</span> <span class=s>&#34;shape cast operation&#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=k>let</span> <span class=nv>description</span> <span class=p>=</span> <span class=s>[{
</span></span></span><span class=line><span class=cl><span class=s>    The &#34;cast&#34; operation converts a tensor from one type to an equivalent type
</span></span></span><span class=line><span class=cl><span class=s>    without changing any data elements. The source and destination types
</span></span></span><span class=line><span class=cl><span class=s>    must both be tensor types with the same element type. If both are ranked,
</span></span></span><span class=line><span class=cl><span class=s>    then shape is required to match. The operation is invalid if converting
</span></span></span><span class=line><span class=cl><span class=s>    to a mismatching constant dimension.
</span></span></span><span class=line><span class=cl><span class=s>  }]</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>let</span> <span class=nv>arguments</span> <span class=p>=</span> <span class=p>(</span><span class=nv>ins</span> <span class=nv>F64Tensor</span><span class=p>:</span><span class=nv>$input</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=k>let</span> <span class=nv>results</span> <span class=p>=</span> <span class=p>(</span><span class=nv>outs</span> <span class=nv>F64Tensor</span><span class=p>:</span><span class=nv>$output</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=k>let</span> <span class=nv>assemblyFormat</span> <span class=p>=</span> <span class=s>&#34;$input attr-dict `:` type($input) `to` type($output)&#34;</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Note that the definition of this cast operation adds a <code>CastOpInterface</code> to the
traits list. This interface provides several utilities for cast-like operation,
such as folding identity casts and verification. We hook into this interface by
providing a definition for the <code>areCastCompatible</code> method:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=c1>/// Returns true if the given set of input and result types are compatible with
</span></span></span><span class=line><span class=cl><span class=c1>/// this cast operation. This is required by the `CastOpInterface` to verify
</span></span></span><span class=line><span class=cl><span class=c1>/// this operation and provide other additional utilities.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>bool</span> <span class=n>CastOp</span><span class=o>::</span><span class=n>areCastCompatible</span><span class=p>(</span><span class=n>TypeRange</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>TypeRange</span> <span class=n>outputs</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=n>inputs</span><span class=p>.</span><span class=n>size</span><span class=p>()</span> <span class=o>!=</span> <span class=mi>1</span> <span class=o>||</span> <span class=n>outputs</span><span class=p>.</span><span class=n>size</span><span class=p>()</span> <span class=o>!=</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>false</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=c1>// The inputs must be Tensors with the same element type.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>TensorType</span> <span class=n>input</span> <span class=o>=</span> <span class=n>llvm</span><span class=o>::</span><span class=n>dyn_cast</span><span class=o>&lt;</span><span class=n>TensorType</span><span class=o>&gt;</span><span class=p>(</span><span class=n>inputs</span><span class=p>.</span><span class=n>front</span><span class=p>());</span>
</span></span><span class=line><span class=cl>  <span class=n>TensorType</span> <span class=n>output</span> <span class=o>=</span> <span class=n>llvm</span><span class=o>::</span><span class=n>dyn_cast</span><span class=o>&lt;</span><span class=n>TensorType</span><span class=o>&gt;</span><span class=p>(</span><span class=n>outputs</span><span class=p>.</span><span class=n>front</span><span class=p>());</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=p>(</span><span class=o>!</span><span class=n>input</span> <span class=o>||</span> <span class=o>!</span><span class=n>output</span> <span class=o>||</span> <span class=n>input</span><span class=p>.</span><span class=n>getElementType</span><span class=p>()</span> <span class=o>!=</span> <span class=n>output</span><span class=p>.</span><span class=n>getElementType</span><span class=p>())</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nb>false</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=c1>// The shape is required to match if both types are ranked.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>return</span> <span class=o>!</span><span class=n>input</span><span class=p>.</span><span class=n>hasRank</span><span class=p>()</span> <span class=o>||</span> <span class=o>!</span><span class=n>output</span><span class=p>.</span><span class=n>hasRank</span><span class=p>()</span> <span class=o>||</span> <span class=n>input</span> <span class=o>==</span> <span class=n>output</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>With a proper cast operation, we can now override the necessary hook on the
ToyInlinerInterface to insert it for us when necessary:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>struct</span> <span class=nc>ToyInlinerInterface</span> <span class=o>:</span> <span class=k>public</span> <span class=n>DialectInlinerInterface</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>/// Attempts to materialize a conversion for a type mismatch between a call
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// from this dialect, and a callable region. This method should generate an
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// operation that takes &#39;input&#39; as the only operand, and produces a single
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// result of &#39;resultType&#39;. If a conversion can not be generated, nullptr
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=c1>/// should be returned.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>Operation</span> <span class=o>*</span><span class=n>materializeCallConversion</span><span class=p>(</span><span class=n>OpBuilder</span> <span class=o>&amp;</span><span class=n>builder</span><span class=p>,</span> <span class=n>Value</span> <span class=n>input</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                       <span class=n>Type</span> <span class=n>resultType</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                       <span class=n>Location</span> <span class=n>conversionLoc</span><span class=p>)</span> <span class=k>const</span> <span class=k>final</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>CastOp</span><span class=o>::</span><span class=n>create</span><span class=p>(</span><span class=n>builder</span><span class=p>,</span> <span class=n>conversionLoc</span><span class=p>,</span> <span class=n>resultType</span><span class=p>,</span> <span class=n>input</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><p>If we run the working example through the pipeline again, we get the expected:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>toy<span class=p>.</span><span class=kt>func</span> <span class=nf>@main</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%0</span> <span class=p>=</span> toy<span class=p>.</span><span class=kt>constant</span> dense<span class=p>&lt;[[</span><span class=m>1.000000e+00</span><span class=p>,</span> <span class=m>2.000000e+00</span><span class=p>,</span> <span class=m>3.000000e+00</span><span class=p>],</span> <span class=p>[</span><span class=m>4.000000e+00</span><span class=p>,</span> <span class=m>5.000000e+00</span><span class=p>,</span> <span class=m>6.000000e+00</span><span class=p>]]&gt;</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%1</span> <span class=p>=</span> toy<span class=p>.</span><span class=kt>constant</span> dense<span class=p>&lt;[[</span><span class=m>1.000000e+00</span><span class=p>,</span> <span class=m>2.000000e+00</span><span class=p>,</span> <span class=m>3.000000e+00</span><span class=p>],</span> <span class=p>[</span><span class=m>4.000000e+00</span><span class=p>,</span> <span class=m>5.000000e+00</span><span class=p>,</span> <span class=m>6.000000e+00</span><span class=p>]]&gt;</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%2</span> <span class=p>=</span> toy<span class=p>.</span>cast <span class=nv>%1</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%3</span> <span class=p>=</span> toy<span class=p>.</span>cast <span class=nv>%0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%4</span> <span class=p>=</span> toy<span class=p>.</span>transpose<span class=p>(</span><span class=nv>%2</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;)</span> to <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%5</span> <span class=p>=</span> toy<span class=p>.</span>transpose<span class=p>(</span><span class=nv>%3</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;)</span> to <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%6</span> <span class=p>=</span> toy<span class=p>.</span>mul <span class=nv>%4</span><span class=p>,</span> <span class=nv>%5</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  toy<span class=p>.</span>print <span class=nv>%6</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;*</span>xf64<span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  toy<span class=p>.</span><span class=kt>return</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>NOTE: The generic inliner will also perform simplifications, so the output may
be a bit cleaner than expected.</p><h3 id=intraprocedural-shape-inference>Intraprocedural Shape Inference&nbsp;<a class=headline-hash href=#intraprocedural-shape-inference>¶</a></h3><p>Now that we have inlined all of the functions, we are left with a main function
containing a mix of static and dynamically shaped operations. We can now write a
simple shape inference pass to propagate shapes intraprocedurally (within a
single function). We could write this as a pass that directly encodes the
constraints of the operations within the Toy dialect, but this seems like a good
candidate for a transformation that could be written generically. As a good rule
of thumb, it is best to express a transformation as generically as possible,
such that it can be extended to other dialects in the future. There is no
telling how many other dialects may have similar needs or encounter the same
problems.</p><p>For shape inference, if we break down the problem to its core, we really just
want operations to tell us the expected outputs given a set of statically known
inputs. (We can definitely get more complex than that, but for our needs we can
keep it simple.) Given that this property is core to a specific operation, we
can define an operation interface that can be specified on operations that need
to have their result shapes inferred.</p><p>Similarly to operations, we can also
<a href=/docs/Interfaces/#attributeoperationtype-interfaces>define operation interfaces</a> using
the operation definition specification (ODS) framework.</p><p>The interface is defined by inheriting from <code>OpInterface</code>, which takes the name
to be given to the generated C++ interface class as a template argument. For our
purposes, we will simply name the generated class <code>ShapeInference</code>. We also
provide a description for the interface.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-tablegen data-lang=tablegen><span class=line><span class=cl><span class=k>def</span> <span class=nv>ShapeInferenceOpInterface</span> <span class=p>:</span> <span class=nv>OpInterface</span><span class=p>&lt;</span><span class=s>&#34;ShapeInference&#34;</span><span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>let</span> <span class=nv>description</span> <span class=p>=</span> <span class=s>[{
</span></span></span><span class=line><span class=cl><span class=s>    Interface to access a registered method to infer the return types for an
</span></span></span><span class=line><span class=cl><span class=s>    operation that can be used during type inference.
</span></span></span><span class=line><span class=cl><span class=s>  }]</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Next, we define the interface methods that the operations will need to provide.
An interface method is comprised of: a description; a C++ return type in string
form; a method name in string form; and a few optional components, depending on
the need. See the
<a href=/docs/Interfaces/#attributeoperationtype-interfaces>ODS documentation</a> for more
information.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-tablegen data-lang=tablegen><span class=line><span class=cl><span class=k>def</span> <span class=nv>ShapeInferenceOpInterface</span> <span class=p>:</span> <span class=nv>OpInterface</span><span class=p>&lt;</span><span class=s>&#34;ShapeInference&#34;</span><span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=k>let</span> <span class=nv>methods</span> <span class=p>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=nv>InterfaceMethod</span><span class=p>&lt;</span><span class=s>&#34;Infer and set the output shape for the current operation.&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=s>&#34;void&#34;</span><span class=p>,</span> <span class=s>&#34;inferShapes&#34;</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=p>];</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Now that the interface is defined, we can add it to the necessary Toy operations
in a similar way to how we added the <code>CallOpInterface</code> to the GenericCallOp:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-tablegen data-lang=tablegen><span class=line><span class=cl><span class=k>def</span> <span class=nv>MulOp</span> <span class=p>:</span> <span class=nv>Toy_Op</span><span class=p>&lt;</span><span class=s>&#34;mul&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=p>[...,</span> <span class=nv>DeclareOpInterfaceMethods</span><span class=p>&lt;</span><span class=nv>ShapeInferenceOpInterface</span><span class=p>&gt;]&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=p>...</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Each of these operations will then need to provide a definition for the
<code>inferShapes()</code> method. As an example, for the mul op, the result shape is
inferred as the shape of the inputs.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=c1>/// Infer the output shape of the MulOp, this is required by the shape inference
</span></span></span><span class=line><span class=cl><span class=c1>/// interface.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>void</span> <span class=n>MulOp</span><span class=o>::</span><span class=n>inferShapes</span><span class=p>()</span> <span class=p>{</span> <span class=n>getResult</span><span class=p>().</span><span class=n>setType</span><span class=p>(</span><span class=n>getLhs</span><span class=p>().</span><span class=n>getType</span><span class=p>());</span> <span class=p>}</span>
</span></span></code></pre></div><p>At this point, each of the necessary Toy operations provide a mechanism by which
to infer their output shapes. The ShapeInferencePass will operate on functions:
it will run on each function in isolation. MLIR also supports general
<a href=/docs/PassManagement/#operation-pass>OperationPasses</a> that run on any
isolated operation, but here our module only contains functions, so there is no
need to generalize to all operations.</p><p>Implementing such a pass is done by creating a class inheriting from
<code>mlir::OperationPass&lt;FuncOp></code> and overriding the <code>runOnOperation()</code> method.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=k>class</span> <span class=nc>ShapeInferencePass</span>
</span></span><span class=line><span class=cl>    <span class=o>:</span> <span class=k>public</span> <span class=n>mlir</span><span class=o>::</span><span class=n>PassWrapper</span><span class=o>&lt;</span><span class=n>ShapeInferencePass</span><span class=p>,</span> <span class=n>OperationPass</span><span class=o>&lt;</span><span class=n>FuncOp</span><span class=o>&gt;&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=kt>void</span> <span class=nf>runOnOperation</span><span class=p>()</span> <span class=k>override</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>FuncOp</span> <span class=n>function</span> <span class=o>=</span> <span class=n>getOperation</span><span class=p>();</span>
</span></span><span class=line><span class=cl>    <span class=p>...</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>};</span>
</span></span></code></pre></div><p>While at it, let&rsquo;s also create a helper method for instantiating the pass:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=n>std</span><span class=o>::</span><span class=n>unique_ptr</span><span class=o>&lt;</span><span class=n>mlir</span><span class=o>::</span><span class=n>Pass</span><span class=o>&gt;</span> <span class=n>mlir</span><span class=o>::</span><span class=n>toy</span><span class=o>::</span><span class=n>createShapeInferencePass</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>std</span><span class=o>::</span><span class=n>make_unique</span><span class=o>&lt;</span><span class=n>ShapeInferencePass</span><span class=o>&gt;</span><span class=p>();</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>The shape inference algorithm operates as follows:</p><ol><li>Build a worklist containing all the operations that return a dynamically
shaped tensor: these are the operations that need shape inference.</li><li>Iterate on the worklist:<ul><li>find an operation to process: the next ready operation in the worklist
has all of its arguments non-generic,</li><li>if no operation is found, break out of the loop,</li><li>remove the operation from the worklist,</li><li>infer the shape of its output from the argument types.</li></ul></li><li>If the worklist is empty, the algorithm succeeded.</li></ol><p>When processing an operation like described, we query if it registered the
<code>ShapeInference</code> interface, using this code snippet:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>  <span class=c1>// Ask the operation to infer its output shapes.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>LDBG</span><span class=p>()</span> <span class=o>&lt;&lt;</span> <span class=s>&#34;Inferring shape for: &#34;</span> <span class=o>&lt;&lt;</span> <span class=o>*</span><span class=n>op</span><span class=p>;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1>/// We check if an operation has a particular interface by casting.
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>if</span> <span class=p>(</span><span class=n>ShapeInference</span> <span class=n>shapeOp</span> <span class=o>=</span> <span class=n>dyn_cast</span><span class=o>&lt;</span><span class=n>ShapeInference</span><span class=o>&gt;</span><span class=p>(</span><span class=n>op</span><span class=p>))</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>shapeOp</span><span class=p>.</span><span class=n>inferShapes</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span> <span class=k>else</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=n>op</span><span class=o>-&gt;</span><span class=n>emitError</span><span class=p>(</span><span class=s>&#34;unable to infer shape of operation without shape &#34;</span>
</span></span><span class=line><span class=cl>                  <span class=s>&#34;inference interface&#34;</span><span class=p>);</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nf>signalPassFailure</span><span class=p>();</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span></code></pre></div><p>We can then add our pass to the pass manager:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl>  <span class=n>pm</span><span class=p>.</span><span class=n>addPass</span><span class=p>(</span><span class=n>mlir</span><span class=o>::</span><span class=n>createShapeInferencePass</span><span class=p>());</span>
</span></span></code></pre></div><p>If we rerun our original example, we now get the following:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>toy<span class=p>.</span><span class=kt>func</span> <span class=nf>@main</span><span class=p>()</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%0</span> <span class=p>=</span> toy<span class=p>.</span><span class=kt>constant</span> dense<span class=p>&lt;[[</span><span class=m>1.000000e+00</span><span class=p>,</span> <span class=m>2.000000e+00</span><span class=p>,</span> <span class=m>3.000000e+00</span><span class=p>],</span> <span class=p>[</span><span class=m>4.000000e+00</span><span class=p>,</span> <span class=m>5.000000e+00</span><span class=p>,</span> <span class=m>6.000000e+00</span><span class=p>]]&gt;</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%1</span> <span class=p>=</span> toy<span class=p>.</span>transpose<span class=p>(</span><span class=nv>%0</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x3x</span><span class=k>f64</span><span class=p>&gt;)</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>3x2x</span><span class=k>f64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%2</span> <span class=p>=</span> toy<span class=p>.</span>mul <span class=nv>%1</span><span class=p>,</span> <span class=nv>%1</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>3x2x</span><span class=k>f64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  toy<span class=p>.</span>print <span class=nv>%2</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>3x2x</span><span class=k>f64</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  toy<span class=p>.</span><span class=kt>return</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>You can build <code>toyc-ch4</code> and try yourself: <code>toyc-ch4 test/Examples/Toy/Ch4/codegen.toy -emit=mlir -opt</code>.</p><p>In the
<a href=/docs/Tutorials/Toy/Ch-5/>next chapter</a>, we will start the process of code generation by
targeting a lower level dialect for optimizing some of the more compute-heavy
Toy operations.</p><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-3/ title="Chapter 3: High-level Language-Specific Analysis and Transformation"><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - Chapter 3: High-level Language-Specific Analysis and Transformation</a>
<a class="nav nav-next" href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-5/ title="Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization">Next - Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=https://mlir.llvm.org/governance/>Governance</a></li><li><a href=https://mlir.llvm.org/users/>Users of MLIR</a></li><li><a href=https://mlir.llvm.org/pubs/>MLIR Related Publications</a></li><li><a href=https://mlir.llvm.org/talks/>Talks</a></li><li><a href=https://mlir.llvm.org/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://mlir.llvm.org/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://mlir.llvm.org/getting_started/Faq/>FAQ</a></li><li><a href=https://mlir.llvm.org/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://mlir.llvm.org/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=https://mlir.llvm.org/getting_started/openprojects/>Open Projects</a></li><li><a href=https://mlir.llvm.org/getting_started/Glossary/>Glossary</a></li><li><a href=https://mlir.llvm.org/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-reduce/>MLIR Reduce</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-rewrite/>mlir-rewrite</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/ActionTracing/>Action: Tracing and Debugging MLIR-based Compilers</a></li><li><a href=https://mlir.llvm.org/docs/Bufferization/>Bufferization</a></li><li><a href=https://mlir.llvm.org/docs/DataLayout/>Data Layout Modeling</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/DefiningDialects/>Defining Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Constraints/>Constraints</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Assembly/>Customizing Assembly Behavior</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/AttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Operations/>Operation Definition Specification (ODS)</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/DialectConversion/>Dialect Conversion</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/>Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMDGPU/>'amdgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArithOps/>'arith' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSME/>'ArmSME' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/BufferizationOps/>'bufferization' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ControlFlowDialect/>'cf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/DLTIDialect/>'dlti' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Func/>'func' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IndexOps/>'index' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IRDL/>'irdl' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MemRef/>'memref' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MLProgramOps/>'ml_program' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MPI/>'mpi' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVGPU/>'nvgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/>'omp' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/ODS/>ODS Documentation</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PtrOps/>'ptr' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Shard/>'shard' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SMT/>'smt' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/UBOps/>'ub' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/VCIXDialect/>'vcix' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeGPU/>'xegpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeVMDialect/>'xevm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MatchOpInterfaces/>OpInterface definitions</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SPIR-V/>SPIR-V Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Transform/>Transform Dialect</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Interfaces/>Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIR/>LLVM IR Target</a></li><li><a href=https://mlir.llvm.org/docs/BytecodeFormat/>MLIR Bytecode Format</a></li><li><a href=https://mlir.llvm.org/docs/CAPI/>MLIR C API</a></li><li><a href=https://mlir.llvm.org/docs/LangRef/>MLIR Language Reference</a></li><li><a href=https://mlir.llvm.org/docs/ReleaseNotes/>MLIR Release Notes</a></li><li><a href=https://mlir.llvm.org/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=https://mlir.llvm.org/docs/OwnershipBasedBufferDeallocation/>Ownership-based Buffer Deallocation</a></li><li><a href=https://mlir.llvm.org/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/Passes/>Passes</a></li><li><a href=https://mlir.llvm.org/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=https://mlir.llvm.org/docs/PDLL/>PDLL - PDL Language</a></li><li><a href=https://mlir.llvm.org/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/SideEffectsAndSpeculation/>Side Effects & Speculation</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/ShapeInference/>Shape Inference</a></li><li><a href=https://mlir.llvm.org/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=https://mlir.llvm.org/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=https://mlir.llvm.org/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Traits/>Traits<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Traits/Broadcastable/>The `Broadcastable` Trait</a></li></ul></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/Tutorials/>Tutorials<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/Tutorials/Toy/>Toy Tutorial<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li class=active><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/transform/>Transform Dialect Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch0/>Chapter 0: A Primer on “Structured” Linalg Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch1/>Chapter 1: Combining Existing Transformations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch2/>Chapter 2: Adding a Simple New Transformation Operation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch3/>Chapter 3: More than Simple Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch4/>Chapter 4: Matching Payload with Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/ChH/>Chapter H: Reproducing Halide Schedule</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/MlirOpt/>Using `mlir-opt`</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>