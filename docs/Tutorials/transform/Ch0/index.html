<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Chapter 0: A Primer on “Structured” Linalg Operations - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.119.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/docs/Tutorials/transform/Ch0/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script>
<link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script>
<script src=https://mlir.llvm.org/js/bundle.js></script>
<script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li><li class=child><a href=/python-bindings/>Python Bindings API docs</a></li></ul></li><li><a href="https://github.com/llvm/llvm-project/issues?q=is%3Aissue%20state%3Aopen%20label%3Amlir">Bugs</a></li><li><a href=https://github.com/llvm/mlir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/MLIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>Chapter 0: A Primer on “Structured” Linalg Operations</h1><p>Before starting the tutorial on the Transform dialect, let us take a brief look at the concept of Structured operations and its implementation in the Linalg dialect. Note that the Transform dialect does not require Structured operations and vice versa. The two co-evolved at the beginning of the Transform dialect, which makes the subset of transformations for Structured operations the most mature and most suitable for the tutorial. If you are already familiar with this concept, skip to Chapter 1.</p><p>Structured code generation intends to preserve the structure of the computation for as long as necessary to enable transformations, up to and including the design of IR abstractions that support specific transformations.</p><h2 id=uniform-elementwise-extension>Uniform Elementwise Extension&nbsp;<a class=headline-hash href=#uniform-elementwise-extension>¶</a></h2><p>Consider a simple scalar arithmetic addition operation in MLIR, which maps directly to a machine instruction on most architectures that support floating point operations:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%2</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span> <span class=p>:</span> <span class=k>f32</span>
</span></span></code></pre></div><p>This operation can be easily extended to uniformly apply to elements of a 1D vector, which is also often available as an instruction of vector machines:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%2</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Only a few modern instruction sets offer instructions for two- or more-dimensional vectors. In MLIR, however, it is possible to transparently extend the uniform elementwise application to vectors of arbitrary rank.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%2</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%0</span><span class=p>,</span> <span class=nv>%1</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x4x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl><span class=nv>%5</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%3</span><span class=p>,</span> <span class=nv>%4</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>2x2x2x2x2x2x2x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>As you can notice, MLIR’s arithmetic operations on vectors preserve the structure of uniform elementwise application. This structure can be leveraged by the compiler, for example, to produce smaller-rank operations available on the target or to fuse multiplication and addition when such a fused instruction is available (which becomes complicated when there are a hundred of multiplications followed by a hundred of additions).</p><h2 id=reduction>Reduction&nbsp;<a class=headline-hash href=#reduction>¶</a></h2><p>Sometimes it is necessary to add elements of a vector to obtain a scalar. Some platforms provide specific instructions for this operation, some others provide ones that can be combined to achieve the desired effect, such as addition of adjacent elements and element shuffle.</p><p>The Vector dialect in MLIR defines an operation to explicitly denote a within-vector reduction:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%1</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>reduction <span class=p>&lt;</span>add<span class=p>&gt;,</span> <span class=nv>%0</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=k>f32</span>
</span></span></code></pre></div><p>When no support is available, such an operation can be transformed into a loop:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%c0</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>0</span> <span class=p>:</span> <span class=k>index</span>
</span></span><span class=line><span class=cl><span class=nv>%c1</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>1</span> <span class=p>:</span> <span class=k>index</span>
</span></span><span class=line><span class=cl><span class=nv>%c8</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>8</span> <span class=p>:</span> <span class=k>index</span>
</span></span><span class=line><span class=cl><span class=nv>%init</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>0.0</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl><span class=nv>%result</span> <span class=p>=</span> scf<span class=p>.</span>for <span class=nv>%i</span> <span class=p>=</span> <span class=nv>%c0</span> to <span class=nv>%c8</span> step <span class=nv>%c1</span> iter_args<span class=p>(</span><span class=nv>%partial</span> <span class=p>=</span> <span class=nv>%init</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span><span class=k>f32</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nv>%element</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>extract <span class=nv>%0</span><span class=p>[</span><span class=nv>%i</span><span class=p>]</span> <span class=p>:</span> <span class=k>f32</span> into <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%updated</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%partial</span><span class=p>,</span> <span class=nv>%element</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  scf<span class=p>.</span>yield <span class=nv>%updated</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Even when special instructions are available, it may still be desirable to use the loop form (with unrolling), depending on instruction latency and register pressure. Preserving the structure of the operation as a single reduction gives the compiler an understanding that a within-vector reduction is performed and, therefore, a choice in implementation.</p><h2 id=contraction>Contraction&nbsp;<a class=headline-hash href=#contraction>¶</a></h2><p>Contraction is a generalization of reduction that multiplies elements from two vectors before adding them up. A simple “add” reduction can be thought of as a contraction where one of the vectors contains <code>1.0</code>, the neutral element of multiplication. Contractions offer even more flexibility to the compiler, and are represented by a dedicated operation in MLIR:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// Neutral initializer for the addition.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%init</span>  <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>0.0</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl><span class=c>// Neutral element of multiplication.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%ones</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> dense<span class=p>&lt;</span><span class=m>1.0</span><span class=p>&gt;</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl><span class=c>// Actual contraction.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>contract <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>indexing_maps =</span> <span class=p>[</span>affine_map<span class=p>&lt;(</span>i<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                   affine_map<span class=p>&lt;(</span>i<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                   affine_map<span class=p>&lt;(</span>i<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>()&gt;],</span>
</span></span><span class=line><span class=cl>  <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;reduction&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=nv>%0</span><span class=p>,</span> <span class=nv>%ones</span><span class=p>,</span> <span class=nv>%init</span> <span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x</span><span class=k>f32</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=k>f32</span>
</span></span></code></pre></div><p>Note the <code>affine_map</code> expressions indicating how vector elements are indexed. Their meaning is perhaps most evident when writing the loop form in pseudo-code equivalent to this contraction:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>for i in <span class=m>0</span> to <span class=m>8</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  init <span class=err>+</span><span class=p>=</span> p0<span class=p>[</span>i<span class=p>]</span> <span class=p>*</span> ones<span class=p>[</span>i<span class=p>]</span>
</span></span></code></pre></div><p>where both <code>%0</code> and <code>%ones</code> use the loop induction variable <code>i</code>, as noted on the right-hand side of the corresponding affine map, <code>(i) -> (i)</code>, and <code>%init</code> does not, as reflected on the right-hand side of its affine map, <code>(i) -> ()</code>.</p><p>Similarly to uniform elementwise extension, MLIR vector contractions are not limited to 1D cases. In the 2D+ case, one can additionally specify which of the vector dimensions are being reduced and which ones are being preserved. This can be achieved by using the <code>iterator_types</code> attribute that specifies, for each dimension, whether it is being reduced (<code>"reduction"</code>) or preserved (<code>"parallel"</code>). Consider the following 3D contraction that encodes a matrix-matrix multiplication:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%result</span> <span class=p>=</span> <span class=kt>vector</span><span class=p>.</span>contract <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>indexing_maps =</span> <span class=p>[</span>affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> k<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                   affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>k<span class=p>,</span> j<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                   affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> j<span class=p>)&gt;],</span>
</span></span><span class=line><span class=cl>  <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;reduction&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=nv>%lhs</span><span class=p>,</span> <span class=nv>%rhs</span><span class=p>,</span> <span class=nv>%init</span><span class=p>:</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x10x</span><span class=k>f32</span><span class=p>&gt;,</span> <span class=kt>vector</span><span class=p>&lt;</span><span class=m>10x16x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=kt>vector</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Looking at the indexing maps, it is easy to recognize the loop form:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>for i in <span class=m>0</span> to <span class=m>8</span><span class=p>:</span>
</span></span><span class=line><span class=cl>  for j in <span class=m>0</span> to <span class=m>16</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    for k in <span class=m>0</span> to <span class=m>10</span><span class=p>:</span>
</span></span><span class=line><span class=cl>      init<span class=p>[</span>i<span class=p>,</span> j<span class=p>]</span> <span class=err>+</span><span class=p>=</span> lhs<span class=p>[</span>i<span class=p>,</span> k<span class=p>]</span> <span class=p>*</span> rhs<span class=p>[</span>k<span class=p>,</span> j<span class=p>]</span>
</span></span></code></pre></div><p>Preserving this higher-level structure of a contraction makes it significantly easier for the compiler to recognize operations such as matrix multiplications and dot products and gives it freedom to produce lower-level operations that leverage most advanced instructions or even pre-generated microkernels.</p><h2 id=generic-operation-on-memory>Generic Operation on Memory&nbsp;<a class=headline-hash href=#generic-operation-on-memory>¶</a></h2><p>Until now, we have been considering operations on vectors stored in virtual registers. A similar contraction abstraction can be defined in memory:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>linalg<span class=p>.</span>generic <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>indexing_maps =</span> <span class=p>[</span>affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> k<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                   affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>k<span class=p>,</span> j<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                   affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> j<span class=p>)&gt;],</span>
</span></span><span class=line><span class=cl>  <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;reduction&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span> ins<span class=p>(</span><span class=nv>%lhs</span><span class=p>,</span> <span class=nv>%rhs</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>8x10x</span><span class=k>f32</span><span class=p>&gt;,</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>10x16x</span><span class=k>f32</span><span class=p>&gt;)</span>
</span></span><span class=line><span class=cl>  outs<span class=p>(</span><span class=nv>%init</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=nl>^bb0</span><span class=p>(</span><span class=nv>%lhs_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>,</span> <span class=nv>%rhs_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>,</span> <span class=nv>%init_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=nv>%0</span> <span class=p>=</span> arith<span class=p>.</span>mulf <span class=nv>%lhs_one</span><span class=p>,</span> <span class=nv>%rhs_one</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  <span class=nv>%1</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%init_one</span><span class=p>,</span> <span class=nv>%0</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  linalg<span class=p>.</span>yield <span class=nv>%1</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>This looks more complicated, so let us unpack. The <code>indexing_maps</code> and <code>iterator_types</code> are <em>exactly</em> the same as we have seen above for vector contractions. The operands are now split into two lists:</p><ul><li><code>in</code> operands containing the buffers that are being only read by the operation;</li><li><code>out</code> operands that are being read and updated by the operation.</li></ul><p>This separation wasn’t necessary on vectors because, in MLIR, vectors are read-only (SSA or functional form) and operations mutating a vector are in fact producing a new one instead.</p><p>Furthermore, the operation now contains a region that explicitly specifies the multiplication and the addition operations that were implicit in the contraction. Block arguments in the region correspond to individual elements read from the buffer: the first two correspond to the <code>in</code> operands and the last one corresponds to the <code>out</code> operand. The value yielded from the region is “written” to the <code>out</code> operand and is available as the last block argument for future executions of the region. Note that the order in which the region is executed for various tuples of elements read from the buffers is not specified, and the write to the <code>out</code> buffer is written as a whole at the end of the operation.</p><h2 id=loop-fusion>“Loop” Fusion&nbsp;<a class=headline-hash href=#loop-fusion>¶</a></h2><p>Since the region of the <code>linalg.generic</code> operation can contain arbitrarily many operations, we can use it to express “fusion” of the implicit loops by simply having more operations chained in the region. For example, the common machine learning rectified linear unit layer (ReLU), which can be defined as <code>relu(x) = max(0, x)</code>, can be expressed using the “compare-and-select” idiom in one <code>linalg.generic</code> operation, without the temporary buffer for the comparison result and without repeating the outer operation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl>linalg<span class=p>.</span>generic <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=k>index</span>ing_maps <span class=p>[</span>affine_map<span class=p>&lt;(</span>i<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>)&gt;,</span> affine_map<span class=p>&lt;(</span>i<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>)&gt;],</span>
</span></span><span class=line><span class=cl>  <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;parallel&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span> ins<span class=p>(</span><span class=nv>%in</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f32</span><span class=p>&gt;)</span> outs<span class=p>(</span><span class=nv>%out</span> <span class=p>:</span> <span class=kt>memref</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f32</span><span class=p>&gt;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=nl>^bb0</span><span class=p>(</span><span class=nv>%in_one</span> <span class=p>:</span> <span class=k>f32</span><span class=p>,</span> <span class=nv>%out_one</span> <span class=p>:</span> <span class=k>f32</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=nv>%c0</span> <span class=p>=</span> arith<span class=p>.</span><span class=kt>constant</span> <span class=m>0.0</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  <span class=nv>%0</span> <span class=p>=</span> arith<span class=p>.</span>cmpf ogt <span class=nv>%in_one</span><span class=p>,</span> <span class=nv>%c0</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  <span class=nv>%1</span> <span class=p>=</span> arith<span class=p>.</span>select <span class=nv>%0</span><span class=p>,</span> <span class=nv>%in_one</span><span class=p>,</span> <span class=nv>%c0</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  linalg<span class=p>.</span>yield <span class=nv>%1</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Such operations can be converted to loops or lowered into vector forms after splitting into multiple operations, each of which maps to a Vector dialect primitive. This modeling, again, gives the compiler more choice in selecting the code generation strategy.</p><h2 id=generic-operation-on-tensors>Generic Operation on Tensors&nbsp;<a class=headline-hash href=#generic-operation-on-tensors>¶</a></h2><p>Let us take one last step up on the abstraction ladder. MLIR provides a tensor abstraction that makes it easy for the compiler to reason about multidimensional yet regular data without having to solve complex problems such as alias analysis and dependency satisfaction, which would be necessary on multidimensional buffers. The tensor abstraction is very similar to the vector abstraction (major differences include the availability of unranked tensors, tensor layouts, and vectors being usable as elemental types of tensors but not of other vectors). Tensors are read-only, and operations updating a tensor produce a new tensor.</p><p>The <code>linalg.generic</code> operation from above can lifted to operate on tensors instead of buffers:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%result</span> <span class=p>=</span> linalg<span class=p>.</span>generic <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>indexing_maps =</span> <span class=p>[</span>affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> k<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                   affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>k<span class=p>,</span> j<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                   affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> j<span class=p>)&gt;],</span>
</span></span><span class=line><span class=cl>  <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;reduction&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span> ins<span class=p>(</span><span class=nv>%lhs</span><span class=p>,</span> <span class=nv>%rhs</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x10x</span><span class=k>f32</span><span class=p>&gt;,</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>10x16x</span><span class=k>f32</span><span class=p>&gt;)</span>
</span></span><span class=line><span class=cl>  outs<span class=p>(</span><span class=nv>%init</span> <span class=p>:</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=nl>^bb0</span><span class=p>(</span><span class=nv>%lhs_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>,</span> <span class=nv>%rhs_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>,</span> <span class=nv>%init_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>):</span>
</span></span><span class=line><span class=cl>  <span class=nv>%0</span> <span class=p>=</span> arith<span class=p>.</span>mulf <span class=nv>%lhs_one</span><span class=p>,</span> <span class=nv>%rhs_one</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  <span class=nv>%1</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%init_one</span><span class=p>,</span> <span class=nv>%0</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  linalg<span class=p>.</span>yield <span class=nv>%1</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>As you can notice, most components of this operation remain identical to its buffer version. It has been specifically designed this way. The main difference, beside the operand types, is that the operation now produces a new result instead of updating the <code>out</code> buffer. The <code>out</code> operand is used only as the initialization value.</p><p>If the <code>linalg.generic</code> operation had existed on vectors, it would have had the exact same structure.</p><h2 id=tiling-and-loop-materialization>Tiling and Loop Materialization&nbsp;<a class=headline-hash href=#tiling-and-loop-materialization>¶</a></h2><p>At this level of abstraction, it becomes easy for the compiler to perform more advanced transformations usually required for high-performance code generation, such as
<a href=https://en.wikipedia.org/wiki/Loop_nest_optimization>tiling</a>. Tiling, in general, can be seen as partitioning the iteration space into smaller parts, or tiles, so that the data required by each part fits into a level of cache for example. The order in which tiles are executed must preserve the original data dependencies.</p><p>In the case of <code>linalg.generic</code> operations, the iteration space is implicit and is defined by the shape of the operands. Therefore, a tile can be expressed by performing the <em>same</em> operation on a subset (slice) of the original data. Since the order in which the body of <code>linalg.generic</code> is applied to different tuples of the input elements is unspecified, tiles can be executed in any order, without the need for dependence analysis. In order to control the execution of different tiles, the implementation of tiling produces loops. Thus tiling <code>linalg.generic</code> operations can also be seen as materializing the loops that have been implicit until now.</p><p>For example, tiling the matrix multiplication presented above with tile sizes <code>(2, 8)</code>, we obtain a loop nest around a <code>linalg.generic</code> expressing the same operation on a <code>2x8</code> tensor.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// A special &#34;multi-for&#34; loop that supports tensor-insertion semantics
</span></span></span><span class=line><span class=cl><span class=c>// as opposed to implicit updates. The resulting 8x16 tensor will be produced
</span></span></span><span class=line><span class=cl><span class=c>// by this loop.
</span></span></span><span class=line><span class=cl><span class=c>// The trip count of iterators is computed dividing the original tensor size,
</span></span></span><span class=line><span class=cl><span class=c>// 8x16, by the tile size, 2x8, to obtain 4x2.
</span></span></span><span class=line><span class=cl><span class=c>// When tensor sizes are dynamic, the trip count computation is emitted as IR
</span></span></span><span class=line><span class=cl><span class=c>// and is being computed at runtime.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%0</span> <span class=p>=</span> scf<span class=p>.</span>forall <span class=p>(</span><span class=nv>%i</span><span class=p>,</span> <span class=nv>%j</span><span class=p>)</span> in <span class=p>(</span><span class=m>4</span><span class=p>,</span> <span class=m>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>     shared_outs<span class=p>(</span><span class=nv>%shared</span> <span class=p>=</span> <span class=nv>%init</span><span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c>// Scale the loop induction variables by the tile sizes.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%3</span> <span class=p>=</span> affine<span class=p>.</span>apply affine_map<span class=p>&lt;(</span>d0<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>d0 <span class=p>*</span> <span class=m>2</span><span class=p>)&gt;(</span><span class=nv>%i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nv>%4</span> <span class=p>=</span> affine<span class=p>.</span>apply affine_map<span class=p>&lt;(</span>d0<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>d0 <span class=p>*</span> <span class=m>8</span><span class=p>)&gt;(</span><span class=nv>%j</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c>// Take slices of inputs and outputs. Only the &#34;i&#34; and &#34;j&#34; dimensions are sliced.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%lhs_slice</span> <span class=p>=</span> <span class=kt>tensor</span><span class=p>.</span>extract_slice <span class=nv>%lhs</span><span class=p>[</span><span class=nv>%3</span><span class=p>,</span> <span class=m>0</span><span class=p>]</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>10</span><span class=p>]</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>             <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x10x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x10x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%rhs_slice</span> <span class=p>=</span> <span class=kt>tensor</span><span class=p>.</span>extract_slice <span class=nv>%rhs</span><span class=p>[</span><span class=m>0</span><span class=p>,</span> <span class=nv>%4</span><span class=p>]</span> <span class=p>[</span><span class=m>10</span><span class=p>,</span> <span class=m>8</span><span class=p>]</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>             <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>10x16x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>10x8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%result_slice</span> <span class=p>=</span> <span class=kt>tensor</span><span class=p>.</span>extract_slice <span class=nv>%shared</span><span class=p>[</span><span class=nv>%3</span><span class=p>,</span> <span class=nv>%4</span><span class=p>]</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>8</span><span class=p>]</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c>// This is exactly the same operation as before, but now operating on smaller
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// slices of data.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%partial</span> <span class=p>=</span>  linalg<span class=p>.</span>generic <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>indexing_maps =</span> <span class=p>[</span>affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> k<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                   affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>k<span class=p>,</span> j<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                   affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> j<span class=p>)&gt;],</span>
</span></span><span class=line><span class=cl>  <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;reduction&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span> ins<span class=p>(</span><span class=nv>%lhs_slice</span><span class=p>,</span> <span class=nv>%rhs_slice</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x10x</span><span class=k>f32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>10x8x</span><span class=k>f32</span><span class=p>&gt;)</span>
</span></span><span class=line><span class=cl>    outs<span class=p>(</span><span class=nv>%result_slice</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%lhs_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>,</span> <span class=nv>%rhs_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>,</span> <span class=nv>%init_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nv>%0</span> <span class=p>=</span> arith<span class=p>.</span>mulf <span class=nv>%lhs_one</span><span class=p>,</span> <span class=nv>%rhs_one</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>    <span class=nv>%1</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%init_one</span><span class=p>,</span> <span class=nv>%0</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>    linalg<span class=p>.</span>yield <span class=nv>%1</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c>// Terminator for the loop with tensor-insertion semantics. Inserts a slice
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// into a larger tensor, potentially in parallel.
</span></span></span><span class=line><span class=cl><span class=c></span>  scf<span class=p>.</span>forall<span class=p>.</span>in_parallel <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>tensor</span><span class=p>.</span>parallel_insert_slice <span class=nv>%partial</span> into <span class=nv>%shared</span><span class=p>[</span><span class=nv>%3</span><span class=p>,</span> <span class=nv>%4</span><span class=p>]</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>8</span><span class=p>]</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=producerconsumer-fusion-and-rematerialization>Producer/Consumer Fusion and Rematerialization&nbsp;<a class=headline-hash href=#producerconsumer-fusion-and-rematerialization>¶</a></h2><p>After materializing loops with tiling, another key code generation transformation becomes simple – fusion. Unlike loop fusion, the Structured operations approach allows for producer/consumer fusion even when the (implicit) iteration spaces of the operations do not match. Given an high-level structured operation on tensors, such as <code>linalg.generic</code>, one can follow use-def chains to identify:</p><ol><li>the subset (slice) of the operand that is used by the tile, and</li><li>the tensor-level structured operation producing the whole tensor that is being sliced.</li></ol><p>By inverting the <code>indexing_map</code> and applying it to the set of elements accessed through the slice, we can compute the part of the iteration space of the operation defining the full tensor necessary to compute the tile. Thus fusion boils down to replacing the <code>tensor.extract_slice</code> operation with the tile of the <code>linalg.generic</code> producing the original operand.</p><p>Let us assume that the matrix multiplication operation is followed by another operation that multiplies each element of the resulting matrix with itself. This trailing elementwise operation has a 2D iteration space, unlike the 3D one in matrix multiplication. Nevertheless, it is possible to tile the trailing operation and then fuse the producer of its operand, the matmul, into the loop generated by tiling. The untiled dimension will be used in its entirety.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// Same loop as before.
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%0</span> <span class=p>=</span> scf<span class=p>.</span>forall <span class=p>(</span><span class=nv>%i</span><span class=p>,</span> <span class=nv>%j</span><span class=p>)</span> in <span class=p>(</span><span class=m>4</span><span class=p>,</span> <span class=m>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>     shared_outs<span class=p>(</span><span class=nv>%shared</span> <span class=p>=</span> <span class=nv>%init</span><span class=p>)</span>
</span></span><span class=line><span class=cl>     <span class=p>-&gt;</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=c>// Scale the loop induction variables by the tile sizes.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%1</span> <span class=p>=</span> affine<span class=p>.</span>apply affine_map<span class=p>&lt;(</span>d0<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>d0 <span class=p>*</span> <span class=m>2</span><span class=p>)&gt;(</span><span class=nv>%i</span><span class=p>)</span>
</span></span><span class=line><span class=cl>  <span class=nv>%2</span> <span class=p>=</span> affine<span class=p>.</span>apply affine_map<span class=p>&lt;(</span>d0<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>d0 <span class=p>*</span> <span class=m>8</span><span class=p>)&gt;(</span><span class=nv>%j</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c>// Take slices of inputs and outputs. Only the &#34;i&#34; and &#34;j&#34; dimensions are sliced.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%lhs_slice</span> <span class=p>=</span> <span class=kt>tensor</span><span class=p>.</span>extract_slice <span class=nv>%lhs</span><span class=p>[</span><span class=nv>%1</span><span class=p>,</span> <span class=m>0</span><span class=p>]</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>10</span><span class=p>]</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>             <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x10x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x10x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%rhs_slice</span> <span class=p>=</span> <span class=kt>tensor</span><span class=p>.</span>extract_slice <span class=nv>%rhs</span><span class=p>[</span><span class=m>0</span><span class=p>,</span> <span class=nv>%2</span><span class=p>]</span> <span class=p>[</span><span class=m>10</span><span class=p>,</span> <span class=m>8</span><span class=p>]</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>             <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>10x16x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>10x8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=nv>%result_slice</span> <span class=p>=</span> <span class=kt>tensor</span><span class=p>.</span>extract_slice <span class=nv>%result</span><span class=p>[</span><span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>]</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>8</span><span class=p>]</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c>// This is exactly the same matmul slice as before. It replaces the slice
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// extraction for the generic operation below.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%partial</span> <span class=p>=</span> linalg<span class=p>.</span>generic <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nl>indexing_maps =</span> <span class=p>[</span>affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> k<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                     affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>k<span class=p>,</span> j<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                     affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>,</span> k<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> j<span class=p>)&gt;],</span>
</span></span><span class=line><span class=cl>    <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;reduction&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span> ins<span class=p>(</span><span class=nv>%lhs_slice</span><span class=p>,</span> <span class=nv>%rhs_slice</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x10x</span><span class=k>f32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>10x8x</span><span class=k>f32</span><span class=p>&gt;)</span>
</span></span><span class=line><span class=cl>   outs<span class=p>(</span><span class=nv>%result_slice</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%lhs_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>,</span> <span class=nv>%rhs_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>,</span> <span class=nv>%init_one</span><span class=p>:</span> <span class=k>f32</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nv>%5</span> <span class=p>=</span> arith<span class=p>.</span>mulf <span class=nv>%lhs_one</span><span class=p>,</span> <span class=nv>%rhs_one</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>    <span class=nv>%6</span> <span class=p>=</span> arith<span class=p>.</span>addf <span class=nv>%init_one</span><span class=p>,</span> <span class=nv>%5</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>    linalg<span class=p>.</span>yield <span class=nv>%6</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c>// Take the slice of the final result. Note that we don&#39;t need to take
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// the slice of the operand because the matmul operation above computes
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// it in-place.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%shared_slice</span> <span class=p>=</span> <span class=kt>tensor</span><span class=p>.</span>extract_slice <span class=nv>%shared</span><span class=p>[</span><span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>]</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>8</span><span class=p>]</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;</span> to <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c>// The elementwise operation that we tiled.
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=nv>%elemwise</span> <span class=p>=</span> linalg<span class=p>.</span>generic <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nl>indexing_maps =</span> <span class=p>[</span>affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> j<span class=p>)&gt;,</span>
</span></span><span class=line><span class=cl>                     affine_map<span class=p>&lt;(</span>i<span class=p>,</span> j<span class=p>)</span> <span class=p>-&gt;</span> <span class=p>(</span>i<span class=p>,</span> j<span class=p>)&gt;],</span>
</span></span><span class=line><span class=cl>    <span class=nl>iterator_types =</span> <span class=p>[</span><span class=s>&#34;parallel&#34;</span><span class=p>,</span> <span class=s>&#34;parallel&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span> ins<span class=p>(</span><span class=nv>%partial</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;)</span>
</span></span><span class=line><span class=cl>   outs<span class=p>(</span><span class=nv>%shared_slice</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>^bb0</span><span class=p>(</span><span class=nv>%in</span><span class=p>:</span> <span class=k>f32</span><span class=p>,</span> <span class=nv>%out</span><span class=p>:</span> <span class=k>f32</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nv>%5</span> <span class=p>=</span> arith<span class=p>.</span>mulf <span class=nv>%in</span><span class=p>,</span> <span class=nv>%in</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>    linalg<span class=p>.</span>yield <span class=nv>%5</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c>// Terminator for the loop with tensor-insertion semantics. Inserts a slice
</span></span></span><span class=line><span class=cl><span class=c></span>  <span class=c>// into a larger tensor, potentially in parallel.
</span></span></span><span class=line><span class=cl><span class=c></span>  scf<span class=p>.</span>forall<span class=p>.</span>in_parallel <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=kt>tensor</span><span class=p>.</span>parallel_insert_slice <span class=nv>%elemwise</span> into <span class=nv>%shared</span><span class=p>[</span><span class=nv>%1</span><span class=p>,</span> <span class=nv>%2</span><span class=p>]</span> <span class=p>[</span><span class=m>2</span><span class=p>,</span> <span class=m>8</span><span class=p>]</span> <span class=p>[</span><span class=m>1</span><span class=p>,</span> <span class=m>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x8x</span><span class=k>f32</span><span class=p>&gt;</span> into <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>This process may result in some elements in the operand tensors being (re)computed on every iteration of the loop. This is also known as <em>rematerialization</em> and expresses the tradeoff between performing redundant computations or storing their result in (slow) memory.</p><h2 id=shorthand-named-forms-of-linalg-ops>Shorthand “Named” Forms of Linalg Ops&nbsp;<a class=headline-hash href=#shorthand-named-forms-of-linalg-ops>¶</a></h2><p>Linalg provides a set of predefined operations for common cases such as matrix multiplication, dot product, convolution, etc. These operations are equivalent to the <code>generic</code> ones but spare the need to spell out the access patterns and the bodies. For example, matrix multiplication is simply:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=nv>%matmul</span> <span class=p>=</span> linalg<span class=p>.</span>matmul ins<span class=p>(</span><span class=nv>%lhs</span><span class=p>,</span> <span class=nv>%rhs</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x10x</span><span class=k>f32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>10x16x</span><span class=k>f32</span><span class=p>&gt;)</span>
</span></span><span class=line><span class=cl>                        outs<span class=p>(</span><span class=nv>%init</span><span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x10x</span><span class=k>f32</span>xf32<span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>8x16x</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span></code></pre></div><div class=edit-meta><br></div><nav class=pagination><a class="nav nav-prev" href=https://mlir.llvm.org/docs/Tutorials/transform/ title="Transform Dialect Tutorial"><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - Transform Dialect Tutorial</a>
<a class="nav nav-next" href=https://mlir.llvm.org/docs/Tutorials/transform/Ch1/ title="Chapter 1: Combining Existing Transformations">Next - Chapter 1: Combining Existing Transformations <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=https://mlir.llvm.org/governance/>Governance</a></li><li><a href=https://mlir.llvm.org/users/>Users of MLIR</a></li><li><a href=https://mlir.llvm.org/pubs/>MLIR Related Publications</a></li><li><a href=https://mlir.llvm.org/talks/>Talks</a></li><li><a href=https://mlir.llvm.org/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://mlir.llvm.org/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://mlir.llvm.org/getting_started/Faq/>FAQ</a></li><li><a href=https://mlir.llvm.org/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://mlir.llvm.org/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=https://mlir.llvm.org/getting_started/openprojects/>Open Projects</a></li><li><a href=https://mlir.llvm.org/getting_started/Glossary/>Glossary</a></li><li><a href=https://mlir.llvm.org/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/>Code Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-reduce/>MLIR Reduce</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-rewrite/>mlir-rewrite</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/OpenMPPasses/></a></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIRTransforms/></a></li><li><a href=https://mlir.llvm.org/docs/ActionTracing/>Action: Tracing and Debugging MLIR-based Compilers</a></li><li><a href=https://mlir.llvm.org/docs/Bufferization/>Bufferization</a></li><li><a href=https://mlir.llvm.org/docs/DataLayout/>Data Layout Modeling</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/DefiningDialects/>Defining Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Constraints/>Constraints</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Assembly/>Customizing Assembly Behavior</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/AttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Operations/>Operation Definition Specification (ODS)</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/DialectConversion/>Dialect Conversion</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/>Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/XeGPUTransformOps/></a></li><li><a href=https://mlir.llvm.org/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMDGPU/>'amdgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArithOps/>'arith' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSME/>'ArmSME' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/BufferizationOps/>'bufferization' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ControlFlowDialect/>'cf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/DLTIDialect/>'dlti' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Func/>'func' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IndexOps/>'index' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IRDL/>'irdl' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MemRef/>'memref' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MLProgramOps/>'ml_program' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MPI/>'mpi' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVGPU/>'nvgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/>'omp' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/ODS/>ODS Documentation</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PtrOps/>'ptr' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Shard/>'shard' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SMT/>'smt' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/UBOps/>'ub' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/VCIXDialect/>'vcix' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/WasmSSAOps/>'wasmssa' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeGPU/>'xegpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeVMDialect/>'xevm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MatchOpInterfaces/>OpInterface definitions</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SPIR-V/>SPIR-V Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Transform/>Transform Dialect</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Interfaces/>Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIR/>LLVM IR Target</a></li><li><a href=https://mlir.llvm.org/docs/BytecodeFormat/>MLIR Bytecode Format</a></li><li><a href=https://mlir.llvm.org/docs/CAPI/>MLIR C API</a></li><li><a href=https://mlir.llvm.org/docs/LangRef/>MLIR Language Reference</a></li><li><a href=https://mlir.llvm.org/docs/ReleaseNotes/>MLIR Release Notes</a></li><li><a href=https://mlir.llvm.org/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=https://mlir.llvm.org/docs/OwnershipBasedBufferDeallocation/>Ownership-based Buffer Deallocation</a></li><li><a href=https://mlir.llvm.org/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/Passes/>Passes</a></li><li><a href=https://mlir.llvm.org/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=https://mlir.llvm.org/docs/PatternSearch/>Pattern Search</a></li><li><a href=https://mlir.llvm.org/docs/PDLL/>PDLL - PDL Language</a></li><li><a href=https://mlir.llvm.org/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/SideEffectsAndSpeculation/>Side Effects & Speculation</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Remarks/>Remark Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/ShapeInference/>Shape Inference</a></li><li><a href=https://mlir.llvm.org/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=https://mlir.llvm.org/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=https://mlir.llvm.org/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Traits/>Traits<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Traits/Broadcastable/>The `Broadcastable` Trait</a></li></ul></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/Tutorials/>Tutorials<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/Toy/>Toy Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li class="parent has-sub-menu"><a href=https://mlir.llvm.org/docs/Tutorials/transform/>Transform Dialect Tutorial<span class="mark opened">-</span></a><ul class=sub-menu><li class=active><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch0/>Chapter 0: A Primer on “Structured” Linalg Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch1/>Chapter 1: Combining Existing Transformations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch2/>Chapter 2: Adding a Simple New Transformation Operation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch3/>Chapter 3: More than Simple Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch4/>Chapter 4: Matching Payload with Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/ChH/>Chapter H: Reproducing Halide Schedule</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/MlirOpt/>Using `mlir-opt`</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>