<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Users of MLIR - MLIR</title><meta name=description content="Multi-Level IR Compiler Framework"><meta name=generator content="Hugo 0.119.0"><link href=https://mlir.llvm.org/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://mlir.llvm.org/users/><link rel=stylesheet href=https://mlir.llvm.org/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script>
<link rel=stylesheet href=https://mlir.llvm.org/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script>
<script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script>
<script src=https://mlir.llvm.org/js/bundle.js></script>
<script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://mlir.llvm.org//mlir-logo.png width=40px align=absmiddle>
MLIR</div></h1><p class=description>Multi-Level IR Compiler Framework</p></header><div class=global-menu><nav><ul><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/mlir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/llvm/llvm-project/tree/main/mlir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/llvm/llvm-project/tree/main/mlir>GitHub</a></li><li class=child><a href=/python-bindings/>Python Bindings API docs</a></li></ul></li><li><a href="https://github.com/llvm/llvm-project/issues?q=is%3Aissue%20state%3Aopen%20label%3Amlir">Bugs</a></li><li><a href=https://github.com/llvm/mlir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/MLIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>Users of MLIR</h1><p>In alphabetical order below.</p><h2 id=accerahttpsgithubcommicrosoftaccera><a href=https://github.com/microsoft/Accera>Accera</a></h2><p>Accera is a compiler that enables you to experiment with loop optimizations without
hand-writing Assembly code. With Accera, these problems and impediments can be
addressed in an optimized way. It is available as a Python library and supports
cross-compiling to a wide range of processor targets.</p><h2 id=allohttpsgithubcomcornell-zhangallo><a href=https://github.com/cornell-zhang/allo>Allo</a></h2><p>Allo is a Python-embedded Accelerator Design Language (ADL) and compiler that
facilitates the construction of large-scale, high-performance hardware accelerators
in a modular and composable manner. It supports progressive hardware customizations,
reusable parameterized kernel templates, and composable schedules on top of MLIR
for productive hardware development.</p><h2 id=beaverhttpsgithubcombeaver-lodgebeaver><a href=https://github.com/beaver-lodge/beaver>Beaver</a></h2><p>Beaver is an MLIR frontend in Elixir and Zig.
Powered by Elixir&rsquo;s composable modularity and meta-programming features,
Beaver provides a simple, intuitive, and extensible interface for MLIR.</p><h2 id=bᴛᴏʀ2ᴍʟɪʀhttpsgithubcomjetafesebtor2mlir-a-format-and-toolchain-for-hardware-verification><a href=https://github.com/jetafese/btor2mlir>Bᴛᴏʀ2ᴍʟɪʀ</a>: A Format and Toolchain for Hardware Verification</h2><p>Bᴛᴏʀ2ᴍʟɪʀ applies MLIR to the domain of hardware verification by offering a clean way to take advantage of a format&rsquo;s
strengths. For example, we support the use of software verification methods for hardware
verification problems represented in the Bᴛᴏʀ2 format. The project aims to spur and support research
in the formal verification domain, and has been shown to be competitive with existing methods.</p><h2 id=catalysthttpsgithubcompennylaneaicatalyst><a href=https://github.com/PennyLaneAI/catalyst>Catalyst</a></h2><p>Catalyst is an AOT/JIT compiler for
<a href=https://pennylane.ai/>PennyLane</a> that
accelerates hybrid quantum programs, with:</p><ul><li>full auto-differentiation support, via custom quantum gradients
and
<a href=https://github.com/EnzymeAD/Enzyme>Enzyme</a>-based backpropagation,</li><li>a dynamic quantum programming model,</li><li>and integration into the Python ML ecosytem.</li></ul><p>Catalyst also comes with the
<a href=https://github.com/PennyLaneAI/pennylane-lightning/>Lightning</a>
high performance simulator by default, but supports an extensible backend system that is
constantly evolving, aiming to deliver execution on heterogenous architectures with GPUs and QPUs.</p><h2 id=circthttpsgithubcomllvmcirct-circuit-ir-compilers-and-tools><a href=https://github.com/llvm/circt>CIRCT</a>: Circuit IR Compilers and Tools</h2><p>The CIRCT project is an (experimental!) effort looking to apply MLIR and the LLVM
development methodology to the domain of hardware design tools.</p><h2 id=clangirhttpsllvmgithubioclangir><a href=https://llvm.github.io/clangir/>ClangIR</a></h2><p>ClangIR is a high-level representation in Clang that reflects aspects of the
C/C++ languages and their extensions. It is implemented using MLIR and occupies
a position between Clang’s AST and LLVM IR.</p><h2 id=concretehttpsgithubcomzama-aiconcrete-tfhe-compiler-that-converts-python-programs-into-fhe-equivalent><a href=https://github.com/zama-ai/concrete>Concrete</a>: TFHE Compiler that converts python programs into FHE equivalent</h2><p>Concrete is an open-source framework that simplifies the use of
<a href=https://fhe.org>Fully Homomorphic Encryption</a> (FHE) and makes writing FHE
programs easy for developers</p><p>FHE is a powerful technology that enables computations on encrypted data without
needing to decrypt it. This capability ensures user privacy and provides robust
protection against data breaches.</p><p>Concrete enables developers to efficiently develop privacy-preserving
applications for various use cases. For instance,
<a href=https://github.com/zama-ai/concrete-ml>Concrete ML</a> is built on top of
Concrete to integrate privacy-preserving features of FHE into machine learning
use cases.</p><h2 id=dsp-mlirhttpsgithubcommpslab-asudsp_mlir-a-framework-for-digital-signal-processing-applications-in-mlir><a href=https://github.com/MPSLab-ASU/DSP_MLIR>DSP-MLIR</a>: A Framework for Digital Signal Processing Applications in MLIR</h2><p>DSP-MLIR is a framework designed specifically for DSP applications. It provides
a DSL (Frontend), compiler, and rewrite patterns that detect DSP patterns and
apply optimizations based on DSP theorems. The framework supports a wide range
of DSP operations, including filters (FIR, IIR, filter response), transforms
(DCT, FFT, IFFT), and other signal processing operations such as delay and gain,
along with additional functionalities for application development.</p><h2 id=enzymehttpsenzymemitedu-general-automatic-differentiation-of-mlir><a href=https://enzyme.mit.edu>Enzyme</a>: General Automatic Differentiation of MLIR</h2><p>Enzyme (specifically EnzymeMLIR) is a first-class automatic differentiation
sytem for MLIR. Operations and types implement or inheret general interfaces
to specify their differentiable behavior, which allows Enzyme to provide
efficient forward and reverse pass derivatives. Source code is available
<a href=https://github.com/EnzymeAD/Enzyme/tree/main/enzyme/Enzyme/MLIR>here</a>.
See also the
<a href=https://github.com/EnzymeAD/Enzyme-JAX>Enzyme-JaX</a> project which
uses Enzyme to differentiate StableHLO, and thus provide MLIR-native
differentiation and codegen for JaX.</p><h2 id=fireflyhttpsgithubcomgetfireflyfirefly-a-new-compiler-and-runtime-for-beam-languages><a href=https://github.com/GetFirefly/firefly>Firefly</a>: A new compiler and runtime for BEAM languages</h2><p>Firefly is not only a compiler, but a runtime as well. It consists of two parts:</p><ul><li>A compiler for Erlang to native code for a given target (x86, ARM, WebAssembly)</li><li>An Erlang runtime, implemented in Rust, which provides the core functionality
needed to implement OTP</li></ul><p>The primary motivator for Firefly&rsquo;s development was the ability to compile Elixir
applications that could target WebAssembly, enabling use of Elixir as a language
for frontend development. It is also possible to use Firefly to target other
platforms as well, by producing self-contained executables on platforms such as
x86.</p><h2 id=flanghttpsgithubcomllvmllvm-projecttreemainflang><a href=https://github.com/llvm/llvm-project/tree/main/flang>Flang</a></h2><p>Flang is a ground-up implementation of a Fortran front end written in modern C++.
It started off as the
<a href=https://github.com/flang-compiler/f18>f18 project</a> with an
aim to replace the previous
<a href=https://github.com/flang-compiler/flang>flang project</a>
and address its various deficiencies. F18 was subsequently accepted into the LLVM
project and rechristened as Flang. The high level IR of the Fortran compiler is modeled
using MLIR.</p><h2 id=heirhttpsgithubcomgoogleheir><a href=https://github.com/google/heir>HEIR</a></h2><p>HEIR (Homomorphic Encryption Intermediate Representation) is an MLIR-based
toolchain developed by Google for compiling programs that utilize homomorphic
encryption. Homomorphic encryption allows computations to be performed directly
on encrypted data without needing to decrypt it first, thereby preserving data
privacy throughout the computational process.</p><p>Building upon the foundation of MLIR (Multi-Level Intermediate Representation),
HEIR provides a flexible and extensible framework for developing compilers
targeting homomorphic encryption. This approach facilitates the optimization and
transformation of code in a manner that is both modular and scalable.</p><h2 id=ireehttpsgithubcomgoogleiree><a href=https://github.com/google/iree>IREE</a></h2><p>IREE (pronounced &ldquo;eerie&rdquo;) is a compiler and minimal runtime system for
compiling ML models for execution against a HAL (Hardware Abstraction Layer)
that is aligned with Vulkan. It aims to be a viable way to compile and run
ML devices on a variety of small and medium sized systems, leveraging either
the GPU (via Vulkan/SPIR-V), CPU or some combination. It also aims to
interoperate seamlessly with existing users of Vulkan APIs, specifically
focused on games and rendering pipelines.</p><h2 id=jsirhttpsgithubcomgooglejsir><a href=https://github.com/google/jsir>JSIR</a></h2><p>JSIR is a next-generation JavaScript analysis tool. At its core is an MLIR-based
high-level intermediate representation, which supports both dataflow analysis
and lossless conversion back to source. This unique design makes it suitable for
source-to-source transformation. JSIR is used at Google for analyzing and
detecting malicious JavaScript files, protecting products like Ads, Android, and
Chrome.</p><h2 id=kokkoshttpskokkosorg><a href=https://kokkos.org>Kokkos</a></h2><p>The Kokkos C++ Performance Portability Ecosystem is a production level solution
for writing modern C++ applications in a hardware agnostic way. It is part of the
US Department of Energies Exascale Project – the leading effort in the US to prepare
the HPC community for the next generation of super computing platforms. The Ecosystem
consists of multiple libraries addressing the primary concerns for developing and
maintaining applications in a portable way. The three main components are the Kokkos
Core Programming Model, the Kokkos Kernels Math Libraries and the Kokkos Profiling and
Debugging Tools.</p><p>There is current
<a href=https://github.com/kokkos/kokkos.github.io/files/13651039/Kokkos_MLIR.pdf>work</a>
ongoing to convert MLIR to portable Kokkos-based source code, add a partition dialect
to MLIR to support tiled and distributed sparse tensors and target spatial dataflow
accelerators.</p><h2 id=lingo-dbhttpswwwlingo-dbcom-revolutionizing-data-processing-with-compiler-technology><a href=https://www.lingo-db.com>Lingo DB</a>: Revolutionizing Data Processing with Compiler Technology</h2><p>LingoDB is a cutting-edge data processing system that leverages compiler technology
to achieve unprecedented flexibility and extensibility without sacrificing
performance. It supports a wide range of data-processing workflows beyond
relational SQL queries, thanks to declarative sub-operators. Furthermore,
LingoDB can perform cross-domain optimization by interleaving optimization
passes of different domains and its flexibility enables sustainable support
for heterogeneous hardware.</p><p>LingoDB heavily builds on the MLIR compiler framework for compiling queries
to efficient machine code without much latency.</p><h2 id=marcohttpsgithubcommarco-compilermarco-modelica-advanced-research-compiler><a href=https://github.com/marco-compiler/marco>MARCO</a>: Modelica Advanced Research COmpiler</h2><p>MARCO is a prototype compiler for the Modelica language, with focus on the
efficient compilation and simulation of large-scale models.
The Modelica source code is processed by external tools to obtain a
modeling language independent representation in Base Modelica, for which an
MLIR dialect has been designed.</p><p>The project is complemented by multiple runtime libraries, written in C++, that
are used to drive the generated simulation, provide support functions, and to
ease interfacing with external differential equations solvers.</p><h2 id=mlir-aiehttpsgithubcomxilinxmlir-aie-toolchain-for-amdxilinx-aiengine-devices><a href=https://github.com/Xilinx/mlir-aie>MLIR-AIE</a>: Toolchain for AMD/Xilinx AIEngine devices</h2><p>MLIR-AIE is a toolchain providing low-level device configuration for Versal
AIEngine-based devices. Support is provided to target the AIEngine portion of
the device, including processors, stream switches, TileDMA and ShimDMA blocks.
Backend code generation is included, targetting the LibXAIE library, along with
some higher-level abstractions enabling higher-level design.</p><h2 id=mlir-dacehttpsgithubcomspclmlir-dace-data-centric-mlir-dialect><a href=https://github.com/spcl/mlir-dace>MLIR-DaCe</a>: Data-Centric MLIR Dialect</h2><p>MLIR-DaCe is a project aiming to bridge the gap between control-centric and
data-centric intermediate representations. By bridging these two groups of IRs,
it allows the combination of control-centric and data-centric optimizations in
optimization pipelines. In order to achieve this, MLIR-DaCe provides a data-centric
dialect in MLIR to connect the MLIR and DaCe frameworks.</p><h2 id=mlir-emitchttpsgithubcomiml130mlir-emitc><a href=https://github.com/iml130/mlir-emitc>MLIR-EmitC</a></h2><p>MLIR-EmitC provides a way to translate ML models into C++ code. The repository
contains scripts and tools to translate Keras and TensorFlow models into the
<a href=https://mlir.llvm.org/docs/Dialects/TOSA/>TOSA</a> and
<a href=https://github.com/openxla/stablehlo/>StableHLO</a> dialect and to convert those to
<a href=https://mlir.llvm.org/docs/Dialects/EmitC/>EmitC</a>.
The latter is used to generate calls to a reference implementation.</p><p>The
<a href=https://mlir.llvm.org/docs/Dialects/EmitC/>EmitC</a> dialect itself, as well
as the C++ emitter, are part of MLIR core and are no longer provided as part of
the MLIR-EmitC repository.</p><h2 id=mojohttpsdocsmodularcommojo><a href=https://docs.modular.com/mojo/>Mojo</a></h2><p>Mojo is a new programming language that bridges the gap between research and
production by combining the best of Python syntax with systems programming and
metaprogramming, all leveraging the MLIR ecosystem.
It aims to be a strict superset of Python (i.e. be compatible with existing
programs) and to embrace the CPython immediately for long-tail ecosystem
enablement.</p><h2 id=nod-distributed-runtimehttpsnodaiprojectdistributedruntime-asynchronous-fine-grained-op-level-parallel-runtime><a href=https://nod.ai/project/distributedruntime/>Nod Distributed Runtime</a>: Asynchronous fine-grained op-level parallel runtime</h2><p>Nod&rsquo;s MLIR based Parallel Compiler and Distributed Runtime provide a way to
easily scale out training and inference of very large models across multiple
heterogeneous devices (CPUs/GPUs/Accelerators/FPGAs) in a cluster while
exploiting fine-grained op-level parallelism.</p><h2 id=onnx-mlirhttpsgithubcomonnxonnx-mlir><a href=https://github.com/onnx/onnx-mlir>ONNX-MLIR</a></h2><p>To represent neural network models, users often use
<a href=http://onnx.ai/onnx-mlir/>Open Neural Network
Exchange (ONNX)</a> which is an open standard format for
machine learning interoperability.
ONNX-MLIR is a MLIR-based compiler for rewriting a model in ONNX into a standalone
binary that is executable on different target hardwares such as x86 machines,
IBM Power Systems, and IBM System Z.</p><p>See also this paper:
<a href=https://arxiv.org/abs/2008.08272>Compiling ONNX Neural Network Models Using
MLIR</a>.</p><h2 id=openxlahttpsgithubcomopenxla><a href=https://github.com/openxla>OpenXLA</a></h2><p>A community-driven, open source ML compiler ecosystem, using the best of XLA & MLIR.</p><h2 id=p4httpsp4org><a href=https://p4.org/>P4</a></h2><p>Programming Protocol-independent Packet Processors (P4) is a domain-specific language
for network devices, specifying how data plane devices (switches, NICs, routers,
filters, etc.) process packets.</p><p><a href=https://github.com/p4lang/project-ideas/issues/20>P4HIR</a> is a project to create a
backend for the P4 reference compiler that uses MLIR, via new dialects to model P4&rsquo;s
semantics.</p><h2 id=plaidmlhttpsgithubcomplaidmlplaidml><a href=https://github.com/plaidml/plaidml>PlaidML</a></h2><p>PlaidML is a tensor compiler that facilitates reusable and performance portable
ML models across various hardware targets including CPUs, GPUs, and
accelerators.</p><h2 id=polyblockshttpswwwpolymagelabscomtechnologypolyblocks-an-mlir-based-jit-and-aot-compiler><a href=https://www.polymagelabs.com/technology/#polyblocks>PolyBlocks</a>: An MLIR-based JIT and AOT compiler</h2><p>PolyBlocks is a high-performance MLIR-based end-to-end compiler for DL and
non-DL computations. It can perform both JIT and AOT compilation. Its compiler
engine is aimed at being fully automatic, modular, analytical model-driven, and
fully code generating (no reliance on vendor/HPC libraries).</p><h2 id=polygeisthttpsgithubcomllvmpolygeist-cc-frontend-and-optimizations-for-mlir><a href=https://github.com/llvm/Polygeist>Polygeist</a>: C/C++ frontend and optimizations for MLIR</h2><p>Polygeist is a C/C++ frontend for MLIR which preserves high-level structure
from programs such as parallelism. Polygeist also includes high-level optimizations
for MLIR, as well as various raising/lowering utilities.</p><p>See both the polyhedral Polygeist paper
<a href=https://ieeexplore.ieee.org/document/9563011>Polygeist: Raising C to Polyhedral MLIR</a>
and the GPU Polygeist paper
<a href=https://arxiv.org/abs/2207.00257>High-Performance GPU-to-CPU Transpilation and Optimization via High-Level Parallel Constructs</a></p><h2 id=pylirhttpsgithubcomzero9178pylir><a href=https://github.com/zero9178/Pylir>Pylir</a></h2><p>Pylir aims to be an optimizing Ahead-of-Time Python Compiler with high language
conformance. It uses MLIR Dialects for the task of high level, language specific
optimizations as well as LLVM for code generation and garbage collector
support.</p><h2 id=risehttpsrise-langorg><a href=https://rise-lang.org/>RISE</a></h2><p>RISE is a spiritual successor to the
<a href=http://www.lift-project.org/>Lift project</a>: &ldquo;a high-level functional data
parallel language with a system of rewrite rules which encode algorithmic
and hardware-specific optimisation choices&rdquo;.</p><h2 id=sophgo-tpu-mlirhttpsgithubcomsophgotpu-mlir><a href=https://github.com/sophgo/tpu-mlir>SOPHGO TPU-MLIR</a></h2><p>TPU-MLIR is an open-source machine-learning compiler based on MLIR for
SOPHGO TPU.
<a href=https://arxiv.org/abs/2210.15016>https://arxiv.org/abs/2210.15016</a>.</p><h2 id=substrait-mlirhttpsgithubcomsubstrait-iosubstrait-mlir-contrib><a href=https://github.com/substrait-io/substrait-mlir-contrib/>Substrait MLIR</a></h2><p>Substrait MLIR is an input/output dialect for
<a href=https://substrait.io/>Substrait</a>, the cross-language serialization format of
database query plans (akin to an intermediate representation/IR for database
queries).</p><h2 id=tensorflowhttpswwwtensorfloworgmlir><a href=https://www.tensorflow.org/mlir>TensorFlow</a></h2><p>MLIR is used as a Graph Transformation framework and the foundation for
building many tools (XLA, TFLite converter, quantization, &mldr;).</p><h2 id=tenstorrent-mlir-compilerhttpsgithubcomtenstorrenttt-mlir><a href=https://github.com/tenstorrent/tt-mlir>Tenstorrent MLIR Compiler</a></h2><p>tt-mlir is a compiler project aimed at defining MLIR dialects to abstract compute
on Tenstorrent AI accelerators. It is built on top of the MLIR compiler infrastructure
and targets TTNN.</p><p>For more information on the project, see
<a href=https://tenstorrent.github.io/tt-mlir/>https://tenstorrent.github.io/tt-mlir/</a>.</p><h2 id=tfrt-tensorflow-runtimehttpsgithubcomtensorflowruntime><a href=https://github.com/tensorflow/runtime>TFRT: TensorFlow Runtime</a></h2><p>TFRT aims to provide a unified, extensible infrastructure layer for an
asynchronous runtime system.</p><h2 id=torch-mlirhttpsgithubcomllvmtorch-mlir><a href=https://github.com/llvm/torch-mlir>Torch-MLIR</a></h2><p>The Torch-MLIR project aims to provide first class compiler support from the
PyTorch ecosystem to the MLIR ecosystem.</p><h2 id=tritonhttpsgithubcomopenaitriton><a href=https://github.com/openai/triton>Triton</a></h2><p>Triton is a language and compiler for writing highly efficient custom
Deep-Learning primitives. The aim of Triton is to provide an open-source
environment to write fast code at higher productivity than CUDA, but also
with higher flexibility than other existing DSLs.</p><h2 id=vasthttpsgithubcomtrailofbitsvast-cc-frontend-for-mlir><a href=https://github.com/trailofbits/vast>VAST</a>: C/C++ frontend for MLIR</h2><p>VAST is a library for program analysis and instrumentation of C/C++ and related languages.
VAST provides a foundation for customizable program representation for a broad spectrum
of analyses. Using the MLIR infrastructure, VAST provides a toolset to represent C/C++
program at various stages of the compilation and to transform the representation to the
best-fit program abstraction.</p><h2 id=veronahttpsgithubcommicrosoftverona><a href=https://github.com/microsoft/verona>Verona</a></h2><p>Project Verona is a research programming language to explore the concept of
concurrent ownership. They are providing a new concurrency model that seamlessly
integrates ownership.</p><h2 id=zaozihttpsgithubcomsequencerzaozi><a href=https://github.com/sequencer/zaozi>Zaozi</a></h2><p>Zaozi is an project aimed at rewriting
<a href=https://github.com/chipsalliance/chisel>Chisel</a> in
pure Scala 3 and MLIR. it provides a lean eDSL in Scala 3 and binds MLIR C-API with JVM
Project Panama.
The goal of this project is providing an eDSL frontend framework for hardware designs.</p><div class=edit-meta><br><a href=https://github.com/llvm/mlir-www//edit/main/website/content/users/_index.md class=edit-page><i class="fas fa-pen-square"></i> Edit on GitHub</a></div><nav class=pagination><a class="nav nav-prev" href=https://mlir.llvm.org/governance/ title=Governance><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - Governance</a>
<a class="nav nav-next" href=https://mlir.llvm.org/pubs/ title="MLIR Related Publications">Next - MLIR Related Publications <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://mlir.llvm.org/>Home</a></li><li><a href=https://mlir.llvm.org/governance/>Governance</a></li><li class=active><a href=https://mlir.llvm.org/users/>Users of MLIR</a></li><li><a href=https://mlir.llvm.org/pubs/>MLIR Related Publications</a></li><li><a href=https://mlir.llvm.org/talks/>Talks</a></li><li><a href=https://mlir.llvm.org/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://mlir.llvm.org/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://mlir.llvm.org/getting_started/Faq/>FAQ</a></li><li><a href=https://mlir.llvm.org/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://mlir.llvm.org/getting_started/DeveloperGuide/>Developer Guide</a></li><li><a href=https://mlir.llvm.org/getting_started/openprojects/>Open Projects</a></li><li><a href=https://mlir.llvm.org/getting_started/Glossary/>Glossary</a></li><li><a href=https://mlir.llvm.org/getting_started/TestingGuide/>Testing Guide</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/>Code Documentation<span class="mark closed">+</span></a><ul class=sub-menu><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Bindings/>Bindings<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Bindings/Python/>MLIR Python Bindings</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tools/>Tools<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tools/MLIRLSP/>MLIR : Language Server Protocol</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-reduce/>MLIR Reduce</a></li><li><a href=https://mlir.llvm.org/docs/Tools/mlir-rewrite/>mlir-rewrite</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/OpenMPPasses/></a></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIRTransforms/></a></li><li><a href=https://mlir.llvm.org/docs/ActionTracing/>Action: Tracing and Debugging MLIR-based Compilers</a></li><li><a href=https://mlir.llvm.org/docs/Bufferization/>Bufferization</a></li><li><a href=https://mlir.llvm.org/docs/DataLayout/>Data Layout Modeling</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/DefiningDialects/>Defining Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Constraints/>Constraints</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Assembly/>Customizing Assembly Behavior</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/AttributesAndTypes/>Defining Dialect Attributes and Types</a></li><li><a href=https://mlir.llvm.org/docs/DefiningDialects/Operations/>Operation Definition Specification (ODS)</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Diagnostics/>Diagnostic Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/DialectConversion/>Dialect Conversion</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/>Dialects<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/XeGPUTransformOps/></a></li><li><a href=https://mlir.llvm.org/docs/Dialects/OpenACCDialect/>'acc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Affine/>'affine' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMDGPU/>'amdgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AMX/>'amx' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArithOps/>'arith' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmNeon/>'arm_neon' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSVE/>'arm_sve' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ArmSME/>'ArmSME' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/AsyncDialect/>'async' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/BufferizationOps/>'bufferization' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ControlFlowDialect/>'cf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ComplexOps/>'complex' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/DLTIDialect/>'dlti' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/EmitC/>'emitc' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Func/>'func' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/GPU/>'gpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IndexOps/>'index' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/IRDL/>'irdl' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/Linalg/>'linalg' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/Linalg/OpDSL/>Linalg OpDSL</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/LLVM/>'llvm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MathOps/>'math' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MemRef/>'memref' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MLProgramOps/>'ml_program' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MPI/>'mpi' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVGPU/>'nvgpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/NVVMDialect/>'nvvm' Dialect</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/>'omp' Dialect<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Dialects/OpenMPDialect/ODS/>ODS Documentation</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLInterpOps/>'pdl_interp' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PDLOps/>'pdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/PtrOps/>'ptr' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/QuantDialect/>'quant' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ROCDLDialect/>'rocdl' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SCFDialect/>'scf' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/ShapeDialect/>'shape' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Shard/>'shard' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SMT/>'smt' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SparseTensorOps/>'sparse_tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TensorOps/>'tensor' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/UBOps/>'ub' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/VCIXDialect/>'vcix' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Vector/>'vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/WasmSSAOps/>'wasmssa' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/X86Vector/>'x86vector' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeGPU/>'xegpu' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/XeVMDialect/>'xevm' Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Builtin/>Builtin Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/MatchOpInterfaces/>OpInterface definitions</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/SPIR-V/>SPIR-V Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/TOSA/>Tensor Operator Set Architecture (TOSA) Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Dialects/Transform/>Transform Dialect</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Interfaces/>Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/TargetLLVMIR/>LLVM IR Target</a></li><li><a href=https://mlir.llvm.org/docs/BytecodeFormat/>MLIR Bytecode Format</a></li><li><a href=https://mlir.llvm.org/docs/CAPI/>MLIR C API</a></li><li><a href=https://mlir.llvm.org/docs/LangRef/>MLIR Language Reference</a></li><li><a href=https://mlir.llvm.org/docs/ReleaseNotes/>MLIR Release Notes</a></li><li><a href=https://mlir.llvm.org/docs/Canonicalization/>Operation Canonicalization</a></li><li><a href=https://mlir.llvm.org/docs/OwnershipBasedBufferDeallocation/>Ownership-based Buffer Deallocation</a></li><li><a href=https://mlir.llvm.org/docs/PassManagement/>Pass Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/Passes/>Passes</a></li><li><a href=https://mlir.llvm.org/docs/PatternRewriter/>Pattern Rewriting : Generic DAG-to-DAG Rewriting</a></li><li><a href=https://mlir.llvm.org/docs/PatternSearch/>Pattern Search</a></li><li><a href=https://mlir.llvm.org/docs/PDLL/>PDLL - PDL Language</a></li><li><a href=https://mlir.llvm.org/docs/Quantization/>Quantization</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Rationale/>Rationale<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleGenericDAGRewriter/>Generic DAG Rewriter Infrastructure Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleLinalgDialect/>Linalg Dialect Rationale: The Case For Compiler-Friendly Custom Operations</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/Rationale/>MLIR Rationale</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/MLIRForGraphAlgorithms/>MLIR: Incremental Application to Graph Algorithms in ML Frameworks</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/RationaleSimplifiedPolyhedralForm/>MLIR: The case for a simplified polyhedral form</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/SideEffectsAndSpeculation/>Side Effects & Speculation</a></li><li><a href=https://mlir.llvm.org/docs/Rationale/UsageOfConst/>Usage of 'const' in MLIR, for core IR types</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Remarks/>Remark Infrastructure</a></li><li><a href=https://mlir.llvm.org/docs/ShapeInference/>Shape Inference</a></li><li><a href=https://mlir.llvm.org/docs/SPIRVToLLVMDialectConversion/>SPIR-V Dialect to LLVM Dialect conversion manual</a></li><li><a href=https://mlir.llvm.org/docs/SymbolsAndSymbolTables/>Symbols and Symbol Tables</a></li><li><a href=https://mlir.llvm.org/docs/DeclarativeRewrites/>Table-driven Declarative Rewrite Rule (DRR)</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Traits/>Traits<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Traits/Broadcastable/>The `Broadcastable` Trait</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/>Tutorials<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/CreatingADialect/>Creating a Dialect</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/QuickstartRewrites/>Quickstart tutorial to adding MLIR graph rewrite</a></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/Toy/>Toy Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-1/>Chapter 1: Toy Language and AST</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/>Chapter 2: Emitting Basic MLIR</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-3/>Chapter 3: High-level Language-Specific Analysis and Transformation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-4/>Chapter 4: Enabling Generic Transformation with Interfaces</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-5/>Chapter 5: Partial Lowering to Lower-Level Dialects for Optimization</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-6/>Chapter 6: Lowering to LLVM and CodeGeneration</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/Toy/Ch-7/>Chapter 7: Adding a Composite Type to Toy</a></li></ul></li><li class=has-sub-menu><a href=https://mlir.llvm.org/docs/Tutorials/transform/>Transform Dialect Tutorial<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch0/>Chapter 0: A Primer on “Structured” Linalg Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch1/>Chapter 1: Combining Existing Transformations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch2/>Chapter 2: Adding a Simple New Transformation Operation</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch3/>Chapter 3: More than Simple Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/Ch4/>Chapter 4: Matching Payload with Transform Operations</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/transform/ChH/>Chapter H: Reproducing Halide Schedule</a></li></ul></li><li><a href=https://mlir.llvm.org/docs/Tutorials/UnderstandingTheIRStructure/>Understanding the IR Structure</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/MlirOpt/>Using `mlir-opt`</a></li><li><a href=https://mlir.llvm.org/docs/Tutorials/DataFlowAnalysis/>Writing DataFlow Analyses in MLIR</a></li></ul></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>